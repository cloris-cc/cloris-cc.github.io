<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>2021-02-26</title>
    <url>/2021/02/26/diary/2021-02-26/</url>
    <content><![CDATA[<p>偶然看到一个<a href="https://video.weibo.com/show?fid=1034:4608502736551968">音乐纪录片</a>，是和马思唯的新专辑《黑马》中的一首歌《东大街》有关。虽然之前就听过这首歌了但是没有什么特别的情感（也和自己对新专辑的期待比较高有关吧）现在看完还是有许多说不上来的感慨 TT</p>
]]></content>
      <categories>
        <category>diary</category>
      </categories>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title>碎碎念</title>
    <url>/2023/09/01/diary/d1/</url>
    <content><![CDATA[<h2 id="今天是超级大的台风！！"><a href="#今天是超级大的台风！！" class="headerlink" title="今天是超级大的台风！！"></a>今天是超级大的台风！！</h2><p>不过终于改好新网站啦，这几天脑袋都迷糊了，准备洗澡次饭去！</p>
<p>还有，the most important thing issss</p>
<blockquote>
<p><strong>茶茶九月快乐！！！</strong> 可爱小猫跟小狗勾带给你好运！</p>
</blockquote>
<p><img src="/img/s/100.gif"></p>
]]></content>
      <categories>
        <category>diary</category>
      </categories>
      <tags>
        <tag>日记</tag>
        <tag>茶茶</tag>
      </tags>
  </entry>
  <entry>
    <title>麻了x.x</title>
    <url>/2023/09/02/diary/d2/</url>
    <content><![CDATA[<p>从早上到现在一直在改页面样式人都麻了，待会得好好干正事了！！！</p>
]]></content>
      <categories>
        <category>diary</category>
      </categories>
      <tags>
        <tag>日记</tag>
      </tags>
  </entry>
  <entry>
    <title>好困好困！！！</title>
    <url>/2021/02/24/diary/day1/</url>
    <content><![CDATA[<p>凌晨04:32 好困好困 折腾了好几天新小破网站终于有雏形了！！</p>
<p>允许我偷懒今天少写点日记刷牙洗脸爬床了！</p>
<span id="more"></span>

<p>茶茶哦呀粟米030</p>
<p>补充：凌晨5点多快六点了！茶茶为了可乐点了外卖哈哈啊哈！</p>
]]></content>
      <categories>
        <category>diary</category>
      </categories>
      <tags>
        <tag>日记</tag>
        <tag>茶茶</tag>
      </tags>
  </entry>
  <entry>
    <title>handson-ml2</title>
    <url>/2023/01/22/machine_learning/handson-ml2/</url>
    <content><![CDATA[<p>[TOC]</p>
<h1 id="Preface"><a href="#Preface" class="headerlink" title="Preface"></a>Preface</h1><p><a href="https://colab.research.google.com/github/ageron/handson-ml3/blob/main/index.ipynb">Prerequisite Links</a> &amp; <a href="https://www.khanacademy.org/">Math</a></p>
<p>The first part is based mostly on Scikit-Learn, while the second part uses TensorFlow and Keras.</p>
<h1 id="Part-1-机器学习的基础知识"><a href="#Part-1-机器学习的基础知识" class="headerlink" title="Part.1 机器学习的基础知识"></a>Part.1 机器学习的基础知识</h1><h2 id="1-机器学习概览"><a href="#1-机器学习概览" class="headerlink" title="1. 机器学习概览"></a>1. 机器学习概览</h2><h3 id="1-1-机器学习的适用场景"><a href="#1-1-机器学习的适用场景" class="headerlink" title="1.1 机器学习的适用场景"></a>1.1 机器学习的适用场景</h3><ul>
<li>现有解决方案需要大量微调或一长串<strong>规则</strong>的问题（机器学习模型通常可以简化代码并比传统方法执行得更好）</li>
<li>使用传统方法无法解决的<strong>复杂</strong>问题（最好的机器学习技术或许可以找到解决方案）</li>
<li><strong>波动的环境</strong>（机器学习系统可以很容易地在新数据上重新训练，始终保持最新）</li>
<li>深入了解复杂问题和大量数据</li>
</ul>
<h3 id="1-1-机器学习的分类标准"><a href="#1-1-机器学习的分类标准" class="headerlink" title="1.1 机器学习的分类标准"></a>1.1 机器学习的分类标准</h3><p>机器学习系统可按以下几个标准分类：</p>
<ul>
<li>是否在人类监督下训练（有监督学习、无监督学习、半监督学习、 自我监督学习和强化学习）。</li>
<li>是否可以即时(增量)学习（在线学习和批量学习）。</li>
<li>是简单地将新数据和已有的数据比较，还是通过检测训练数据中的模式并建立预测模型来工作，就像科学家所做的那样（基于实例与基于模型的学习）</li>
</ul>
<p>这些标准不是互相独立的，而是可以任意组合的。例如，一个高级的邮件过滤系统可能同时具备即时学习、基于模型、有监督的学习。</p>
<h4 id="有无监督"><a href="#有无监督" class="headerlink" title="有无监督"></a>有无监督</h4><p>ML systems 根据 <em>supervision</em> 的类型，可主要分为以下几类：supervised learning, unsupervised learning, self-supervised learning, semi-supervised learning, and reinforcement learning.</p>
<p><strong>有监督学习 (Supervised learning)：</strong></p>
<p>常见算法：k-近邻算法，线性回归，逻辑回归，支持向量机(SVM)，决策树和随机森林，神经网络。</p>
<p>有些回归模型也可以用于分类任务，反之亦然。例如，逻辑回归模型经常用于分类，它能输出对应分类的概率值 (e.g., 20% chance of being spam)。</p>
<p>在有监督学习中，<em>target</em> 和 <em>label</em> 是同义词，只不过 target 在回归任务中更常用，<em>label</em> 多出现在分类任务中。<em>features</em> 有时也被叫做 <em>predictors</em> 或 <em>attributes</em>。</p>
<p><strong>无监督学习 (Unsupervised learning) ：</strong></p>
<p>常见算法：<strong>聚类 (<em>clustering</em>)</strong> ，k-均值，DBSCAN，分层聚类分析（HCA），<strong>异常检测 (<em>anomaly detection</em>)</strong> 和新颖性检测 (<em>novelty detection</em>)，单类 SVM，孤立森，可视化和**降维 (dimensionality reduction)*<em>，主成分分析（PCA），核主成分分析，局部线性嵌入（LLE），t-分布随机近邻嵌入（t-SNE），关联规则学习（</em>association rule learning*），Apriori，Eclat。</p>
<p><em>dimensionality reduction</em>: 合并多个相关的 features 为一个新的 feature，这个过程也可成为 <em>feature extraction</em>。 例如，汽车的里程数和车龄可以合并为一个新特征——磨损，这个磨损就是提取出来的新特征。</p>
<p><strong>半监督学习算法 (Semi-supervised learning)：</strong></p>
<p>无监督算法和有监督算法的结合。</p>
<p>如照片托管服务，当上传一些照片后，系统会自动分类人物A，人物B，人物C… 各自出现在哪些照片中，这是无监督学习，使用了 Clustering 算法；接着，当你给人物A的任意一张照片标注姓名，其余包括人物A的照片也会自动一起被标记，这属于有监督学习范畴。</p>
<p><strong>自我监督学习 (Self-upervised learning) ：</strong></p>
<p>Another approach to machine learning involves actually <strong>generating a fully labeled dataset</strong> from a fully unlabeled one. </p>
<p>例如，如果你有一个大型的无标签图像数据集，你可以随机地遮挡每个图像的一小部分，然后**训练一个模型 (初期是为了完成 auxiliary task) 来恢复原始图像  **（图 1-12）。在训练过程中，被遮挡的图像作为模型的输入，而原始图像作为 labels。</p>
<p><img src="https://s2.loli.net/2023/01/27/jsUcg3GKpYBODRw.png" alt="image-20230127000404930"></p>
<p>这个补充恢复图像的新模型经过稍微调整后，也可以用来完成分类任务。这种从一个任务迁移到另一个不同任务的方法，称为**迁移学习 (<em>transfer learning</em>)**。</p>
<p>此外，自监督学习不能等同于无监督学习的分支，因为在完成辅助任务的过程中，新模型的训练用到了原始图像作为 labels。通常情况下，无监督学习的任务主要有 clustering, dimensionality reduction, or anomaly detection; 自监督学习的任务通常是 classification and regression.</p>
<p><strong>强化学习 (Reinforcement learning)：</strong></p>
<p>通过 <em>Agent</em> 观察 Environment，再根据 <em>Policy</em> 执行 Act 并获得奖惩，更新 Policy 直到最佳。</p>
<p><img src="https://s2.loli.net/2023/01/23/QLHazypCqd3hncb.png" alt="image-20230123191249966"></p>
<h4 id="能否在线"><a href="#能否在线" class="headerlink" title="能否在线"></a>能否在线</h4><p><strong>批量学习 (Batch Learning)：</strong></p>
<p>批量学习无法快速学习。它在完成训练后，模型就投入生产环境使用并停止学习，它只是将其所学到的应用出来。这种方式称为<em>离线学习 (offline learning</em>)。在信息快速更新的时代，离线学习模型的性能会随时间慢慢衰减。</p>
<p>如果批量学习也想实现持续学习，可以在原数据集的基础上，再增加生产环境的数据训练新的模型，再进行更新迭代。例如，每天或每周训练一次新系统，只是训练可能需要花上好几个小时并耗费大量计算资源，在一些需要即时更新的系统上（如股票预测）也不适用。</p>
<p><strong>在线学习 (<em>Online Learning</em>)：</strong></p>
<p>在线学习的是通过**持续(sequentially)<strong>给模型送入</strong>小批量(<em>mini-batches</em>)**数据实现快速学习的。这样每次学习的过程都很快，且不会一次性占用太多时间资源。对于超大数据集——超出一台计算机的 RAM 容量，在线学习算法也同样适用，超大数据集会被切分成 mini-batches 进行训练 (这称为核外学习 <em>out-of-core</em> learning)。</p>
<p>在线学习面临的一个重大挑战是，如果给系统输入了不良数据 (bad data)，系统的性能将会逐渐下降。为了降低这种风险，你需要密切监控系统，一旦检测到性能下降，就及时中断学习（可能还需要恢复到之前的工作状态）。当然，同时你还需要监控输入数据，并对异常数据做出响应（例如，使用异常检测算法）。</p>
<h4 id="基于实例-vs-基于模型的学习"><a href="#基于实例-vs-基于模型的学习" class="headerlink" title="基于实例 vs 基于模型的学习"></a>基于实例 vs 基于模型的学习</h4><p><em><strong>Instance-based learning</strong></em> :</p>
<p>The system learns the examples by heart, then generalizes to new cases by using a <em>similarity measure (相似性度量)</em> to compare them to the learned examples.</p>
<p>For example, in Figure 1-16 the new instance would be classified as a triangle because the majority of the most similar instances belong to that class.</p>
<p><img src="https://s2.loli.net/2023/01/28/gdzKfamctFGC7ve.png" alt="image-20230128034645014"></p>
<p><em><strong>Model-based learning</strong></em> :</p>
<p><img src="https://s2.loli.net/2023/01/28/gCT9aFGbdyMUHoj.png" alt="image-20230128034710896"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line"><span class="comment"># Download and prepare the data</span></span><br><span class="line">data_root = <span class="string">&quot;https://github.com/ageron/data/raw/main/&quot;</span></span><br><span class="line">lifesat = pd.read_csv(data_root + <span class="string">&quot;lifesat/lifesat.csv&quot;</span>)</span><br><span class="line">X = lifesat[[<span class="string">&quot;GDP per capita (USD)&quot;</span>]].values</span><br><span class="line">y = lifesat[[<span class="string">&quot;Life satisfaction&quot;</span>]].values</span><br><span class="line"></span><br><span class="line"><span class="comment"># Visualize the data</span></span><br><span class="line">lifesat.plot(kind=<span class="string">&#x27;scatter&#x27;</span>, grid=<span class="literal">True</span>,</span><br><span class="line">             x=<span class="string">&quot;GDP per capita (USD)&quot;</span>, y=<span class="string">&quot;Life satisfaction&quot;</span>)</span><br><span class="line">plt.axis([<span class="number">23_500</span>, <span class="number">62_500</span>, <span class="number">4</span>, <span class="number">9</span>])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Select a linear model</span></span><br><span class="line">model = LinearRegression()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Train the model</span></span><br><span class="line">model.fit(X, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make a prediction for Cyprus</span></span><br><span class="line">X_new = [[<span class="number">37_655.2</span>]]  <span class="comment"># Cyprus&#x27; GDP per capita in 2020</span></span><br><span class="line"><span class="built_in">print</span>(model.predict(X_new)) <span class="comment"># outputs [[6.30165767]]</span></span><br></pre></td></tr></table></figure>

<p>如果本例要使用基于实例的学习，可使用 KNN。K 最近邻 (<em>k-Nearest Neighbor</em>，KNN) 分类算法，是一个理论上比较成熟的方法，也是最简单的<a href="https://baike.baidu.com/item/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95/18635836?fromModule=lemma_inlink">机器学习算法</a>之一。该方法的思路是：在特征空间中，如果一个样本附近的k个最近(即特征空间中最邻近)样本的大多数属于某一个类别，则该样本也属于这个类别。</p>
<p>Replacing the linear regression model with <em>k</em>-nearest neighbors regression in the previous code is as easy as replacing these lines:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">model = LinearRegression()</span><br></pre></td></tr></table></figure>

<p>with these two:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsRegressor</span><br><span class="line">model = KNeighborsRegressor(n_neighbors=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>



<h3 id="1-2-机器学习的主要挑战"><a href="#1-2-机器学习的主要挑战" class="headerlink" title="1.2 机器学习的主要挑战"></a>1.2 机器学习的主要挑战</h3><p>机器学习的主要挑战在 2 方面：数据和算法。</p>
<p><strong>Bad data:</strong></p>
<ul>
<li><p>训练数据的数量不足：<strong>在某些复杂的问题上，充足的数据比算法更重要。</strong></p>
<p>缓解方法有：数据增强</p>
</li>
<li><p>训练数据不具代表性：如果样本集太小，可能会出现采样噪声 (<em>sampling noise</em>)，即产生 nonrepresentative data；而即便是非常大的样本数据，如果采样方式欠妥，也同样可能导致非代表性数据集，这就是采样偏差 (<em>sampling bias</em>)。sampling bias 具体有：<em>nonresponse bias</em>.</p>
<p>缓解方法有：合理采样</p>
</li>
<li><p>低质量数据</p>
<p>缓解方法有：直接丢弃或修正错误；若部分缺失，可考虑数据填充（如用中位数、平均值等代替）或直接丢弃有缺失的 data 或 features。</p>
</li>
<li><p>无关特征：只有训练数据里包含足够多的相关特征以及较少的无关特征，系统才能够完成学习。</p>
<p>缓解方法有：特征工程 (<em>feature engineering</em>)，其步骤如下。</p>
<ul>
<li>特征选择 (<em>feature selection</em>)。从现有特征中选择最有用的特征进行训练。</li>
<li>特征提取 (<em>feature extraction</em>)。将现有特征进行整合，产生更有用的特征——即使用 reducetion algorithms。</li>
<li>通过收集新数据创建新的特征。</li>
</ul>
</li>
<li><p>数据不匹配：验证集和测试集必须与在<strong>生产环境中使用的数据</strong>具有相同的代表性。（通过混合数据）</p>
<p>缓解方法有：划分出 train-dev 数据集</p>
</li>
</ul>
<p><strong>Bad algorithm:</strong></p>
<ul>
<li><p>过拟合训练数据 (<em>overfitting the training data</em>)</p>
<p>Overfitting happens when the model is too complex relative to the amount and noisiness of the training data. Here are possible solutions:</p>
<ul>
<li>Simplify the model by selecting one with fewer parameters (e.g., a linear model rather than a high-degree polynomial model), by reducing the number of attributes in the training data, or by constraining the model.</li>
<li>Gather more training data. </li>
<li>Reduce the noise in the training data (e.g., fix data errors and remove outliers).</li>
</ul>
<p>Constraining a model to make it simpler and reduce the risk of overfitting is called <em>regularization (正则化)</em>. For example, the linear model we defined earlier has two parameters, θ<del>0</del>  and θ<del>1</del> . This gives the learning algorithm two <em>degrees of freedom (自由度)</em> to adapt the model to the training data:</p>
<p><img src="https://s2.loli.net/2023/01/28/MdgyfeExns7jAoI.png" alt="image-20230128221906781"></p>
<p>The amount of regularization to apply during learning can be controlled by a <em>hyper‐parameter (超参数)</em>. A hyperparameter is a parameter of a learning algorithm (not of the model). As such, it is not affected by the learning algorithm itself; it must be set prior to training and remains constant during training.</p>
</li>
<li><p>欠拟合训练数据 (<em>underfitting the training data</em>)</p>
<p>Here are the main options for fixing this problem:</p>
<ul>
<li>Select a more powerful model, with more parameters.</li>
<li>Feed better features to the learning algorithm (feature engineering).</li>
<li>Reduce the constraints on the model (for example by reducing the regularization hyperparameter).</li>
</ul>
</li>
</ul>
<h3 id="1-3-Testing-and-Validating"><a href="#1-3-Testing-and-Validating" class="headerlink" title="1.3 Testing and Validating"></a>1.3 Testing and Validating</h3><p>The error rate on new cases is called the <em>generalization error</em> (or <em>out-of-sample error</em>). If the training error is low but the generalization error is high, it means that your model is overfitting the training data.</p>
<p>在寻找最佳超参数的过程中需要<strong>训练评估多个模型</strong>。如果仅仅使用一份 test set，可能会对 test set 产生过拟合。最好的办法是为 k 个候选模型再保留 k 份 <em>validation set</em> (<em>dev set</em>)，在候选模型二次训练 ( train set and (k-1) valid sets) 和验证 (1 份 valid set) 的时候使用：</p>
<p><img src="https://s2.loli.net/2023/01/29/ThiZ2Na5H1eMWkw.png" alt="image-20230129002353543"></p>
<p>循环上述过程直到 k 个模型均完成。这个过程称为 <em>K-fold cross-validation (K 折交叉验证)</em>.</p>
<h3 id="1-x-Exercises"><a href="#1-x-Exercises" class="headerlink" title="1.x Exercises"></a>1.x Exercises</h3><ol>
<li><p>How would you define machine learning? </p>
</li>
<li><p>Can you name four types of applications where it shines? </p>
</li>
<li><p>What is a labeled training set? </p>
</li>
<li><p>What are the two most common supervised tasks?</p>
<p>classification，regression.</p>
</li>
<li><p>Can you name four common unsupervised tasks? </p>
<p>Clustering，<del>anomaly detection</del> visualization，dimensionality reduction and <strong>association rule learning.</strong></p>
</li>
<li><p>What type of algorithm would you use to allow a robot to walk in various unknown terrains?</p>
<p>Reinforcement learning.</p>
</li>
<li><p>What type of algorithm would you use to segment your customers into multiple groups?</p>
<p>Clustering，<del>association rule learning</del> or classification.</p>
</li>
<li><p>Would you frame the problem of spam detection as a supervised learning problem or an unsupervised learning problem?</p>
<p>Supervised learning.</p>
</li>
<li><p>What is an online learning system? </p>
<p>Learn <strong>incrementally</strong> on the fly.</p>
</li>
<li><p>What is out-of-core learning?</p>
<p>Out-of-RAM online learning by chopping the data into mini-batches.</p>
</li>
<li><p>What type of algorithm relies on a similarity measure to make predictions?</p>
<p>Instance-based learning.</p>
</li>
<li><p>What is the difference between a model parameter and a model hyperparameter?</p>
<p>Hyperparameter is a parameter of the learning algorithm and untrainable.</p>
</li>
<li><p>What do model-based algorithms search for? What is the most common strategy they use to succeed? How do they make predictions?</p>
<p>no hyperparameter. search for best model parameters that are trainable by minimizing a cost function.</p>
</li>
<li><p>Can you name four of the main challenges in machine learning?</p>
</li>
<li><p>If your model performs great on the training data but generalizes poorly to new instances, what is happening? Can you name three possible solutions?</p>
<p>Overfitting. Simplify the model with Regularization, either get more data or reduce noise in training data.</p>
</li>
<li><p>What is a test set, and why would you want to use it? </p>
<p>Estimate the generalization error.</p>
</li>
<li><p>What is the purpose of a validation set? </p>
<p>To <strong>compare models</strong> and select the best one; tune the hyperparameters.</p>
</li>
<li><p>What is the train-dev set, when do you need it, and how do you use it?</p>
<p>Data mismatch.</p>
</li>
<li><p>What can go wrong if you tune hyperparameters using the test set?</p>
<p>Be likely to overfitting the test set. Moreover, the generalization error will be worse than you expect.</p>
</li>
</ol>
<h2 id="2-End-to-End-Machine-Learning-Project"><a href="#2-End-to-End-Machine-Learning-Project" class="headerlink" title="2. End-to-End Machine Learning Project"></a>2. End-to-End Machine Learning Project</h2><p>Objective: Predict median housing price.</p>
<h3 id="2-1-Preparation"><a href="#2-1-Preparation" class="headerlink" title="2.1 Preparation"></a>2.1 Preparation</h3><h4 id="Working-with-Real-Data"><a href="#Working-with-Real-Data" class="headerlink" title="Working with Real Data"></a>Working with Real Data</h4><p>Popular open data repositories:</p>
<ul>
<li><p>OpenML.org</p>
</li>
<li><p>Kaggle.com</p>
</li>
<li><p>PapersWithCode.com</p>
</li>
<li><p>UC Irvine Machine Learning Repository</p>
</li>
<li><p>Amazon’s AWS datasets</p>
</li>
<li><p>TensorFlow datasets</p>
</li>
</ul>
<p>Meta portals (they list open data repositories):</p>
<ul>
<li><p>DataPortals.org</p>
</li>
<li><p>OpenDataMonitor.eu</p>
</li>
</ul>
<p>Other pages listing many popular open data repositories:</p>
<ul>
<li><p>Wikipedia’s list of machine learning datasets</p>
</li>
<li><p>Quora.com</p>
</li>
<li><p>The datasets subreddit</p>
</li>
</ul>
<p>Note: If the data were huge, you could either split your batch learning work across multiple servers (using the MapReduce technique) or use an online learning technique.</p>
<h4 id="Select-a-Performance-Measure"><a href="#Select-a-Performance-Measure" class="headerlink" title="Select a Performance Measure"></a>Select a Performance Measure</h4><p>A typical performance measure for regression problems is the <em>root mean square error (RMSE，根均方误差)</em>.</p>
<p>$RMSE(X, h) &#x3D; \sqrt{\frac 1 m\Sigma ^m _{i&#x3D;1}(h(x^{i})-y^{i})^2}$</p>
<ul>
<li><em>m</em> is the number of instances in the dataset</li>
<li><strong>x^i^</strong> is a vector of all the feature values (excluding the label) of the i^th^ instance in the dataset, and <em>y</em>^i^ is its target.</li>
<li><strong>X</strong> is a matrix containing all the feature values (excluding labels) of all instances in the dataset. </li>
<li><em>h</em> is your system’s prediction function, also called a <em>hypothesis</em>.</li>
<li>RMSE(<strong>X</strong>,<em>h</em>) is the cost function measured on the set of examples using your hypothesis <em>h</em>.</li>
</ul>
<p>We use lowercase italic font for scalar values (such as <em>m</em> or <em>y^(i)^</em> ) and function names (such as <em>h</em>), lowercase bold font for vectors (such as <strong>x^(i)^</strong> ), and uppercase bold font for matrices (such as <strong>X</strong>).</p>
<p>Moreover, if there are many outlier districts, you may consider using the <em>mean absolute error (MAE, 平均绝对误差)</em>, also called the <em>average absolute deviation</em>.</p>
<p>$MAE(X,h)&#x3D;\frac 1 m \Sigma ^m _{i&#x3D;1} \mid h(x^i)-y^i \mid$</p>
<h4 id="Runtime"><a href="#Runtime" class="headerlink" title="Runtime"></a>Runtime</h4><p>If you plan to use Google Colab as <em>runtime</em>, remember to mount your Google Drive as if it’s a local directory. By default, your Google Drive will be mounted at &#x2F;content&#x2F;drive&#x2F;MyDrive. If you want to back up a data file, simply copy it to this directory by running <code>!cp /content/my_great_model /content/drive/MyDrive</code>. Any command starting with a bang (!) is treated as a shell command, not as Python code: <code>cp</code> is the Linux shell command to copy a file from one path to another. Note that Colab runtimes run on Linux (specifically, Ubuntu).</p>
<h3 id="2-2-Load-the-Data"><a href="#2-2-Load-the-Data" class="headerlink" title="2.2 Load the Data"></a>2.2 Load the Data</h3><p>Down a single compressed file, <em>housing.tgz</em>, which contains a <em>comma-separated value (CSV)</em> file called housing.csv with all the data.</p>
<p>Here is the function to fetch and load the data:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"><span class="keyword">import</span> tarfile</span><br><span class="line"><span class="keyword">import</span> urllib.request</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">load_housing_data</span>():</span><br><span class="line">    tarball_path = Path(<span class="string">&quot;datasets/housing.tgz&quot;</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tarball_path.is_file():</span><br><span class="line">        Path(<span class="string">&quot;datasets&quot;</span>).mkdir(parents=<span class="literal">True</span>, exist_ok=<span class="literal">True</span>)</span><br><span class="line">        url = <span class="string">&quot;https://github.com/ageron/data/raw/main/housing.tgz&quot;</span></span><br><span class="line">        urllib.request.urlretrieve(url, tarball_path)</span><br><span class="line">        <span class="keyword">with</span> tarfile.<span class="built_in">open</span>(tarball_path) <span class="keyword">as</span> housing_tarball:</span><br><span class="line">            housing_tarball.extractall(path=<span class="string">&quot;datasets&quot;</span>)</span><br><span class="line">    <span class="comment"># loads this CSV file into a Pandas DataFrame object</span></span><br><span class="line">    <span class="keyword">return</span> pd.read_csv(Path(<span class="string">&quot;datasets/housing/housing.csv&quot;</span>))</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">housing = load_housing_data()</span><br></pre></td></tr></table></figure>

<p>Nextly, you can start by looking at the top 5 rows of data using the DataFrame’s <code>head()</code> method.</p>
<p>The <code>info()</code> method is useful to get a quick description of the data:</p>
<p><img src="https://s2.loli.net/2023/01/30/7jcgPOdEWJeM3Ap.png" alt="image-20230130214016605"></p>
<p><em>total_bedrooms</em> attribute has only 20,433 non-null values, meaning that 207 districts are missing this feature. You will need to take care of this later.</p>
<p>All attributes are numerical, except for <em>ocean_proximity</em>. Its type is object, so it could hold any kind of Python object. But since you loaded this data from a CSV file, you know that it must be a text attribute. When you looked at the top five rows, you probably noticed that the values in the <em>ocean_proximity</em> column were repetitive, which means that it is probably a <strong>categorical</strong> attribute. You can find out what categories exist and how many districts belong to each category by using the <code>value_counts()</code> method:</p>
<p><img src="https://s2.loli.net/2023/01/30/Q8yiFquCGsvmXtU.png" alt="image-20230130220759418"></p>
<p>The <code>describe()</code> method shows a summary of the <strong>numerical</strong> attributes (Figure 2-7).</p>
<p><img src="https://s2.loli.net/2023/01/30/qzSPGdZOLHRoyN2.png" alt="image-20230130221221230"></p>
<p>Besides the 4 ways of introducing data, another quick way to get a feel of the type of data is to plot a histogram for each numerical attribute or call the <code>hist()</code> method on the whole dataset:</p>
<p><img src="https://s2.loli.net/2023/01/30/m4JXPbraOnI3T9s.png" alt="image-20230130222607768"></p>
<h3 id="2-3-Create-a-test-set"><a href="#2-3-Create-a-test-set" class="headerlink" title="2.3 Create a test set"></a>2.3 Create a test set</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shuffle_and_split_data</span>(<span class="params">data, test_radio</span>):</span><br><span class="line">    shuffled_indices = np.random.permutation(<span class="built_in">len</span>(data))</span><br><span class="line">    test_set_size = <span class="built_in">int</span>(<span class="built_in">len</span>(data) * test_radio)</span><br><span class="line">    test_indices = shuffled_indices[:test_set_size]</span><br><span class="line">    train_indices = shuffled_indices[test_set_size:]</span><br><span class="line">    <span class="keyword">return</span> data.iloc[train_indices], data.iloc[test_indices]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">train_set, test_set = shuffle_and_split_data(housing, <span class="number">0.2</span>)</span><br><span class="line"><span class="built_in">len</span>(train_set), <span class="built_in">len</span>(test_set)</span><br></pre></td></tr></table></figure>

<p>To ensure the consistence of test set, you can set a random number generator’s seed.(e.g. with <code>np.random.seed(42)</code>).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>Note: another source of randomness is the order of Python sets: it is based on Python’s <code>hash()</code> function, which is randomly “salted” when Python starts up (this started in Python 3.3, to prevent some denial-of-service attacks). To remove this randomness, the solution is to set the <code>PYTHONHASHSEED</code> environment variable to <code>&quot;0&quot;</code> <em>before</em> Python even starts up. Nothing will happen if you do it after that. Luckily, if you’re running this notebook on Colab, the variable is already set for you.</p>
<p>If the data is updatable, to have a stable train&#x2F;test split even after updating the dataset, you could compute a hash of each instance’s identifier and put that instance in the test set if the hash is lower than or equal to 20% of the maximum hash value. Here is a possible implementation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> zlib <span class="keyword">import</span> crc32</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_id_in_test_set</span>(<span class="params">identifier, test_radio</span>):</span><br><span class="line">    <span class="keyword">return</span> crc32(np.int64(identifier)) &lt; test_radio * <span class="number">2</span> ** <span class="number">32</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_data_with_id_hash</span>(<span class="params">data, test_radio, id_column</span>):</span><br><span class="line">    ids = data[id_column]</span><br><span class="line">    in_test_set = ids.apply(<span class="keyword">lambda</span> id_: is_id_in_test_set(id_, test_radio))</span><br><span class="line">    <span class="keyword">return</span> data.loc[~in_test_set], data.loc[in_test_set]</span><br></pre></td></tr></table></figure>


<p>Unfortunately, the housing dataset does not have an identifier column. The simplest solution is to use the row index as the ID:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing_with_id = housing.reset_index()  <span class="comment"># adds an `index` column</span></span><br><span class="line">train_set, test_set = split_data_with_id_hash(housing_with_id, <span class="number">0.2</span>, <span class="string">&quot;index&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>Alternatively, a district’s latitude and longitude are guaranteed to be stable for a few million years, so you could combine them into an ID like so:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing_with_id[<span class="string">&quot;id&quot;</span>] = housing[<span class="string">&quot;longitude&quot;</span>] * <span class="number">1000</span> + housing[<span class="string">&quot;latitude&quot;</span>]</span><br><span class="line">train_set, test_set = split_data_with_id_hash(housing_with_id, <span class="number">0.2</span>, <span class="string">&quot;id&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>Scikit-Learn provides a few functions to split datasets into multiple subsets in various ways. The simplest function is <code>train_test_split()</code>, which does pretty much the same thing as the shuffle_and_split_data() function we defined earlier:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">train_set, test_set = train_test_split(housing, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>



<p>数据集过小的情况下，不适合使用 random sampling (default)，因为会出现 sampling bias ，即样本不具备代表性；应该使用 <em>stratified sampling</em>.</p>
<p>假设你已经知道了 median_income 对预测结果影响更大，为了保证样本具有代表性，可以将其分为 5 类（每个类别都应该有充足的样本数量）。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing[<span class="string">&quot;income_cat&quot;</span>] = pd.cut(housing[<span class="string">&quot;median_income&quot;</span>],</span><br><span class="line">                               bins=[<span class="number">0.</span>, <span class="number">1.5</span>, <span class="number">3.0</span>, <span class="number">4.5</span>, <span class="number">6.</span>, np.inf],</span><br><span class="line">                               labels=[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>])</span><br></pre></td></tr></table></figure>

<p>用 histogram 表示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing[<span class="string">&quot;income_cat&quot;</span>].value_counts().sort_index().plot.bar(rot=<span class="number">0</span>, grid=<span class="literal">True</span>)</span><br><span class="line">plt.xlabel(<span class="string">&quot;Income category&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Number of districts&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/01/31/vPouqwNT9bcJ4EA.png" alt="image-20230131023524789"></p>
<p>Scikit-learn 可以使用 <code>StratifiedShuffleSplit</code> 分层抽样，并划分出多份数据集利于 cross-validation:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedShuffleSplit</span><br><span class="line"></span><br><span class="line">splitter = StratifiedShuffleSplit(n_splits=<span class="number">10</span>, test_size=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">strat_splits = []</span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> splitter.split(housing, housing[<span class="string">&quot;income_cat&quot;</span>]):</span><br><span class="line">    strat_train_set_n = housing.iloc[train_index]</span><br><span class="line">    strat_test_set_n = housing.iloc[test_index]</span><br><span class="line">    strat_splits.append([strat_train_set_n, strat_test_set_n])</span><br></pre></td></tr></table></figure>

<p>For now, you can just use the first split:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">strat_train_set, strat_test_set = strat_splits[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>Or, since <em>stratified sampling</em> is fairly common, there’s a shorter way to get a single split using the <code>train_test_split()</code> function with the <code>stratify</code> argument:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">strat_train_set, strat_test_set = train_test_split(</span><br><span class="line"> housing, test_size=<span class="number">0.2</span>, stratify=housing[<span class="string">&quot;income_cat&quot;</span>], random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<p>观察是否有按 income_cat 字段分层抽样，以及不同采样方式的对比：</p>
<p><img src="https://s2.loli.net/2023/01/31/gpFAhBJQOD9MLST.png" alt="image-20230131044924230"></p>
<p><img src="https://s2.loli.net/2023/01/31/oPDkdbNGLIspT2y.png" alt="image-20230131045623179"></p>
<p>最后，在该节结束前还原数据集：</p>
<p><img src="https://s2.loli.net/2023/01/31/e2RHmLAT348yrcU.png" alt="image-20230131050014895"></p>
<h3 id="2-4-Explore-and-Visualize-the-Data-to-Gain-Insights"><a href="#2-4-Explore-and-Visualize-the-Data-to-Gain-Insights" class="headerlink" title="2.4 Explore and Visualize the Data to Gain Insights"></a>2.4 Explore and Visualize the Data to Gain Insights</h3><p>In order to obtain a clean dataset, you can make a copy :</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing = strat_train_set.copy()</span><br></pre></td></tr></table></figure>

<h4 id="Visualizing-Geographical-Data"><a href="#Visualizing-Geographical-Data" class="headerlink" title="Visualizing Geographical Data"></a>Visualizing Geographical Data</h4><p>因为数据集中包含地理位置信息，所以可以绘制散点图来观察数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing.plot(kind=<span class="string">&quot;scatter&quot;</span>, x=<span class="string">&quot;longitude&quot;</span>, y=<span class="string">&quot;latitude&quot;</span>, grid=<span class="literal">True</span>, alpha=<span class="number">0.2</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/01/31/Z2UpxTVletRqsMa.png" alt="image-20230131100224620"></p>
<p>接着，population 用圆的大小 s 表示；median_house_value 用颜色 c 表示：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing.plot(kind=<span class="string">&quot;scatter&quot;</span>, x=<span class="string">&quot;longitude&quot;</span>, y=<span class="string">&quot;latitude&quot;</span>, grid=<span class="literal">True</span>,</span><br><span class="line">             s=housing[<span class="string">&quot;population&quot;</span>] / <span class="number">100</span>, label=<span class="string">&quot;population&quot;</span>,</span><br><span class="line">             c=<span class="string">&quot;median_house_value&quot;</span>, cmap=<span class="string">&quot;jet&quot;</span>, colorbar=<span class="literal">True</span>,</span><br><span class="line">             legend=<span class="literal">True</span>, sharex=<span class="literal">False</span>, figsize=(<span class="number">10</span>, <span class="number">7</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<ul>
<li><em>alpha</em> 表示 density</li>
<li><em>s</em> 表示 circle 大小（和密度二选一），结合 label 在右上角作标注</li>
<li><em>c</em> 表示 color，结合 cmap 和 colorbar</li>
<li><em>sharex</em> 不共享会显示 x 轴</li>
</ul>
<p><img src="https://s2.loli.net/2023/01/31/r4CF8QVdcuKUj6Y.png" alt="image-20230131100647448"></p>
<h4 id="Look-for-Correlations"><a href="#Look-for-Correlations" class="headerlink" title="Look for Correlations"></a>Look for Correlations</h4><p>在统计学中，皮尔逊积矩相关系数（英语：Pearson product-moment correlation coefficient，缩写：PPMCC，或PCCs，有时简称相关系数）用于度量两组数据的变量X和Y之间线性相关的程度。它是两个变量的协方差与其标准差的乘积之比； 因此，它本质上是协方差的归一化度量，因此结果始终具有介于-1和1之间的值。与协方差本身一样，该度量只能反映变量的线性相关性，而忽略了许多其他类型的关系或相关性。举个简单的例子，可以预期高中青少年样本的年龄和身高的皮尔逊积矩相关系数显著大于0，但小于1（因为1表示不切实际的完美相关性）。</p>
<p>注意，相关系数的值 [-1, 1] 和 斜率 (slope) 没有关系。正数代表上升，负数代表下降；<strong>数字绝对值大小代表离散程度</strong>，趋于 1 表示更集中。如下图：</p>
<p><img src="https://s2.loli.net/2023/01/31/qEOn8lrc9w3fhU6.png" alt="image-20230131234418520"></p>
<p>Since the dataset is not too large, you can easily compute the <em>standard correlation coefficient (相关系数)</em> (also called <em>Pearson’s r</em>) between every pair of attributes using the <code>corr() </code>method:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">corr_matrix = housing.corr()</span><br></pre></td></tr></table></figure>

<p>Now you can look at how much each attribute correlates with the <em>median house value</em>:</p>
<p><img src="https://s2.loli.net/2023/01/31/TdUVIOMmt1rqLD5.png" alt="image-20230131110325057"></p>
<p>Coefficients close to 0 mean that there is no linear correlation.</p>
<p>Another way to check for correlation between attributes is to use the Pandas <code>scatter_matrix()</code> function, which plots every numerical attribute against every other numerical attribute. Since there are now 11 numerical attributes, you would get 11^2^  &#x3D; 121 plots, which would not fit on a page—so you decide to focus on a few promising attributes that seem most correlated with the median housing value (Figure 2-14):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas.plotting <span class="keyword">import</span> scatter_matrix</span><br><span class="line"></span><br><span class="line">attributes = [<span class="string">&quot;median_house_value&quot;</span>, <span class="string">&quot;median_income&quot;</span>, <span class="string">&quot;total_rooms&quot;</span>, <span class="string">&quot;housing_median_age&quot;</span>]</span><br><span class="line">scatter_matrix(housing[attributes], figsize=(<span class="number">12</span>, <span class="number">8</span>))</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/01/31/a7xPRzNq3OkZ5oe.png" alt="image-20230131112314968"></p>
<p>从上述散点图中，也可以看出 median_income 和 median_house_value 的线性相关性最强。因此，我们再进一步观察该图：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing.plot(kind=<span class="string">&quot;scatter&quot;</span>, x=<span class="string">&quot;median_income&quot;</span>, y=<span class="string">&quot;median_house_value&quot;</span>,</span><br><span class="line">             alpha=<span class="number">0.1</span>, grid=<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/01/31/Jskz9NnGSeRMfBl.png" alt="image-20230131234544267"></p>
<p>You may want to try removing the corresponding districts (horizontal lines around $500,000 $450,000 $350,000…) to prevent your algorithms from learning to reproduce these data quirks.</p>
<h4 id="Experiment-with-Attribute-Combinations"><a href="#Experiment-with-Attribute-Combinations" class="headerlink" title="Experiment with Attribute Combinations"></a>Experiment with Attribute Combinations</h4><p>特征提取 —— 重要的特征</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing[<span class="string">&quot;rooms_per_house&quot;</span>] = housing[<span class="string">&quot;total_rooms&quot;</span>] / housing[<span class="string">&quot;households&quot;</span>]</span><br><span class="line">housing[<span class="string">&quot;bedrooms_ratio&quot;</span>] = housing[<span class="string">&quot;total_bedrooms&quot;</span>] / housing[<span class="string">&quot;total_rooms&quot;</span>]</span><br><span class="line">housing[<span class="string">&quot;people_per_house&quot;</span>] = housing[<span class="string">&quot;population&quot;</span>] / housing[<span class="string">&quot;households&quot;</span>]</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/01/GIf3U2DOFJqM9Sm.png" alt="image-20230201001227872"></p>
<p>新提取的 bedrooms_ratio 在三者中的相关性最高。</p>
<h3 id="2-5-Prepare-the-Data-for-Machine-Learning-Algorithms"><a href="#2-5-Prepare-the-Data-for-Machine-Learning-Algorithms" class="headerlink" title="2.5 Prepare the Data for Machine Learning Algorithms"></a>2.5 Prepare the Data for Machine Learning Algorithms</h3><p>Note that <code>drop()</code> creates a copy of the data and does not affect strat_train_set:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing = strat_train_set.drop(<span class="string">&quot;median_house_value&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">housing_labels = strat_train_set[<span class="string">&quot;median_house_value&quot;</span>].copy()</span><br></pre></td></tr></table></figure>

<h4 id="Clean-the-Data"><a href="#Clean-the-Data" class="headerlink" title="Clean the Data"></a>Clean the Data</h4><p>对于缺失的特征 total_bedrooms, 有以下三种处理方法：</p>
<ol>
<li>丢弃特征的对应数据</li>
<li>丢弃整个特征列</li>
<li>数据填充</li>
</ol>
<p>代码实现可以通过 Pandas DataFrame’s 的 <code>dropna()</code>, <code>drop()</code>, and <code>fillna()</code> 方法（n&#x2F;a, not avaliable）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing.dropna(subset=[<span class="string">&quot;total_bedrooms&quot;</span>], inplace=<span class="literal">True</span>)  <span class="comment"># option 1</span></span><br><span class="line"></span><br><span class="line">housing.drop(<span class="string">&quot;total_bedrooms&quot;</span>, axis=<span class="number">1</span>)  <span class="comment"># option 2</span></span><br><span class="line"></span><br><span class="line">median = housing[<span class="string">&quot;total_bedrooms&quot;</span>].median()  <span class="comment"># option 3</span></span><br><span class="line">housing[<span class="string">&quot;total_bedrooms&quot;</span>].fillna(median, inplace=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<p>Scikit-Learn 的 SimpleImputer 可以很方便的替代 option 3，它可以计算并存储每个 feature 的 median：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.impute <span class="keyword">import</span> SimpleImputer</span><br><span class="line"></span><br><span class="line">imputer = SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>由于 median 只能作用于数字属性，所以需要先拷贝一份只有数字属性的数据集（即排除了 ocean_proximity）：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing_num = housing.select_dtypes(include=[np.number])</span><br></pre></td></tr></table></figure>

<p>Now you can fit the imputer instance to the training data using the <code>fit()</code> method:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">imputer.fit(housing_num)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/01/aMDPZjVEfxztgwY.png" alt="image-20230201021747709"></p>
<p>Now you can use this “trained” imputer to transform the training set by replacing missing values with the learned medians:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = imputer.transform(housing_num)</span><br></pre></td></tr></table></figure>

<p>Missing values can also be replaced with the mean value <code>(strategy=&quot;mean&quot;)</code>, or with the most frequent value <code>(strategy=&quot;most_frequent&quot;)</code>, or with a constant value <code>(strategy=&quot;constant&quot;, fill_value=…)</code>. The last two strategies support nonnumerical data.</p>
<p>另外，<code>sklearn.impute</code> 下还有两种重要的 imputers（numerical features only）：</p>
<ul>
<li><em>KNNImputer</em> replaces each missing value with the mean of the k-nearest neighbors’ values for that feature. The distance is based on all the available features.</li>
<li><em>IterativeImputer</em> trains a regression model per feature to predict the missing values based on all the other available features. It then trains the model again on the updated data, and repeats the process several times, improving the models and the replacement values at each iteration.</li>
</ul>
<p>Scikit-Learn 相关概念介绍：</p>
<ul>
<li>Estimators: Any object that can estimate some parameters based on a dataset is called an <em>estimator</em> (e.g., a SimpleImputer is an estimator). </li>
<li>Transformers: Some estimators (such as a SimpleImputer) can also transform a dataset; these are called <em>transformers</em>. </li>
<li>Predictors: Some estimators are capable of making predictions.</li>
</ul>
<p>SimpleImputer 通过 <code>fit()</code> 方法训练(计算)出 median (Estimator)；通过 <code>transform()</code> 方法转换掉 na 值 (Transformer).</p>
<p><em>Inspection</em>: All the estimator’s hyperparameters are accessible directly via public instance variables (e.g., imputer.strategy), and all the estimator’s learned parameters are accessible via public instance variables with an underscore suffix (e.g., imputer.statistics_).</p>
<p>Scikit-Learn transformers output NumPy arrays (or sometimes SciPy sparse matrices) even when they are fed Pandas DataFrames as input. So, the output of <code>imputer.transform(housing_num)</code> is a NumPy array: X has neither column names nor index. Luckily, it’s not too hard to wrap X in a DataFrame and recover the column names and index from housing_num:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 重新转为 DataFrame 格式</span></span><br><span class="line">housing_tr = pd.DataFrame(X, columns=housing_num.columns, </span><br><span class="line">			                          index=housing_num.index)</span><br></pre></td></tr></table></figure>

<p>或者，可以直接使用一个 global configuration: <code>sklearn.set_config(pandas_in_out=True)</code></p>
<p>如果需要去除 outliers，可以使用 <code>IsolationForest </code>作异常检测：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> IsolationForest</span><br><span class="line"></span><br><span class="line">isolation_forest = IsolationForest(random_state=<span class="number">42</span>)</span><br><span class="line">outlier_pred = isolation_forest.fit_predict(X)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/06/cixGgr5f9KbT3Oo.png" alt="image-20230206195009454"></p>
<h4 id="Handling-Text-and-Categorical-Attributes"><a href="#Handling-Text-and-Categorical-Attributes" class="headerlink" title="Handling Text and Categorical Attributes"></a>Handling Text and Categorical Attributes</h4><p>So far we have only dealt with numerical attributes, but your data may also contain text attributes. In this dataset, there is just one: the ocean_proximity attribute. Let’s look at its value for the first few instances:</p>
<p><img src="https://s2.loli.net/2023/02/01/6qbQgW8ziZtkrBN.png" alt="image-20230201084005415"></p>
<p>Let’s convert these categories from text to numbers. For this, we can use Scikit-Learn’s <code>OrdinalEncoder</code> class:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"></span><br><span class="line">ordinal_encoder = OrdinalEncoder()</span><br><span class="line">housing_cat_encoded = ordinal_encoder.fit_transform(housing_cat)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/01/uB6P9La72cXKqfS.png" alt="image-20230201085644795"></p>
<p>You can get the list of categories using the <code>categories_</code> instance variable. It is a list containing a 1D array of categories for each categorical attribute (in this case, a list containing a single array since there is just one categorical attribute):</p>
<p><img src="https://s2.loli.net/2023/02/01/jgDhyVMXQP7iqK1.png" alt="image-20230201090652975"></p>
<p>One issue with this representation is that ML algorithms will assume that two nearby values are more similar than two distant values. This may be fine in some cases, but it is obviously not the case for the ocean_proximity column (for example, categories 0 and 4 are clearly more similar than categories 0 and 1). To fix this issue, a common solution is <em>one-hot encoding</em>. The new attributes are sometimes called <em>dummy</em> attributes (zeros). Scikit-Learn provides a OneHotEncoder class to convert categorical values into one-hot vectors:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line">cat_encoder = OneHotEncoder()</span><br><span class="line">housing_cat_1hot = cat_encoder.fit_transform(housing_cat)</span><br></pre></td></tr></table></figure>

<p>By default, the output of a <code>OneHotEncoder</code> is a SciPy <em>sparse matrix</em>, instead of a NumPy array:</p>
<p><img src="https://s2.loli.net/2023/02/01/qFoAb9BXTV3ZjMx.png" alt="image-20230201093455725"></p>
<p>A sparse matrix is a very efficient representation for matrices that contain mostly zeros.  Indeed, internally it only stores the nonzero values and their positions. It will save plenty of memory and speed up computations. You can use a sparse matrix mostly like a normal 2D array, but if you want to convert it to a (dense) NumPy array, just call the <code>toarray()</code> method:</p>
<p><img src="https://s2.loli.net/2023/02/01/rma5YXHychu38Cd.png" alt="image-20230201095214462"></p>
<p>Alternatively, you can set <code>sparse=False</code> when creating the OneHotEncoder, in which case the transform() method will return a regular (dense) NumPy array directly.</p>
<p>Other attributes from estimator using a DataFrame:</p>
<p><img src="https://s2.loli.net/2023/02/01/J3ykGI2AMu1HhOS.png" alt="image-20230201103946728"></p>
<h4 id="Feature-Scaling-and-Transformation"><a href="#Feature-Scaling-and-Transformation" class="headerlink" title="Feature Scaling and Transformation"></a>Feature Scaling and Transformation</h4><p>几个容易混淆的概念：</p>
<ul>
<li><p>Regularization: 约束(简化)模型的正则化技术</p>
</li>
<li><p>Normalization: 数据缩放之归一化 (also called min-max scaling)</p>
</li>
<li><p>Standardization: 数据缩放之标准化</p>
</li>
</ul>
<p>One of the most important transformations you need to apply to your data is <em>feature scaling</em>. With few exceptions, machine learning algorithms don’t perform well when the input numerical attributes have very different scales. This is the case for the housing data: the total number of rooms ranges from about 6 to 39,320, while the median incomes only range from 0 to 15. Without any scaling, most models will be biased toward ignoring the median income and focusing more on the number of rooms.</p>
<p>There are two common ways to get all attributes to have the same scale: <em>min-max</em> <em>scaling</em> (or called <em>normalization</em>) and <em>standardization</em>.</p>
<p>Note that while the training set values will always be scaled to the specified range, if new data contains outliers, these may end up scaled outside the range. If you want to avoid this, just set the <code>clip</code> hyperparameter to True.</p>
<p>Min-max scaling (or normalization) formula:</p>
<p>$f(x)&#x3D;\frac {x-min} {max-min}$ , $f(x)\in[0, 1]$</p>
<p>Scikit-Learn provides a transformer called <code>MinMaxScaler</code> for this. It has a <code>feature_range</code> hyperparameter that lets you change the range if, for some reason, you don’t want 0-1 (e.g., neural networks work best with <strong>zero-mean</strong> inputs, so a range of -1 to 1 is preferable). It’s quite easy to use:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">min_max_scaler = MinMaxScaler(feature_range=(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">housing_num_min_max_scaled = min_max_scaler.fit_transform(housing_num)</span><br></pre></td></tr></table></figure>



<p>Standardization formula:</p>
<p>$f(x) &#x3D; \frac {x - mean} {std}$</p>
<p>标准化有几点不同：</p>
<ol>
<li>因为减去了 mean，所以标准化后的数据的均值为 0</li>
<li>因为除以了 std，所以标准化后的数据的标准差为 1</li>
<li>标准化后的数据值没有强制的特定范围，也因此不受输入数据中 outliers 太大影响</li>
</ol>
<p>从上面 1、2 点可以看出，标准化后的数据就是标准正态分布。</p>
<p>Sciket-Learn provides a transformer called <code>StandardScaler</code> for standardization:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">std_scaler = StandardScaler()</span><br><span class="line">housing_num_std_scaled = std_scaler.fit_transform(housing_num)</span><br></pre></td></tr></table></figure>

<p>If you want to scale a sparse matrix without converting it to a dense matrix first, you can use a <code>StandardScaler</code> with its <code>with_mean </code> hyperparameter set to <code>False</code>: it will only divide the data by the standard deviation, without subtracting the mean (as this would break sparsity).</p>
<p>When a feature’s distribution has a <em>heavy tail</em> (i.e., when values far from the mean are not exponentially rare), both min-max scaling and standardization will squash most values into a small range.</p>
<p><img src="https://s2.loli.net/2023/02/02/7Pm9hXobr3KfVRv.png" alt="image-20230202030402244"></p>
<p>例如，上图中 10000<del>15000 的数据都属于 heavy tail。如果要避免这种情况的影响，可以对数据开平方根或使用 0</del>1 幂的运算；如果是特别严重的情况，可以使用数据的对数 (logarithm, log) 替换。 Figure 2-17 shows how much better this feature looks when you compute its log: it’s very close to a Gaussian distribution (i.e., bell-shaped).</p>
<p>Another approach to handle heavy-tailed features consists in <em>bucketizing (分桶)</em> the feature. This means chopping its distribution into roughly equal-sized buckets, and replacing each feature value with the index of the bucket it belongs to, much like we did to create the <code>income_cat </code> feature (although we only used it for stratified sampling).</p>
<p>So far we’ve only looked at the input features, but the target values (or labels) may also need to be transformed. For example, if the target distribution has a heavy tail, you may choose to replace the target with its logarithm. But if you do, the regression model will now predict the <em>log</em> of the median house value, not the median house value itself. You will need to compute the exponential of the model’s prediction if you want the predicted median house value. </p>
<p>Luckily, most of Scikit-Learn’s transformers have an <code> inverse_transform()</code> method, making it easy to compute the inverse of their transformations. Note that we convert the labels from a Pandas Series to a DataFrame, since the <code>StandardScaler </code> expects 2D inputs.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">target_scaler = StandardScaler()</span><br><span class="line">scaled_labels = target_scaler.fit_transform(housing_labels.to_frame())</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(housing[[<span class="string">&quot;median_income&quot;</span>]], scaled_labels)</span><br><span class="line">some_new_data = housing[[<span class="string">&quot;median_income&quot;</span>]].iloc[:<span class="number">5</span>]</span><br><span class="line"></span><br><span class="line">scaled_predictions = model.predict(some_new_data)</span><br><span class="line">predictions = target_scaler.inverse_transform(scaled_predictions)</span><br></pre></td></tr></table></figure>

<p>或者，有一个更简洁的方法——使用 <code>TransformedTargetRegressor</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> TransformedTargetRegressor</span><br><span class="line"></span><br><span class="line">model = TransformedTargetRegressor(LinearRegression(),</span><br><span class="line">                                  				Transformer=StandardScaler())</span><br><span class="line">model.fit(housing[[<span class="string">&quot;median_income&quot;</span>]], housing_labels)</span><br><span class="line">predictions = model.predict(some_new_data)</span><br></pre></td></tr></table></figure>

<p>它会自动转换 labels，并在预测的时候自动调用 <code>inverse_transform()</code> 方法。</p>
<h4 id="Custom-Transformers"><a href="#Custom-Transformers" class="headerlink" title="Custom Transformers"></a>Custom Transformers</h4><p>Let’s create a log-transformer and apply it to the <code>population</code> feature:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> FunctionTransformer</span><br><span class="line"></span><br><span class="line">log_transformer = FunctionTransformer(np.log, inverse_func=np.exp)  <span class="comment"># exponential</span></span><br><span class="line">log_pop = log_transformer.transform(housing[[<span class="string">&quot;population&quot;</span>]])</span><br></pre></td></tr></table></figure>

<p>For example, here’s how to create a transformer that computes the same Gaussian RBF similarity measure as earlier:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> rbf_kernel</span><br><span class="line"></span><br><span class="line">rbf_transformer = FunctionTransformer(rbf_kernel, </span><br><span class="line">                                     			    kw_args=<span class="built_in">dict</span>(Y=[[<span class="number">35.</span>]], gamma=<span class="number">0.1</span>))</span><br><span class="line">age_simil_35 = rbf_transformer.transform(housing[[<span class="string">&quot;housing_median_age&quot;</span>]])</span><br></pre></td></tr></table></figure>



<h4 id="Transformation-Pipelines"><a href="#Transformation-Pipelines" class="headerlink" title="Transformation Pipelines"></a>Transformation Pipelines</h4><p>Scikit-Learn provides the <code>Pipeline </code>class to help with such sequences of transformations. Here is a small pipeline for numerical attributes, which will first impute then scale the input features:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> Pipeline</span><br><span class="line"></span><br><span class="line">num_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">&quot;impute&quot;</span>, SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>)),</span><br><span class="line">    (<span class="string">&quot;standardize&quot;</span>, StandardScaler()),</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>在 Pipiline 中，所有 estimators 除了最后一个，都必须是 transformers（即都有 fit_transform() 方法）。此外如果需要可视化 pipeline，可以增加代码行 <code>set_config(display=&quot;diagram&quot;)</code>。</p>
<p>如果不想为 estimators 命名，可以使用 <code>make_pipeline()</code> 函数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line">num_pipeline = make_pipeline(SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>), StandardScaler())</span><br></pre></td></tr></table></figure>

<p>The pipeline exposes the same methods as the final estimator. When you call the pipeline’s last estimator’s method, it calls fit_transform() sequentially on all the transformers.</p>
<p><img src="https://s2.loli.net/2023/02/02/eTy2AcmXBoNjM3x.png" alt="image-20230202124604715"></p>
<p>接着，如果想将转换后的 NumPy 数据还原到 DataFrame 格式，可以使用 pipeline 的 <code>get_feature_names_out()</code> 方法：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">df_housing_num_prepared = pd.DataFrame(</span><br><span class="line">	housing_num_prepared,</span><br><span class="line">        columns=num_pipeline.get_feature_names_out(), </span><br><span class="line">        index=housing_num.index</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p><code>ColumnTransformer </code> 可以使用 pipelines 一次性处理所有 columns：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> ColumnTransformer</span><br><span class="line"></span><br><span class="line">num_attribs = [<span class="string">&quot;longitude&quot;</span>, <span class="string">&quot;latitude&quot;</span>, <span class="string">&quot;housing_median_age&quot;</span>, <span class="string">&quot;total_rooms&quot;</span>,</span><br><span class="line"> 			 <span class="string">&quot;total_bedrooms&quot;</span>, <span class="string">&quot;population&quot;</span>, <span class="string">&quot;households&quot;</span>, <span class="string">&quot;median_income&quot;</span>]</span><br><span class="line">cat_attribs = [<span class="string">&quot;ocean_proximity&quot;</span>]</span><br><span class="line"></span><br><span class="line">cat_pipeline = make_pipeline( </span><br><span class="line">    	SimpleImputer(strategy=<span class="string">&quot;most_frequent&quot;</span>), </span><br><span class="line">    	OneHotEncoder(handle_unknown=<span class="string">&quot;ignore&quot;</span>))</span><br><span class="line"></span><br><span class="line">preprocessing = ColumnTransformer([</span><br><span class="line">           (<span class="string">&quot;num&quot;</span>, num_pipeline, num_attribs),</span><br><span class="line">           (<span class="string">&quot;cat&quot;</span>, cat_pipeline, cat_attribs),    </span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>注意，ColumnTransformer 中的参数 <code>remainder=&quot;drop&quot;</code> (default) 表示数据集 columns 中拥有该 transformer 没有定义的 attributes 会直接被 drop。如果要保留原数据中多出来的 columns，可指定 <code>remainder=&quot;passthrough&quot;</code>.</p>
<p>手动列出所有需要转换的列名非常麻烦，可以使用 <code>make_column_selector(dtype_include=)</code>处理。类似的，如果不想命名的话，可以直接使用 make_column_transformer, 下面的 transformer 会被自动命名为 “pipeline-1” and “pipeline-2”:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> make_column_selector, make_comlumn_transformer</span><br><span class="line"></span><br><span class="line">preprocessing = make_column_transformer(</span><br><span class="line">	(num_pipeline, make_column_selector(dtype_include=np.number)),</span><br><span class="line">	(cat_pipeline, make_column_selector(dtype_include=<span class="built_in">object</span>))</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">housing_prepared = preprocessing.fit_transform(housing)</span><br><span class="line"></span><br><span class="line"><span class="comment"># optional: construct DataFrame</span></span><br></pre></td></tr></table></figure>



<p>Description: The <code>OneHotEncoder</code> returns a sparse matrix and the <code>num_pipeline</code> returns a dense matrix. When there is such a mix of sparse and dense matrices, the <code>ColumnTransformer</code> estimates the density of the final matrix (i.e., the ratio of nonzero cells), and it returns a sparse matrix if the <strong>density</strong> is lower than a given threshold (by default, <code>sparse_threshold=0.3</code>). In this example, it returns a dense matrix.</p>
<p>Conclusions：</p>
<ul>
<li>对于缺失值，如果是数字可以 imputed by <code>median</code>…；如果是类别 imputed by <code>most_frequent</code>.</li>
<li>类别可以用 one-hot encode</li>
<li>A few ratio features will be computed and added: <code>bedrooms_ratio</code>, <code>rooms_per_house</code>, and <code>people_per_house</code>. Hopefully these will better correlate with the median house value, and thereby help the ML models.</li>
<li>A few <em>cluster similarity</em> features will also be added. These will likely be more useful to the model than latitude and longitude.</li>
<li>所有的 numerical features 都会被 standardized.</li>
</ul>
<p>The code that builds the pipeline to do all of this should look familiar to you by now:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">column_ratio</span>(<span class="params">X</span>):</span><br><span class="line">    <span class="keyword">return</span> X[:, [<span class="number">0</span>]] / X[:, [<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ratio_name</span>(<span class="params">function_transformer, feature_names_in</span>):</span><br><span class="line">    <span class="keyword">return</span> [<span class="string">&quot;ratio&quot;</span>]  <span class="comment"># feature names out</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">ratio_pipeline</span>():</span><br><span class="line">    <span class="keyword">return</span> make_pipeline(</span><br><span class="line">        SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>),</span><br><span class="line">        FunctionTransformer(column_ratio, feature_names_out=ratio_name),</span><br><span class="line">        StandardScaler())</span><br><span class="line"></span><br><span class="line">log_pipeline = make_pipeline(</span><br><span class="line">    SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>),</span><br><span class="line">    FunctionTransformer(np.log, feature_names_out=<span class="string">&quot;one-to-one&quot;</span>),</span><br><span class="line">    StandardScaler())</span><br><span class="line">cluster_simil = ClusterSimilarity(n_clusters=<span class="number">10</span>, gamma=<span class="number">1.</span>, random_state=<span class="number">42</span>)</span><br><span class="line">default_num_pipeline = make_pipeline(SimpleImputer(strategy=<span class="string">&quot;median&quot;</span>),</span><br><span class="line">                                     StandardScaler())</span><br><span class="line">preprocessing = ColumnTransformer([</span><br><span class="line">        (<span class="string">&quot;bedrooms&quot;</span>, ratio_pipeline(), [<span class="string">&quot;total_bedrooms&quot;</span>, <span class="string">&quot;total_rooms&quot;</span>]),</span><br><span class="line">        (<span class="string">&quot;rooms_per_house&quot;</span>, ratio_pipeline(), [<span class="string">&quot;total_rooms&quot;</span>, <span class="string">&quot;households&quot;</span>]),</span><br><span class="line">        (<span class="string">&quot;people_per_house&quot;</span>, ratio_pipeline(), [<span class="string">&quot;population&quot;</span>, <span class="string">&quot;households&quot;</span>]),</span><br><span class="line">        (<span class="string">&quot;log&quot;</span>, log_pipeline, [<span class="string">&quot;total_bedrooms&quot;</span>, <span class="string">&quot;total_rooms&quot;</span>, <span class="string">&quot;population&quot;</span>,</span><br><span class="line">                               <span class="string">&quot;households&quot;</span>, <span class="string">&quot;median_income&quot;</span>]),</span><br><span class="line">        (<span class="string">&quot;geo&quot;</span>, cluster_simil, [<span class="string">&quot;latitude&quot;</span>, <span class="string">&quot;longitude&quot;</span>]),</span><br><span class="line">        (<span class="string">&quot;cat&quot;</span>, cat_pipeline, make_column_selector(dtype_include=<span class="built_in">object</span>)),</span><br><span class="line">    ],</span><br><span class="line">    remainder=default_num_pipeline)  <span class="comment"># one column remaining: housing_median_age</span></span><br></pre></td></tr></table></figure>

<p>If you run this ColumnTransformer, it performs all the transformations and outputs a NumPy array with 24 features:</p>
<p><img src="https://s2.loli.net/2023/02/03/cmRtl8LNPBrun64.png" alt="image-20230203142825150"></p>
<h3 id="2-6-Select-and-Train-a-Model"><a href="#2-6-Select-and-Train-a-Model" class="headerlink" title="2.6 Select and Train a Model"></a>2.6 Select and Train a Model</h3><h4 id="Train-and-Evaluate-on-the-Training-Set"><a href="#Train-and-Evaluate-on-the-Training-Set" class="headerlink" title="Train and Evaluate on the Training Set"></a>Train and Evaluate on the Training Set</h4><p>LinearRegressor: underfitting</p>
<p>DecisionTreeRegressor: more likely overfitting, but not sure without using test set till you are ready to launch a model you are confident about.  So you need to use part of the training set for training and part of it for model validation. </p>
<h4 id="Better-Evaluation-Using-Cross-Validation"><a href="#Better-Evaluation-Using-Cross-Validation" class="headerlink" title="Better Evaluation Using Cross-Validation"></a>Better Evaluation Using Cross-Validation</h4><p>One way to <strong>evaluate</strong> the decision tree model would be to use the <code>train_test_split()  </code>function to split the training set into a smaller training set and a validation set, then train your models against the smaller training set and evaluate them against the validation set. It’s a bit of effort, but nothing too difficult, and it would work fairly well.</p>
<p>A great alternative is to use Scikit-Learn’s <em>k_-fold cross-validation</em> feature. The following code randomly splits the training set into 10 nonoverlapping subsets called folds, then it trains and evaluates the decision tree model 10 times, picking a different fold for evaluation every time and using the other 9 folds for training. The result is an array containing the 10 evaluation scores:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># cv: cross validation</span></span><br><span class="line">tree_rmses = -cross_val_score(tree_reg, housing, housing_labels, </span><br><span class="line">                             			scoring=<span class="string">&quot;neg_root_mean_squared_error&quot;</span>, cv=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p>Attention: Scikit-Learn’s cross-validation features expect a utility function (greater is better) rather than a cost function (lower is better), so the scoring function is actually the opposite of the RMSE. It’s a negative value, so you need to switch the sign of the output to get the RMSE scores.</p>
<p>看一下运行结果：</p>
<p><img src="https://s2.loli.net/2023/02/03/BfdWctxOKS46F12.png" alt="image-20230203162228506"></p>
<p>最后，再试试最后一个模型：<code>RandomForestRegressor</code>. Random forests work by training many decision trees on random subsets of the features, then averaging out their predictions. Such models composed of many other models are called <em>ensembles</em>: they are capable of boosting the performance of the underlying model (in this case, decision trees). The code is much the same as earlier:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestRegressor</span><br><span class="line"></span><br><span class="line">forest_reg = make_pipeline(preprocessing,</span><br><span class="line"> 					      RandomForestRegressor(random_state=<span class="number">42</span>))</span><br><span class="line">forest_rmses = -cross_val_score(forest_reg, housing, housing_labels,</span><br><span class="line"> 						       scoring=<span class="string">&quot;neg_root_mean_squared_error&quot;</span>, cv=<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/03/i3nNMQbLhpwOg2T.png" alt="image-20230203171438009"></p>
<p>Wow, this is much better: random forests really look very promising for this task! However, if you train a RandomForest and measure the RMSE on the training set, you will find roughly 17,474: that’s much lower, meaning that there’s still quite a lot of overfitting going on. Possible solutions are to simplify the model, constrain it (i.e., regularize it), or get a lot more training data. Before you dive much deeper into random forests, however, you should try out many other models from various categories of machine learning algorithms (e.g., several support vector machines with different kernels, and possibly a neural network), without spending too much time tweaking the hyperparameters. The goal is to shortlist a few (two to five) promising models.</p>
<h3 id="2-7-Fine-Tune-Your-Model"><a href="#2-7-Fine-Tune-Your-Model" class="headerlink" title="2.7 Fine-Tune Your Model"></a>2.7 Fine-Tune Your Model</h3><h4 id="Grid-Search"><a href="#Grid-Search" class="headerlink" title="Grid Search"></a>Grid Search</h4><p>由于超参数搜索需要运行多个模型，因此 grid search 内部也使用了 cross validation 评估模型。下面是搜索 2 个超参数的最佳组合代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">full_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">&quot;preprocessing&quot;</span>, preprocessing),</span><br><span class="line">    (<span class="string">&quot;random_forest&quot;</span>)</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">param_grid = [</span><br><span class="line">     &#123;<span class="string">&#x27;preprocessing__geo__n_clusters&#x27;</span>: [<span class="number">5</span>, <span class="number">8</span>, <span class="number">10</span>],</span><br><span class="line">      <span class="string">&#x27;random_forest__max_features&#x27;</span>: [<span class="number">4</span>, <span class="number">6</span>, <span class="number">8</span>]&#125;,</span><br><span class="line">     &#123;<span class="string">&#x27;preprocessing__geo__n_clusters&#x27;</span>: [<span class="number">10</span>, <span class="number">15</span>],</span><br><span class="line">      <span class="string">&#x27;random_forest__max_features&#x27;</span>: [<span class="number">6</span>, <span class="number">8</span>, <span class="number">10</span>]&#125;,</span><br><span class="line">]</span><br><span class="line">grid_search = GridSearchCV(full_pipeline, param_grid, cv=<span class="number">3</span>, </span><br><span class="line">                           			  scoring=<span class="string">&#x27;neg_root_mean_squared_error&#x27;</span>)</span><br><span class="line">grid_search.fit(housing, housing_labels)</span><br></pre></td></tr></table></figure>

<p>以两个下划线为后缀的是 transformer 的命名。Combinations: 3 * 3 + 2 * 3 &#x3D; 15. It will train the pipeline 3 times per combination, since we are using 3-fold cross validation. 因此总共需要训练 15 * 3 &#x3D; 45 轮！</p>
<p><img src="https://s2.loli.net/2023/02/04/pytT42RvbM9QlPj.png" alt="image-20230204132710633"></p>
<p>Suggestion: Since 15 is the maximum value that was evaluated for <code>n_clusters</code>, you should probably try searching again with higher values; the score may continue to improve.</p>
<p>可以通过 <code>grid_search.best_estimator_</code> 直接访问最佳 estimator；评估结果可以通过 <code>grid_search.cv_results_</code> 获取，默认是 dictionary 格式，可以转换为 DataFrame 更直观：</p>
<p><img src="https://s2.loli.net/2023/02/04/pgPrvNESu3JjV1s.png" alt="image-20230204133732114"></p>
<p>RMSE score 为 44042，比之前的 47019 好一点了。</p>
<h4 id="Randomized-Search"><a href="#Randomized-Search" class="headerlink" title="Randomized Search"></a>Randomized Search</h4><p>这个类用起来与 GridSearchCV 类大致相同，但它不会尝试所有 combinations，而是在每次迭代中为每个超参数选择一个随机值，然后对一定数量的随机组合进行评估。这种方法有两个显著好处：</p>
<ul>
<li>如果运行随机搜索 1000 次迭代，那么将会探索每个超参数的1000个不同的值（网格搜索需要手动列出 1000 个参数值）。</li>
<li>通过简单地设置迭代次数，可以更好地控制训练次数。</li>
<li>假设你不知道其中有一个超参数对模型的性能完全没有影响，普通的 grid search 依旧会训练完所有该参数值，而 random search 能感知到。</li>
</ul>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> randint</span><br><span class="line"></span><br><span class="line">param_distribs = &#123; <span class="string">&#x27;preprocessing__geo__n_clusters&#x27;</span>: randint(low=<span class="number">3</span>, high=<span class="number">50</span>),</span><br><span class="line"> 				<span class="string">&#x27;random_forest__max_features&#x27;</span>: randint(low=<span class="number">2</span>, high=<span class="number">20</span>)&#125;</span><br><span class="line"></span><br><span class="line">rnd_search = RandomizedSearchCV(</span><br><span class="line"> 	  full_pipeline, param_distributions=param_distribs, n_iter=<span class="number">10</span>, cv=<span class="number">3</span>,</span><br><span class="line">	 scoring=<span class="string">&#x27;neg_root_mean_squared_error&#x27;</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">rnd_search.fit(housing, housing_labels)</span><br></pre></td></tr></table></figure>

<p>此外，Scikit-Learn 还有 <code>HalvingRandomSearchCV</code> 和 <code>HalvingGridSearchCV</code>. 也是适用于大范围高效搜索。通过小训练集（逐渐变大）来筛选 half 参数组合：</p>
<p><img src="https://s2.loli.net/2023/02/04/KUcWGY7xFkRP8dI.png" alt="image-20230204142522443"></p>
<h4 id="Ensemble-Methods"><a href="#Ensemble-Methods" class="headerlink" title="Ensemble Methods"></a>Ensemble Methods</h4><p>We will cover this topic in more detail in Chapter 7.</p>
<h4 id="Analyzing-the-Best-Models-and-Their-Errors"><a href="#Analyzing-the-Best-Models-and-Their-Errors" class="headerlink" title="Analyzing the Best Models and Their Errors"></a>Analyzing the Best Models and Their Errors</h4><p>可以通过 <code>feature_importances_</code> 观察模型，并考虑丢弃重要性不高的特征：</p>
<p><img src="https://s2.loli.net/2023/02/04/3KHRYyZ2sOxNMuf.png" alt="image-20230204144434107"></p>
<p>Display them next to their corresponding attribute names:</p>
<p><img src="https://s2.loli.net/2023/02/04/OX3pRY5ciuIQqwT.png" alt="image-20230204144445215"></p>
<p>With this information, you may want to try dropping some of the less useful features (e.g., apparently only one ocean_proximity category is really useful, so you could try dropping the others).</p>
<p>Suggestion: The <code>sklearn.feature_selection.SelectFromModel</code> transformer can automatically drop the least useful features for you: when you fit it, it trains a model (typically a random forest), looks at its <code>feature_importances_</code> attribute, and selects the most useful features. Then when you call <code>transform()</code>, it drops the other features.</p>
<p>Now is also a good time to ensure that your model not only works well on average, but also on all categories of districts, whether they’re rural or urban, rich or poor, northern or southern, minority or not, etc. Creating subsets of your validation set for each category takes a bit of work, but it’s important: if your model performs poorly on a whole category of districts, then it should probably not be deployed until the issue is solved, or at least it should not be used to make predictions for that category, as it may do more harm than good.</p>
<h4 id="Evaluate-Your-System-on-the-Test-Set"><a href="#Evaluate-Your-System-on-the-Test-Set" class="headerlink" title="Evaluate Your System on the Test Set"></a>Evaluate Your System on the Test Set</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_test = strat_test_set.drop(<span class="string">&quot;median_house_value&quot;</span>, axis=<span class="number">1</span>)</span><br><span class="line">y_test = strat_test_set[<span class="string">&quot;median_house_value&quot;</span>].copy()</span><br><span class="line"></span><br><span class="line">final_predictions = final_model.predict(X_test)</span><br><span class="line"></span><br><span class="line">final_rmse = mean_squared_error(y_test, final_predictions, squared=<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(final_rmse)  <span class="comment"># prints 41424.40026462184</span></span><br></pre></td></tr></table></figure>

<p>In some cases, such a point estimate of the generalization error will not be quite enough to convince you to launch: what if it is just 0.1% better than the model currently in production? You might want to have an idea of how precise this estimate is. For this, you can compute a 95% <em>confidence interval</em> for the generalization error using <code>scipy.stats.t.interval()</code>. You get a fairly large interval from 39,275 to 43,467, and your previous point estimate of 41,424 is roughly in the middle of it:</p>
<p><img src="https://s2.loli.net/2023/02/04/cws21lm7ZBEp3oS.png" alt="image-20230204152137734"></p>
<h3 id="2-8-Launch-Monitor-and-Maintain-Your-System"><a href="#2-8-Launch-Monitor-and-Maintain-Your-System" class="headerlink" title="2.8 Launch, Monitor, and Maintain Your System"></a>2.8 Launch, Monitor, and Maintain Your System</h3><p>To save the model, you can use the <code>joblib</code> library like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> joblib</span><br><span class="line"></span><br><span class="line">joblib.dump(final_model, <span class="string">&quot;my_california_housing_model.pkl&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>Suggestion: It’s often a good idea to save <strong>every model</strong> you experiment with so that you can come back easily to any model you want. You may also save the cross-validation scores and perhaps the actual predictions on the validation set. This will allow you to easily compare scores across model types, and compare the types of errors they make.</p>
<p>Here are a few things you can automate:</p>
<ul>
<li>Collect fresh data regularly and label it (e.g., using human raters).</li>
<li>Write a script to train the model and fine-tune the hyperparameters automatically. This script could run automatically, for example every day or every week, depending on your needs.</li>
<li>Write another script that will evaluate both the new model and the previous model on the updated test set, and deploy the model to production if the performance has not decreased (if it did, make sure you investigate why). The script should probably test the performance of your model on various subsets of the test set, such as poor or rich districts, rural or urban districts, etc.</li>
</ul>
<p>监控数据质量的 metrics 可以是：whether missing features; standard deviation drifts too far from the training set;  categorical feature starts containing new categories, etc.</p>
<p>最后，记得备份每个模型和每个版本的 datasets！</p>
<h3 id="2-x-Exercises"><a href="#2-x-Exercises" class="headerlink" title="2.x Exercises"></a>2.x Exercises</h3><h2 id="3-Classification"><a href="#3-Classification" class="headerlink" title="3. Classification"></a>3. Classification</h2><h3 id="3-1-MNIST"><a href="#3-1-MNIST" class="headerlink" title="3.1 MNIST"></a>3.1 MNIST</h3><p>The <code>sklearn.datasets</code> package contains mostly three types of functions: <code>fetch_*</code> functions such as fetch_openml() to download real-life datasets, <code>load_*</code> functions to load small toy datasets bundled with Scikit-Learn (so they don’t need to be downloaded over the internet), and <code>make_*</code> functions to generate fake datasets, useful for tests. In this case, we use <code>fetch_openml()</code> to get MNIST:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line"></span><br><span class="line">mnist = fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>, as_frame=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>

<p>Generated datasets are usually returned as an (X, y) tuple containing the input data and the targets, both as NumPy arrays. Other datasets are returned as sklearn.utils.Bunch objects, which are dictionaries whose entries can also be accessed as attributes. They generally contain the following entries (you can inspect more entires by <code>mnist.keys()</code>):</p>
<ul>
<li><p>DESC</p>
<p>A description of the dataset</p>
</li>
<li><p>data</p>
<p>The input data, usually as a 2D NumPy array</p>
</li>
<li><p>target</p>
<p>The labels, usually as a 1D NumPy array</p>
</li>
</ul>
<p><img src="https://s2.loli.net/2023/02/09/QmuUX321jq4rI9H.png" alt="image-20230209181007430"></p>
<p>The <code>fetch_openml()</code> function is a bit unusual since by default it returns the inputs as a Pandas DataFrame and the labels as a Pandas Series (unless the dataset is sparse). But the MNIST dataset contains images, and DataFrames aren’t ideal for that, so it’s preferable to set <code>as_frame=False</code> to get the data as NumPy arrays instead. Let’s look at these arrays:</p>
<p><img src="https://s2.loli.net/2023/02/09/AU5qy8erLEpKsPk.png" alt="image-20230209170253351"></p>
<p>There are 70,000 images, and each image has 784 features. This is because each image is 28 × 28 pixels, and each feature simply represents one pixel’s intensity, from 0 (white) to 255 (black). Let’s take a peek at one digit from the dataset: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">pic = X[<span class="number">0</span>].reshape(<span class="number">28</span>, <span class="number">28</span>)</span><br><span class="line">plt.imshow(pic, cmap=<span class="string">&quot;binary&quot;</span>)</span><br><span class="line">plt.axis(<span class="string">&quot;off&quot;</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Target:&quot;</span>, y[<span class="number">0</span>])</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/09/JBXOn7TUNkQGtV6.png" alt="image-20230209171411084"></p>
<p>此外，该数据集已乱序，所以不需要 shuffle 操作。</p>
<h3 id="3-2-Train-a-Binary-Classifier"><a href="#3-2-Train-a-Binary-Classifier" class="headerlink" title="3.2 Train a Binary Classifier"></a>3.2 Train a Binary Classifier</h3><p>This “5-detector” will be an example of a binary classifier, capable of distinguishing between just two classes, 5 and non-5. First we’ll create the target vectors for this classification task:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train_5 = (y_train == <span class="string">&#x27;5&#x27;</span>)  <span class="comment"># True for all 5s, False for all other digits</span></span><br><span class="line">y_test_5 = (y_test == <span class="string">&#x27;5&#x27;</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDClassifier</span><br><span class="line"></span><br><span class="line">sgd_clf = SGDClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">sgd_clf.fit(X_train, y_train_5)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sgd_clf.predict([X[<span class="number">0</span>]])</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([ True])</p>
<h3 id="3-3-Performance-Measures"><a href="#3-3-Performance-Measures" class="headerlink" title="3.3 Performance Measures"></a>3.3 Performance Measures</h3><h4 id="Measuring-Accuracy-Using-Cross-Validation"><a href="#Measuring-Accuracy-Using-Cross-Validation" class="headerlink" title="Measuring Accuracy Using Cross-Validation"></a>Measuring Accuracy Using Cross-Validation</h4><p>Let’s look at a dummy classifier that just classifies every single image in the most frequent class, which in this case is the negative class (i.e., non 5):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.dummy <span class="keyword">import</span> DummyClassifier</span><br><span class="line"></span><br><span class="line">dummy_clf = DummyClassifier()</span><br><span class="line">dummy_clf.fit(X_train, y_train_5)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">any</span>(dummy_clf.predict(X_train)))  <span class="comment"># prints False: no 5s detected</span></span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/09/WyoihMZFQs8xmvT.png" alt="image-20230209205037761"></p>
<p>因为数据集中非数字 5 的图片占比 90% 以上，所以不管如何预测，正确率总会很高。这也说明了 accuracy 在不总是衡量分类器性能的最佳指标，特别是对 <em>skewed datasets</em> 更是如此。A much better way to evaluate the performance of a classifier is to look at the <em>confusion matrix</em> (CM).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># implement cross validation by yourself</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line"><span class="keyword">from</span> sklearn.base <span class="keyword">import</span> clone</span><br><span class="line"></span><br><span class="line">skfolds = StratifiedKFold(n_splits=<span class="number">3</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> train_index, test_index <span class="keyword">in</span> skfolds.split(X_train, y_train_5):</span><br><span class="line">    clone_clf = clone(sgd_clf)</span><br><span class="line">    X_train_folds = X_train[train_index]</span><br><span class="line">    y_train_folds = y_train_5[train_index]</span><br><span class="line">    X_test_fold = X_train[test_index]</span><br><span class="line">    y_test_fold = y_train_5[test_index]</span><br><span class="line"></span><br><span class="line">    clone_clf.fit(X_train_folds, y_train_folds)</span><br><span class="line">    y_pred = clone_clf.predict(X_test_fold)</span><br><span class="line">    n_correct = <span class="built_in">sum</span>(y_pred == y_test_fold)</span><br><span class="line">    <span class="built_in">print</span>(n_correct / <span class="built_in">len</span>(y_pred))</span><br></pre></td></tr></table></figure>



<h4 id="Confusion-Matrices"><a href="#Confusion-Matrices" class="headerlink" title="Confusion Matrices"></a>Confusion Matrices</h4><p>To compute the confusion matrix, you first need to have a set of predictions so that they can be compared to the actual targets. You could make predictions on the test set, but it’s best to keep that untouched for now (remember that you want to use the test set only at the very end of your project, once you have a classifier that you are ready to launch). Instead, you can use the <code>cross_val_predict()</code> function:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> cross_val_predict</span><br><span class="line"></span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>)</span><br></pre></td></tr></table></figure>

<p>Instead of returning the evaluation scores, it returns the predictions made on each test fold. The model makes predictions on data that it never saw during training.</p>
<p>现在，可以直接使用 <code>confusion_matrix()</code>:</p>
<p><img src="https://s2.loli.net/2023/02/09/AxHX3Y8fW52atDC.png" alt="image-20230209220235368"></p>
<p>Each row in a confusion matrix represents an <em>actual class</em>, while each column represents a <em>predicted class</em>. </p>
<figure class="highlight plaintext"><table><tr><td class="code"><pre><span class="line">[[TN, FP], </span><br><span class="line"> [FN, TP]]</span><br></pre></td></tr></table></figure>

<table>
<thead>
<tr>
<th>TN: True Negatives</th>
<th>FP: False Positives (also called <em>type I errors</em>)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>FN: False Negatives (also call <em>type II errors</em>)</strong></td>
<td><strong>TP: True Positives</strong></td>
</tr>
</tbody></table>
<p>A perfect classifier would only have true positives and true negatives, so its confusion matrix would have nonzero values only on its main diagonal (top left to bottom right):</p>
<p><img src="https://s2.loli.net/2023/02/09/eSgWQhM2fpnIU9K.png" alt="image-20230209230757920"></p>
<p>The confusion matrix gives you a lot of information, but sometimes you may prefer a more concise metric. An interesting one to look at is the <strong>accuracy</strong> of the <strong>positive</strong> predictions; this is called the <em>precision (精度)</em> of the classifier (Equation 3-1).</p>
<p>​	<em>Equation 3-1. Precision</em></p>
<p>​	$precision &#x3D; \frac {TP} {TP + FP}$</p>
<p>A trivial way to have perfect precision is to create a classifier that always makes negative predictions, except for one single positive prediction on the instance it’s most confident about. If this one prediction is correct, then the classifier has 100% precision (precision &#x3D; 1&#x2F;1 &#x3D; 100%). Obviously, such a classifier would not be very useful, since it would ignore all but one positive instance. So, precision is typically used along with another metric named <em>recall (召回率)</em>, also called <em>sensitivity</em> or the <em>true positive rate</em> (TPR): this is the ratio of positive instances that are correctly detected by the classifier (Equation 3-2).</p>
<p>​	<em>Equation 3-2. Recall</em></p>
<p>​	$recall &#x3D;  \frac {TP} {TP+FN}$</p>
<p>If you are confused about the confusion matrix, Figure 3-3 may help.</p>
<p><img src="https://s2.loli.net/2023/02/09/KR6yHVu8XqwdT3f.png" alt="image-20230209232314933"></p>
<h4 id="Precision-and-Recall"><a href="#Precision-and-Recall" class="headerlink" title="Precision and Recall"></a>Precision and Recall</h4><p>Scikit-Learn provides several functions to compute classifier metrics, including precision and recall:</p>
<p><img src="https://s2.loli.net/2023/02/09/LepwDZ8WMu7mb4X.png" alt="image-20230209233602901"></p>
<p>It is often convenient to combine precision and recall into a single metric called the F<del>1</del> score, especially when you need a single metric to compare two classifiers. The F<del>1</del> score is the <em>harmonic mean</em> of precision and recall (Equation 3-3). Whereas the regular mean treats all values equally, the harmonic mean gives much more weight to low values. As a result, the classifier will only get a high F<del>1</del> score if both recall and precision are high.</p>
<p>​	<em>Equation 3-3. F<del>1</del> score</em></p>
<p>​	$F_1 &#x3D; \frac 2 {\frac 1 {precision} + \frac 1 {recall}} &#x3D; 2 \times \frac {precision \times recall} {precision + recall} &#x3D; \frac {TP} {TP + \frac {FN + FP} 2}$</p>
<p>To compute the F1 score, simply call the <code>f1_score()</code> function:</p>
<p><img src="https://s2.loli.net/2023/02/10/tvpXOJ17coDCKTl.png" alt="image-20230210001016392"></p>
<p>The F1 score favors classifiers that have similar precision and recall. This is not always what you want: in some contexts you mostly care about precision, and in other contexts you really care about recall. </p>
<p>Unfortunately, you can’t have it both ways: increasing precision reduces recall, and vice versa. This is called the <em>precision&#x2F;recall trade-off</em>.</p>
<h4 id="The-Precision-Recall-Trade-off"><a href="#The-Precision-Recall-Trade-off" class="headerlink" title="The Precision&#x2F;Recall Trade-off"></a>The Precision&#x2F;Recall Trade-off</h4><p>To understand this trade-off, let’s look at how the <code>SGDClassifier</code> makes its classification decisions. For each instance, it computes a score based on a <em>decision function (决策函数)</em>.</p>
<p><img src="https://s2.loli.net/2023/02/10/rfqJV6U5YbwTR84.png" alt="image-20230210092048779"></p>
<p>从上图可以看出，提高 thresholds 可以<strong>总体上</strong>增加 precision，但相反的绝对会降低 recall. 在代码中，可以通过 <code>decision_function</code> 获取预测结果的决策函数值 (注意 threshold 值是未知)：</p>
<p><img src="https://s2.loli.net/2023/02/10/cL12v6aXRnx9bV7.png" alt="image-20230210093821251"></p>
<p>How do you decide which threshold to use? First, use the <code>cross_val_predict()</code> function to get the scores of all instances in the training set, but this time specify that you want to return decision scores instead of predictions:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_scores = cross_val_predict(sgd_clf, X_train, y_train_5, cv=<span class="number">3</span>, </span><br><span class="line">                             		     method=<span class="string">&quot;decision_function&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>得到所有 y_scores 决策分数后，即可使用 <code>precision_recall_curve()</code> 函数获取 precisions, recalls, thresholds: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">threshold = <span class="number">3000</span></span><br><span class="line"></span><br><span class="line">plt.plot(thresholds, precisions[:-<span class="number">1</span>], <span class="string">&quot;b--&quot;</span>, label=<span class="string">&quot;Precision&quot;</span>)</span><br><span class="line">plt.plot(thresholds, recalls[:-<span class="number">1</span>], <span class="string">&quot;g-&quot;</span>, label=<span class="string">&quot;Recall&quot;</span>)</span><br><span class="line">plt.vlines(threshold, <span class="number">0</span>, <span class="number">1.0</span>, <span class="string">&#x27;k&#x27;</span>, <span class="string">&quot;dotted&quot;</span>, label=<span class="string">&quot;Threshold&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.axis([-<span class="number">50000</span>, <span class="number">50000</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend(loc=<span class="string">&quot;center right&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/10/qQLeZzG4R5jyin3.png" alt="image-20230210103107012"></p>
<p>At this threshold value, precision is near 90% and recall is around 50%. Another way to select a good precision&#x2F;recall trade-off is to plot precision directly against recall, as shown in Figure 3-6 (the same threshold is shown):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">plt.plot(recalls, precisions, label=<span class="string">&quot;Precision/Recall curve&quot;</span>)</span><br><span class="line"></span><br><span class="line">idx = (thresholds &gt;= threshold).argmax()</span><br><span class="line">plt.plot([recalls[idx]], [precisions[idx]], <span class="string">&quot;ko&quot;</span>,</span><br><span class="line">         label=<span class="string">&quot;Point at threshold 3,000&quot;</span>)</span><br><span class="line">plt.plot([recalls[idx], recalls[idx]], [<span class="number">0.</span>, precisions[idx]], <span class="string">&quot;k:&quot;</span>)</span><br><span class="line">plt.plot([<span class="number">0.0</span>, recalls[idx]], [precisions[idx], precisions[idx]], <span class="string">&quot;k:&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.axis([<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/10/YBdE6hGVu4QvMN2.png" alt="image-20230210131934436"></p>
<p>Suppose you decide to aim for 90% precision. You could use the first plot to find the threshold you need to use, but that’s not very precise. Alternatively, you can search for the lowest threshold that gives you at least 90% precision. For this, you can use the NumPy array’s <code>argmax()</code> method. This returns the first index of the maximum value, which in this case means the first <code>True</code> value:</p>
<p><img src="https://s2.loli.net/2023/02/10/ZihUDjOuc2IfsW6.png" alt="image-20230210113649267"></p>
<p>To make predictions (on the training set for now), instead of calling the classifier’s <code>predict()</code> method, you can run this code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train_pred_90 = (y_scores &gt;= threshold_for_90_precision)</span><br></pre></td></tr></table></figure>

<p>Let’s check these predictions’ precision and recall:</p>
<p><img src="https://s2.loli.net/2023/02/10/Axlf4rJGOUtS3e5.png" alt="image-20230210113835821"></p>
<p>Suggestion: If someone says, “Let’s reach 99% precision”, you should ask, “At what recall?”</p>
<h4 id="The-ROC-Curve"><a href="#The-ROC-Curve" class="headerlink" title="The ROC Curve"></a>The ROC Curve</h4><p>The receiver operating characteristic (ROC) curve is another common tool used with binary classifiers. TPR: recall , FPR: fall-out.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> roc_curve</span><br><span class="line"></span><br><span class="line">fpr, tpr, thresholds = roc_curve(y_train_5, y_scores)</span><br><span class="line"></span><br><span class="line">plt.plot(fpr, tpr, linewidth=<span class="number">2</span>, label=<span class="string">&quot;ROC curve&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/10/PGVlxMJiOLv1TIg.png" alt="image-20230210131303371"></p>
<p>One way to compare classifiers is to measure the <em>area under the curve</em> (AUC). A perfect classifier will have a ROC AUC equal to 1, whereas a purely random classifier will have a ROC AUC equal to 0.5. Scikit-Learn provides a function to estimate the ROC AUC:</p>
<p><img src="https://s2.loli.net/2023/02/10/KJ4SswgIElbfCq7.png" alt="image-20230210131511245"></p>
<p>Suggestion: Since the ROC curve is so similar to the precision&#x2F;recall (PR) curve, you may wonder how to decide which one to use. As a rule of thumb, you should prefer the PR curve whenever the positive class is rare or when you care more about the false positives than the false negatives. Otherwise, use the ROC curve. For example, looking at the previous ROC curve (and the ROC AUC score), you may think that the classifier is really good. But this is mostly because there are few positives (5s) compared to the negatives (non-5s). In contrast, the PR curve makes it clear that the classifier has room for improvement: the curve could really be closer to the top-right corner (see Figure 3-6 again).</p>
<p>Let’s now create a <code>RandomForestClassifier</code>, whose <code>PR</code> curve and <code>F</code><del>1</del> score we can compare to those of the <code>SGDClassifier</code>. the RandomForestClassifier class does not have a <code>decision_function()</code> method, due to the way it works (we will cover this in Chapter 7). Luckily, it has a <code>predict_proba()</code> method that returns class probabilities for each instance:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">forest_clf = RandomForestClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">y_probas_forest = cross_val_predict(forest_clf, X_train, y_train_5,</span><br><span class="line">                                    cv=<span class="number">3</span>, method=<span class="string">&quot;predict_proba&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>Let’s look at the class probabilities for the first two images in the training set:</p>
<p><img src="https://s2.loli.net/2023/02/10/WbCR7UnLfGEtx1K.png" alt="image-20230210132804078"></p>
<p>The model predicts that the first image is positive with 89% probability, and it predicts that the second image is negative with 99% probability. </p>
<p>绘制 PR 曲线：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_scores_forest = y_probas_forest[:, <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">precisions_forest, recalls_forest, thresholds_forest = \</span><br><span class="line">    precision_recall_curve(y_train_5, y_probas_forest[:, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line">plt.plot(recalls, precisions, <span class="string">&quot;b:&quot;</span>, label=<span class="string">&quot;SGD&quot;</span>)</span><br><span class="line">plt.plot(recalls_forest, precisions_forest, <span class="string">&quot;b-&quot;</span>, label=<span class="string">&quot;Random Forest&quot;</span>)</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">&quot;Recall&quot;</span>)</span><br><span class="line">plt.ylabel(<span class="string">&quot;Precision&quot;</span>)</span><br><span class="line">plt.grid()</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/10/DxgybqN2LctO45G.png" alt="image-20230210135146260"></p>
<p>Its F<del>1</del> score and ROC AUC score are also significantly better:</p>
<p><img src="https://s2.loli.net/2023/02/10/yUQkVbEMJcztl36.png" alt="image-20230210135453068"></p>
<h3 id="3-4-Multiclass-Classification"><a href="#3-4-Multiclass-Classification" class="headerlink" title="3.4 Multiclass Classification"></a>3.4 Multiclass Classification</h3><p>Some Scikit-Learn classifiers (e.g., <code>LogisticRegression</code>, <code>RandomForestClassifier</code>, and <code>GaussianNB</code>) are capable of handling multiple classes natively. Others are strictly binary classifiers (e.g., <code>SGDClassifier </code>and <code>SVC</code>). However, there are various strategies (OvO <em>one-versus-one</em>, OvR <em>one-versus-the-rest</em>) that you can use to perform multiclass classification with multiple binary classifiers.</p>
<p>Scikit-Learn detects when you try to use a binary classification algorithm for a multiclass classification task, and it automatically runs OvR or OvO, depending on the algorithm. Here is a support vector machine classifier runing OvO by default.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 45 classfiers</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">svm_clf = SVC(random_state=<span class="number">42</span>)</span><br><span class="line">svm_clf.fit(X_train[:<span class="number">2000</span>], y_train[:<span class="number">2000</span>])</span><br><span class="line"></span><br><span class="line">some_digit_scores = svm_clf.decision_function([some_digit])  <span class="comment"># ten scores</span></span><br><span class="line">class_id = some_digit_scores.argmax()</span><br><span class="line">svm_clf.classes_[class_id]</span><br></pre></td></tr></table></figure>

<p>可以使用 <code>OneVsRestClassifier </code> 来运行 OvR：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 10 classifiers</span></span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line">ovr_clf = OneVsRestClassifier(SVC(random_state=<span class="number">42</span>))</span><br><span class="line">ovr_clf.fit(X_train[:<span class="number">2000</span>], y_train[:<span class="number">2000</span>])</span><br></pre></td></tr></table></figure>

<p>Training an <code>SGDClassifier</code> on a multiclass dataset and using it to make predictions is just as easy:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sgd_clf = SGDClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">sgd_clf.fit(X_train, y_train)</span><br><span class="line">sgd_clf.predict([some_digit])</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([‘3’], dtype&#x3D;’&lt;U1’)</p>
<p>但预测结果错误了，观察它的决策分数可以发现较前面的都低很多：</p>
<p><img src="https://s2.loli.net/2023/02/10/tUiZ5WhL3VXTeHx.png" alt="image-20230210162924592"></p>
<p><img src="https://s2.loli.net/2023/02/10/LeJOADta1WHxlR3.png" alt="image-20230210162933518"></p>
<p>但是准确率还可以，进一步用 scaler 优化准确率可以提高：</p>
<p><img src="https://s2.loli.net/2023/02/10/KDBbRwFJAPrlumt.png" alt="image-20230210163103551"></p>
<h3 id="3-5-Error-Analysis"><a href="#3-5-Error-Analysis" class="headerlink" title="3.5 Error Analysis"></a>3.5 Error Analysis</h3><p>因为有 10 个类，所以直接计算出的混淆矩阵会很大。为了更直观地观察 confusion matrix，可以使用 ConfusionMatrixDisplay 函数直接绘制出来：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> ConfusionMatrixDisplay</span><br><span class="line"></span><br><span class="line">y_train_pred = cross_val_predict(sgd_clf, X_train_scaled, y_train, cv=<span class="number">3</span>)</span><br><span class="line">ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>如果想转换为百分比，可以指定 <code>normalize=&quot;true&quot;</code> 和 <code>values_format=&quot;.0%&quot;</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,</span><br><span class="line">                                        			  normalize=<span class="string">&quot;true&quot;</span>, values_format=<span class="string">&quot;.0%&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p>如果想更直观的观察错误：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sample_weight = (y_train_pred != y_train)</span><br><span class="line">ConfusionMatrixDisplay.from_predictions(y_train, y_train_pred,</span><br><span class="line">                                                                  sample_weight=sample_weight,</span><br><span class="line">                                                                  normalize=<span class="string">&quot;true&quot;</span>, values_format=<span class="string">&quot;.0%&quot;</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/10/9HDb8KB3RucO1Is.png" alt="image-20230210173101941"></p>
<p>从上述图像可以看出许多数字被错误的归类为 8。可以使用一些办法来缓解该错误。例如，可以收集一些看起来像 8 但实际上不是的图片，这样分类器就能跟真正的 8 区别开来。又或者，可以写一个算法来计算圆圈的个数，8 有两个，6 有一个，5 没有。Or you could preprocess the images (e.g., using Scikit-Image, Pillow, or OpenCV) to make some patterns, such as closed loops, stand out more.</p>
<p>Analyzing individual errors can also be a good way to gain insights into what your classifier is doing and why it is failing. For example, let’s plot examples of 3s and 5s in a confusion matrix style (Figure 3-11):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">cl_a, cl_b = <span class="string">&#x27;3&#x27;</span>, <span class="string">&#x27;5&#x27;</span></span><br><span class="line">X_aa = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_ab = X_train[(y_train == cl_a) &amp; (y_train_pred == cl_b)]</span><br><span class="line">X_ba = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_a)]</span><br><span class="line">X_bb = X_train[(y_train == cl_b) &amp; (y_train_pred == cl_b)]</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/10/UEyLAaWe8JGX65V.png" alt="image-20230210180230532"></p>
<h3 id="3-6-Multilabel-Classification"><a href="#3-6-Multilabel-Classification" class="headerlink" title="3.6 Multilabel Classification"></a>3.6 Multilabel Classification</h3><p>Consider a face-recognition classifier: what should it do if it recognizes several people in the same picture?  It should attach one tag (label) per person it recognizes. </p>
<p>直接使用 <code>np.c_(y_1, y_2)</code> concatenate labels, 并使用 <code>KNeighborsClassifier</code> 来接受二维 <code>y_multilabel</code> 即可：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">y_train_large = (y_train &gt;= <span class="string">&#x27;7&#x27;</span>)</span><br><span class="line">y_train_odd = (y_train.astype(np.int8) % <span class="number">2</span> == <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">y_multilabel = np.c_[y_train_large, y_train_odd]</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train, y_multilabel)</span><br></pre></td></tr></table></figure>

<p>There are many ways to evaluate a multilabel classifier, and selecting the right metric really depends on your project. One approach is to measure the F<del>1</del> score for each individual label (or any other binary classifier metric discussed earlier), then simply compute the average score. The following code computes the average F1 score across all labels:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">y_train_knn_pred = cross_val_predict(knn_clf, X_train, y_multilabel, cv=<span class="number">3</span>)</span><br><span class="line">f1_score(y_multilabel, y_train_knn_pred, average=<span class="string">&quot;macro&quot;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 0.976410265560605</span></span><br></pre></td></tr></table></figure>

<p>对于那些无法接受 multilabel 多维数据参数的分类器如 SVC, SGD 等，可以使用 <code>sklearn.multioutput.ChainClassifier</code> 进行包装：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.multioutput <span class="keyword">import</span> ClassifierChain</span><br><span class="line"></span><br><span class="line">chain_clf = ClassifierChain(SVC(), cv=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line">chain_clf.fit(X_train, y_multilabel)</span><br></pre></td></tr></table></figure>







<h3 id="3-7-Multioutput-Classification"><a href="#3-7-Multioutput-Classification" class="headerlink" title="3.7 Multioutput Classification"></a>3.7 Multioutput Classification</h3><p>类似的，输出结果也是 multilabel, 只是每个 label 可以是 multiclass。</p>
<p>To illustrate this, let’s build a system that removes noise from images. It will take as input a noisy digit image, and it will (hopefully) output a clean digit image, represented as an array of pixel intensities, just like the MNIST images. Notice that the classifier’s output is multilabel (one label per pixel) and each label can have multiple values (pixel intensity ranges from 0 to 255). This is thus an example of a multioutput classification system. Arguably, predicting pixel intensity is more akin to regression than to classification.</p>
<p>手动构建噪点图片集 X_train_mod, 原始图片 X_train 作为 label:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># add noise to build train set</span></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">noise = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="built_in">len</span>(X_train), <span class="number">784</span>))</span><br><span class="line">X_train_mod = X_train + noise</span><br><span class="line">noise = np.random.randint(<span class="number">0</span>, <span class="number">100</span>, (<span class="built_in">len</span>(X_test), <span class="number">784</span>))</span><br><span class="line">X_test_mod = X_test + noise</span><br><span class="line"></span><br><span class="line">y_train_mod = X_train</span><br><span class="line">y_test_mod = X_test</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/11/m7Ob1oezHXxaklt.png" alt="image-20230211142057910"></p>
<p>Now let’s train the classifier and make it clean up this image (Figure 3-13):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line">knn_clf.fit(X_train_mod, y_train_mod)</span><br><span class="line">clean_digit = knn_clf.predict([X_test_mod[<span class="number">0</span>]])</span><br><span class="line">plot_digit(clean_digit)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/11/FQOLaAupPwZYkKs.png" alt="image-20230211143221038"></p>
<p>经过降噪后的图片似乎比原来的更清晰？（总之和原始图片不同了）</p>
<h3 id="3-x-Exercises"><a href="#3-x-Exercises" class="headerlink" title="3.x Exercises"></a>3.x Exercises</h3><ol>
<li><p>Try to build a classifier for the MNIST dataset that achieves over 97% accuracy on the test set. Hint: the <code>KNeighborsClassifier</code> works quite well for this task; you just need to find good hyperparameter values (try a grid search on the <code>weights</code> and <code>n_neighbors</code> hyperparameters).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">knn_clf = KNeighborsClassifier(n_jobs=-<span class="number">1</span>)</span><br><span class="line">knn_clf.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line">accuracy = knn_clf.score(X_test_scaled, y_test)</span><br><span class="line">accuracy</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.9688</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">param_grid = [</span><br><span class="line">    &#123;<span class="string">&#x27;weights&#x27;</span>: [<span class="string">&#x27;uniform&#x27;</span>, <span class="string">&#x27;distance&#x27;</span>], <span class="string">&#x27;n_neighbors&#x27;</span>: [<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]&#125;</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid,</span><br><span class="line">                           cv=<span class="number">3</span>, n_jobs=-<span class="number">1</span>, scoring=<span class="string">&#x27;accuracy&#x27;</span>)</span><br><span class="line">grid_search.fit(X_train_scaled, y_train)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;Best params: &quot;</span>, grid_search.best_params_)</span><br><span class="line">best_knn_clf = grid_search.best_estimator_</span><br><span class="line"></span><br><span class="line">accuracy = best_knn_clf.score(X_test_scaled, y_test)</span><br><span class="line">accuracy</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; Best params:  {‘n_neighbors’: 4, ‘weights’: ‘distance’}</p>
<p>&gt;&gt;&gt; 0.9714</p>
</li>
<li><p>Write a function that can shift an MNIST image in any direction (left, right, up, or down) by one pixel. Then, for each image in the training set, create four shifted copies (one per direction) and add them to the training set. Finally, train your best model on this expanded training set and measure its accuracy on the test set. You should observe that your model performs even better now! This technique of artificially growing the training set is called <em>data augmentation</em> or <em>training set expansion</em>.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.ndimage <span class="keyword">import</span> shift</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">shift_image</span>(<span class="params">image, axis_0, axis_1</span>):</span><br><span class="line">    image = image.reshape((<span class="number">28</span>, <span class="number">28</span>))</span><br><span class="line">    shifted_image = shift(image, [axis_0, axis_1], cval=<span class="number">0</span>, mode=<span class="string">&quot;constant&quot;</span>)</span><br><span class="line">    <span class="keyword">return</span> shifted_image.reshape([-<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># data augmentation (or train set expansion)</span></span><br><span class="line">x_train_augmented = [image <span class="keyword">for</span> image <span class="keyword">in</span> X_train]</span><br><span class="line">y_train_augmented = [label <span class="keyword">for</span> label <span class="keyword">in</span> y_train]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> axis_0, axis_1 <span class="keyword">in</span> ((<span class="number">1</span>, <span class="number">0</span>), (-<span class="number">1</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">1</span>), (<span class="number">0</span>, -<span class="number">1</span>)):</span><br><span class="line">    <span class="keyword">for</span> image, label <span class="keyword">in</span> <span class="built_in">zip</span>(X_train, y_train):</span><br><span class="line">        x_train_augmented.append(shift_image(image, axis_0, axis_1))</span><br><span class="line">        y_train_augmented.append(label)</span><br><span class="line"></span><br><span class="line">X_train_augmented = np.array(x_train_augmented)</span><br><span class="line">y_train_augmented = np.array(y_train_augmented)</span><br></pre></td></tr></table></figure>

<p>注意，构建模型的时候可以使用之前搜索的超参数：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">knn_clf = KNeighborsClassifier(**grid_search.best_params_)</span><br></pre></td></tr></table></figure>
</li>
<li><p>Tackle the Titanic dataset. A great place to start is on Kaggle. Alternatively, you can download the data from <a href="https://homl.info/titanic.tgz">https://homl.info/titanic.tgz</a> and unzip this tarball like you did for the housing data in Chapter 2. This will give you two CSV files, <em>train.csv</em> and <em>test.csv</em>, which you can load using <code>pandas.read_csv()</code>. The goal is to train a classifier that can predict the <code>Survived</code> column based on the other columns.</p>
<p>一共有 12 个 features，其中非数字型有 5 个，分别是 Name, Sex, Ticket, Cabin, Embarked，其中 Name 应该不是有效特征可以移除。数字列中，PassengerId 可以作为 index，Pclass 应该是类别。</p>
<p>另外，有缺失值的特征有 Age (714), Cabin (204), Embarked (889). Cabin 缺失太多可以直接移除，Age 缺失可以用中位数填充，Embarked 缺失可以用众数填充。</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">train_data = train_data.set_index(<span class="string">&#x27;PassengerId&#x27;</span>)</span><br><span class="line">test_data = test_data.set_index(<span class="string">&#x27;PassengerId&#x27;</span>)</span><br><span class="line"></span><br><span class="line">X_train, y_train = train_data.drop([<span class="string">&#x27;Survived&#x27;</span>, <span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Cabin&#x27;</span>], axis=<span class="number">1</span>), train_data[<span class="string">&#x27;Survived&#x27;</span>]</span><br><span class="line"><span class="comment"># no y_test cause no survived column</span></span><br><span class="line">X_test = test_data.drop([<span class="string">&#x27;Name&#x27;</span>, <span class="string">&#x27;Cabin&#x27;</span>], axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">num_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;median&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;scaler&#x27;</span>, StandardScaler())</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">cat_pipeline = Pipeline([</span><br><span class="line">    (<span class="string">&#x27;imputer&#x27;</span>, SimpleImputer(strategy=<span class="string">&#x27;most_frequent&#x27;</span>)),</span><br><span class="line">    (<span class="string">&#x27;cat_encoder&#x27;</span>, OneHotEncoder(sparse=<span class="literal">False</span>))</span><br><span class="line">])</span><br><span class="line"></span><br><span class="line">num_attribs = [<span class="string">&#x27;Age&#x27;</span>, <span class="string">&#x27;SibSp&#x27;</span>, <span class="string">&#x27;Parch&#x27;</span>, <span class="string">&#x27;Fare&#x27;</span>]</span><br><span class="line">cat_attribs = [<span class="string">&#x27;Pclass&#x27;</span>, <span class="string">&#x27;Sex&#x27;</span>, <span class="string">&#x27;Embarked&#x27;</span>]  <span class="comment"># exclude Ticket</span></span><br><span class="line"></span><br><span class="line">preprocess_pipeline = ColumnTransformer([</span><br><span class="line">    <span class="comment"># (&quot;ordinal_encoder&quot;, OrdinalEncoder()),</span></span><br><span class="line">    (<span class="string">&#x27;num&#x27;</span>, num_pipeline, num_attribs),</span><br><span class="line">    (<span class="string">&#x27;cat&#x27;</span>, cat_pipeline, cat_attribs)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_train = preprocess_pipeline.fit_transform(X_train)</span><br><span class="line">X_test = preprocess_pipeline.transform(X_test)</span><br></pre></td></tr></table></figure>

<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">forest_clf = RandomForestClassifier(n_estimators=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">forest_clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 因为没有 y_test，所以用交叉验证</span></span><br><span class="line">scores = cross_val_score(forest_clf, X_train, y_train, cv=<span class="number">10</span>)</span><br><span class="line">scores.mean()</span><br></pre></td></tr></table></figure>


</li>
<li><p>dsaadsa</p>
<p>Q&amp;A:</p>
<p>怎么下载保存多个文件？</p>
<p>Tips:</p>
<p>邮件格式：空行后是正文+后缀，文本转换考虑用词嵌入 Embedding 或者 矩阵？</p>
<p>文件名中的 ham 用 0 标注，spam 用 1 标注为垃圾邮件</p>
</li>
</ol>
<h2 id="4-Training-Models"><a href="#4-Training-Models" class="headerlink" title="4. Training Models"></a>4. Training Models</h2><h3 id="4-1-Linear-Regression"><a href="#4-1-Linear-Regression" class="headerlink" title="4.1 Linear Regression"></a>4.1 Linear Regression</h3><p>A linear model makes a prediction by simply computing a weighted sum of the input features, plus a constant called the <strong><em>bias term</em> (偏置项, or intercept term, or <em>θ</em><del>0</del> )</strong>, as shown in Equation 4-1. </p>
<p>​		<em>Equation 4-1. Linear regression model prediction</em></p>
<p>​		$\hat y &#x3D; θ_0 + θ_1x_1 + θ_2x_2 + ⋯ + θ_nx_n$</p>
<p>公式中，模型参数的 <em>θ</em><del>0</del> 为 bias term，<em>θ</em><del>1</del> <em>θ</em><del>2</del> <em>θ</em><del>n</del> 为 weights；数据集的 x<del>i</del> 为 i^th^ feature value。</p>
<p>这个公式可以用向量形式表示，如 Equation 4-2.</p>
<p>​		<em>Equation 4-2. Linear regression model prediction (vectorized form)</em></p>
<p>​		$\hat y &#x3D; ℎ_θ(x) &#x3D; θ · x$</p>
<p>The MSE of a linear regression hypothesis <em>h</em><del>θ</del> on a training set <strong>X</strong> is calculated using Equation 4-3.</p>
<p>​		<em>Equation 4-3. MSE cost function for a linear regression model</em></p>
<p>​		$RMSE(X, h) &#x3D; \frac 1 m\Sigma ^m _{i&#x3D;1}(θ^Tx^{(i)}-y^{(i)})^2$</p>
<p>​		or $RMSE &#x3D; \sqrt{\frac{1}{n}\sum_{i&#x3D;1}^{n}(y_i - \hat{y_i})^2}$</p>
<p>To find the value of θ that minimizes the MSE, there exists a closed-form solution—in other words, a mathematical equation that gives the result directly. This is called the Normal equation (Equation 4-4).</p>
<p>​		<em>Equation 4-4. Normal equation</em></p>
<p>​		$\hat θ &#x3D; (X^⊺X)^{−1} X^⊺ y$</p>
<p>In this equation:</p>
<ul>
<li>$\hat θ$  is the value of <strong>θ</strong> that minimizes the cost function.</li>
<li><strong>y</strong> is the vector of target values containing <em>y</em>^(1)^ to <em>y</em>^(m)^</li>
</ul>
<p>Let’s generate some linear-looking data to test this equation on (Figure 4-1):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">100</span>  <span class="comment"># number of instances</span></span><br><span class="line">X = <span class="number">2</span> * np.random.rand(m, <span class="number">1</span>)  <span class="comment"># column vector</span></span><br><span class="line">y = <span class="number">4</span> + <span class="number">3</span> * X + np.random.randn(m, <span class="number">1</span>)  <span class="comment"># column vector</span></span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/14/6hKVDp9x28wOvXC.png" alt="image-20230214234850089"></p>
<p>Now let’s compute $\hat θ$ using the Normal equation. The <code>@</code> operator performs matrix multiplication: </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> add_dummy_feature</span><br><span class="line"></span><br><span class="line">X_b = add_dummy_feature(X)  <span class="comment"># add x0 = 1 to each instance. Or you can replace with X_b = np.c_[np.ones((100, 1)), X]</span></span><br><span class="line">theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y)</span><br></pre></td></tr></table></figure>

<p>The function that we used to generate the data is <em>y</em> &#x3D; 4 + 3<em>x</em><del>1</del> + Gaussian noise. Let’s see what the equation found:</p>
<p><img src="https://s2.loli.net/2023/02/15/jNTysQDcZLJlES5.png" alt="image-20230215175137980"></p>
<p>Now we can make predictions using $\hat θ$:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_new = np.array([[<span class="number">0</span>], [<span class="number">2</span>]])</span><br><span class="line">X_new_b = add_dummy_feature(X_new)</span><br><span class="line">y_predict = X_new_b @ theta_best</span><br><span class="line"></span><br><span class="line">plt.plot(X_new, y_predict, <span class="string">&quot;r-&quot;</span>, label=<span class="string">&quot;Predictions&quot;</span>)</span><br><span class="line">plt.plot(X, y, <span class="string">&quot;b.&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/15/tBQXo1FURqCrY6j.png" alt="image-20230215181306110"></p>
<p>除了用 Normal Equation 之外，再直接用 Scikit-Learn 的 Linear Regression 模型看看：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/15/w9V6NfYZnlu8tCp.png" alt="image-20230215182029224"></p>
<p>Notice that Scikit-Learn separates the <em>bias term</em> (<code>intercept_</code>) from the <em>feature weights</em> (<code>coef_</code>). </p>
<p>Computational Complexity: Normal Equation 为 <em>O</em>(<em>n</em>^2.4^) 到 <em>O</em>(<em>n</em>^3^)，而 Linear Regression 模型内部的算法 <em>singular value decomposition</em> (SVD, 奇异值分解) 复杂度相对低一些，为 O(n^2^)。</p>
<h3 id="4-2-Gradient-Descent"><a href="#4-2-Gradient-Descent" class="headerlink" title="4.2 Gradient Descent"></a>4.2 Gradient Descent</h3><p>Gradient descent is a <strong>generic optimization algorithm</strong> capable of finding <strong>optimal solutions</strong> to a wide range of problems. The general idea of gradient descent is to tweak parameters iteratively in order to minimize a cost function.</p>
<p>Suppose you are lost in the mountains in a dense fog, and you can only feel the slope of the ground below your feet. A good strategy to get to the bottom of the valley quickly is to go downhill in the direction of the steepest slope. This is exactly what gradient descent does: it measures the local gradient of the error function with regard to the parameter vector <code>θ</code>, and it goes in the direction of descending gradient. Once the gradient is zero, you have reached a minimum!</p>
<p>In practice, you start by filling <code>θ</code> with random values (this is called <em>random initialization</em>). Then you improve it gradually, taking one baby step at a time, each step attempting to decrease the cost function (e.g., the MSE), until the algorithm <em>converges</em> to a minimum (see Figure 4-3).</p>
<p><img src="https://s2.loli.net/2023/02/15/j6oOPZvsRCETLW3.png" alt="image-20230215193035637"></p>
<p>An important parameter in gradient descent is the size of the steps, determined by the learning rate hyperparameter. If the learning rate is too small, then the algorithm will have to go through many iterations to converge, which will take a long time (see Figure 4-4).</p>
<p><img src="https://s2.loli.net/2023/02/15/IE58GrwV2aPk6by.png" alt="image-20230215193149853"></p>
<p><img src="https://s2.loli.net/2023/02/15/xJf78Twi1sqPWte.png" alt="image-20230215195111000"></p>
<p><img src="https://s2.loli.net/2023/02/15/g3eG5FqhJ4n7ufi.png" alt="image-20230215195333521"></p>
<p>Fortunately, the MSE cost function for a linear regression model happens to be a <em>convex function</em>, which means that if you pick any two points on the curve, the line segment joining them is never below the curve. This implies that there are no local minima, just one global minimum. It is also a continuous function with a slope that never changes abruptly. These two facts have a great consequence: gradient descent is guaranteed to approach arbitrarily closely the global minimum (if you wait long enough and if the learning rate is not too high).</p>
<p>While the cost function has the shape of a bowl, it can be an elongated bowl if the features have very different scales. Figure 4-7 shows gradient descent on a training set where features 1 and 2 have the same scale (on the left), and on a training set where feature 1 has much smaller values than feature 2 (on the right).</p>
<p><img src="https://s2.loli.net/2023/02/15/9fa7nDY8vxNulrK.png" alt="image-20230215195951540"></p>
<p>As you can see, on the left the gradient descent algorithm goes straight toward the minimum, thereby reaching it quickly, whereas on the right it first goes in a direction almost orthogonal to the direction of the global minimum, and it ends with a long march down an almost flat valley. It will eventually reach the minimum, but it will take a long time.</p>
<p>Attension: When using gradient descent, you should ensure that all features have a similar scale (e.g., using Scikit-Learn’s <code>StandardScaler</code> class), or else it will take much longer to converge.</p>
<h4 id="Batch-Gradient-Descent"><a href="#Batch-Gradient-Descent" class="headerlink" title="Batch Gradient Descent"></a>Batch Gradient Descent</h4><h5 id="Source"><a href="#Source" class="headerlink" title="Source"></a>Source</h5><p>为了实现梯度下降，需要先计算 cost function 关于模型每个参数 $\theta _j$ (weights) 的梯度。换句话说，就是计算出关于每个 $\theta$​ 的偏导数 (<em>partial derivative</em>)。Equation 4-5 computes the partial derivative of the MSE with regard to parameter <em>θ<del>j</del></em>, noted ∂ MSE(<strong>θ</strong>) &#x2F; ∂θ*<del>j</del>*.</p>
<p>​		<em>Equation 4-5. Partial derivatives of the cost function</em></p>
<p>​		$\frac \partial {\partial \theta_j}MSE(\theta) &#x3D; \frac 2 m \Sigma ^m _{i&#x3D;1} (\theta^T x^{(i)} - y^{(i)})x^{(i)}_j$</p>
<p>Instead of computing these partial derivatives individually, you can use Equation 4-6 to compute them all in one go. The gradient vector, noted ∇<del>θ</del>MSE(θ), contains all the partial derivatives of the cost function (one for each model parameter).</p>
<p><img src="https://s2.loli.net/2023/02/15/R3knoEmblvqVNJc.png" alt="image-20230215224218068"></p>
<p>其中，每次偏导计算中均使用了所有的样本数据，因此更稳定，但代价是计算开销更大，这也是和随机梯度下降的区别之处。</p>
<p>继 Normal Equation，SVD of Linear Regression 后，再用梯度下降公式 (Equation 4-7) 求解最优 θ (bias and weights)：</p>
<p>​		<em>Equation 4-7. Gradient descent step</em></p>
<p>​		$θ^{(next \space step)} &#x3D; θ − η∇_θ MSE (θ)$</p>
<p>Let’s look at a quick implementation of this algorithm:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">eta = <span class="number">0.1</span>  <span class="comment"># learning rate</span></span><br><span class="line">n_epochs = <span class="number">1000</span>  <span class="comment"># n_iterations</span></span><br><span class="line">m = <span class="built_in">len</span>(X_b)  <span class="comment"># number of instances</span></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># random initialized model parameters</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    gradients = <span class="number">2</span> / m * X_b.T @ (X_b @ theta - y)</span><br><span class="line">    theta = theta - eta * gradients</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/15/Y8DrZy7wSfkTUOE.png" alt="image-20230215224420906"></p>
<p>可见三种方法计算出来的 theta 均正确。</p>
<p><img src="https://s2.loli.net/2023/02/15/53P6A7dRkiN9bHK.png" alt="image-20230215210907860"></p>
<p><img src="https://s2.loli.net/2023/02/15/LpBvVzEYTcyR3DA.png" alt="image-20230215231415087"></p>
<h5 id="From-chatGPT"><a href="#From-chatGPT" class="headerlink" title="From chatGPT"></a>From chatGPT</h5><p>批量梯度下降（Batch Gradient Descent）是一种常见的优化算法，用于最小化某个损失函数。它的基本思想是通过计算损失函数对模型参数的梯度来更新模型参数，不断迭代以找到最优的参数值。与随机梯度下降相比，批量梯度下降在每次更新时使用所有的样本数据，因此更稳定，但代价是计算开销更大。</p>
<p>批量梯度下降的具体过程如下：</p>
<ol>
<li>随机初始化模型参数 $\theta$</li>
<li>对于每个训练迭代：<ol>
<li>计算当前参数下的损失函数 $J(\theta)$</li>
<li>计算损失函数关于参数的梯度 $\nabla J(\theta)$</li>
<li>更新参数 $\theta \leftarrow \theta - \alpha \nabla J(\theta)$，其中 $\alpha$ 是学习率</li>
</ol>
</li>
<li>循环执行步骤2，直到满足停止条件（例如达到最大迭代次数，或损失函数收敛）</li>
</ol>
<p>批量梯度下降的数学公式如下：</p>
<p>$$\theta \leftarrow \theta - \alpha \nabla J(\theta)$$</p>
<p>其中，$\theta$ 是模型参数向量，$\alpha$ 是学习率（控制参数更新的步长），$\nabla J(\theta)$ 是损失函数 $J(\theta)$ 关于参数向量 $\theta$ 的梯度。梯度的计算通常使用反向传播算法，具体实现取决于模型和损失函数的具体形式。</p>
<p>在实际应用中，批量梯度下降可能需要进行一些优化，例如使用动量（Momentum）、自适应学习率（Adaptive Learning Rate）等技巧来加速收敛和避免局部最优解。</p>
<p><img src="https://s2.loli.net/2023/02/15/nesvqMrKQlgE75S.png" alt="image-20230215234142259"></p>
<p><img src="https://s2.loli.net/2023/02/15/ONEmKaPSQhYrWM3.png" alt="image-20230215235506551"></p>
<h4 id="Stochastic-Gradient-Descent"><a href="#Stochastic-Gradient-Descent" class="headerlink" title="Stochastic Gradient Descent"></a>Stochastic Gradient Descent</h4><p>随机梯度下降在每个迭代更新的过程中，随机使用单个样本或一小批样本（mini-batch）来计算而非整个数据集。因此，它的计算速度更快。但是，由于随机梯度下降每次只使用一小部分样本，因此更新过程具有一定的随机性，并且可能收敛到局部最优解而不是全局最优解。</p>
<p><img src="https://s2.loli.net/2023/02/17/u4IpPSQAhtWrKkV.png" alt="image-20230217185728180"></p>
<p>When the cost function is very irregular (as in Figure 4-6), this can actually help the algorithm <strong>jump out of local minima</strong>, so stochastic gradient descent has a better chance of finding the global minimum than batch gradient descent does.</p>
<p>Therefore, randomness is good to escape from local optima, but bad because it means that the algorithm can never settle at the minimum. One solution to this dilemma is to <strong>gradually reduce the learning rate</strong>. The steps start out large (which helps make quick progress and escape local minima), then get smaller and smaller, allowing the algorithm to settle at the global minimum.</p>
<p>The function that determines the learning rate at each iteration is called the <em>learning schedule</em>. If the learning rate is reduced too quickly, you may get stuck in a local minimum, or even end up frozen halfway to the minimum. If the learning rate is reduced too slowly, you may jump around the minimum for a long time and end up with a suboptimal solution if you halt training too early.</p>
<p>This code implements stochastic gradient descent using a simple learning schedule:</p>
<figure class="highlight py"><table><tr><td class="code"><pre><span class="line">n_epochs = <span class="number">50</span></span><br><span class="line">t0, t1 = <span class="number">5</span>, <span class="number">50</span>  <span class="comment"># learning schedule hyperparameters</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">learning_schedule</span>(<span class="params">t</span>):</span><br><span class="line">    <span class="keyword">return</span> t0 / (t + t1)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">theta = np.random.randn(<span class="number">2</span>, <span class="number">1</span>)  <span class="comment"># random initialization</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    <span class="keyword">for</span> iteration <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        random_index = np.random.randint(m)</span><br><span class="line">        xi = X_b[random_index:random_index + <span class="number">1</span>]</span><br><span class="line">        yi = y[random_index:random_index + <span class="number">1</span>]</span><br><span class="line">        gradients = <span class="number">2</span> * xi.T @ (xi @ theta - yi)  <span class="comment"># for SGD, don&#x27;t need to compute the full gradient and divide by m</span></span><br><span class="line">        eta = learning_schedule(epoch * m + iteration)</span><br><span class="line">        theta = theta - eta * gradients</span><br></pre></td></tr></table></figure>

<p>By convention we iterate by rounds of m iterations; each round is called an epoch, as earlier. While the batch gradient descent code iterated 1,000 times through the whole training set, this code goes through the training set only 50 times and reaches a pretty good solution:</p>
<p><img src="https://s2.loli.net/2023/02/17/8kQPfcoTEjdX2iz.png" alt="image-20230217221540819"></p>
<p><img src="https://s2.loli.net/2023/02/17/zWE6GOS4J2ZhIVg.png" alt="image-20230217222259861"></p>
<p>Attention: When using stochastic gradient descent, the training instances must be independent and identically distributed (IID) to ensure that the parameters get pulled toward the global optimum, on average. A simple way to ensure this is to shuffle the instances during training (e.g., pick each instance randomly, or shuffle the training set at the beginning of each epoch). If you do not shuffle the instances—for example, if the instances are sorted by label—then SGD will start by optimizing for one label, then the next, and so on, and it will not settle close to the global minimum.</p>
<p>To perform linear regression using stochastic GD with Scikit-Learn, you can use the <code>SGDRegressor</code> class, which defaults to optimizing the MSE cost function. The following code runs for maximum 1,000 epochs (<code>max_iter</code>) or until the loss drops by less than 10^–5^ (<code>tol</code>, tolerance) during 100 epochs (<code>n_iter_no_change</code>). It starts with a learning rate of 0.01 (<code>eta0</code>), using the default learning schedule (different from the one we used). Lastly, it does not use any regularization (<code>penalty=None</code>; more details on this shortly):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> SGDRegressor</span><br><span class="line"></span><br><span class="line">sgd_reg = SGDRegressor(max_iter=<span class="number">1000</span>, tol=<span class="number">1e-3</span>, penalty=<span class="literal">None</span>, eta0=<span class="number">0.1</span>,</span><br><span class="line">                       n_iter_no_change=<span class="number">100</span>, random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())  <span class="comment"># because fit() expects 1D targets, y.ravel() is the same as y.reshape(-1)</span></span><br></pre></td></tr></table></figure>

<p>Once again, you find a solution quite close to the one returned by the Normal equation:</p>
<p>​	&gt;&gt;&gt;  sgd_reg.intercept_, sgd_reg.coef_</p>
<p>​	(array([4.21278812]), array([2.77270267]))</p>
<h4 id="Mini-Batch-Gradient-Descent"><a href="#Mini-Batch-Gradient-Descent" class="headerlink" title="Mini-Batch Gradient Descent"></a>Mini-Batch Gradient Descent</h4><p>顾名思义，小批量梯度下降是使用 mini-batches 实例去计算更新梯度的，而非使用所有实例 (batch GD) 或单个示例 (stochastic GD)。它最主要的优点是能利用 GPUs 优化的矩阵运算来提高性能。</p>
<p>该算法过程的 parameter space 比 stochastic GD 更稳定，并且最终能更<strong>接近</strong>最小值。但是也容易陷入局部最优解，因此最好也使用 learning schedule 来走出局部最优和到达真正的最小值。</p>
<p><img src="https://s2.loli.net/2023/02/18/c9htz8uUQwbTHWj.png" alt="image-20230218184617444"></p>
<p>Table 4-1 compares the algorithms we’ve discussed so far for linear regression (recall that <em>m</em> is the number of training instances and <em>n</em> is the number of features).</p>
<p><img src="https://s2.loli.net/2023/02/18/4CA5tze1xlXMk79.png" alt="image-20230218190445446"></p>
<p>There is almost no difference after training: all these algorithms end up with very similar models and make predictions in exactly the same way.</p>
<h3 id="4-3-Polynomial-Regression"><a href="#4-3-Polynomial-Regression" class="headerlink" title="4.3 Polynomial Regression"></a>4.3 Polynomial Regression</h3><p>What if your data is more complex than a straight line? Surprisingly, you can use a linear model to fit nonlinear data. A simple way to do this is to add powers of each feature as new features, then train a linear model on this extended set of features. This technique is called <em>polynomial regression</em>.</p>
<p>想看一个简单的例子。下述代码生成了一些非线性数据：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">m = <span class="number">100</span></span><br><span class="line">X = <span class="number">6</span> * np.random.rand(m, <span class="number">1</span>) - <span class="number">3</span></span><br><span class="line">y = <span class="number">0.5</span> * X ** <span class="number">2</span> + X + <span class="number">2</span> + np.random.randn(m, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/18/RixCKDsgJaLvNP5.png" alt="image-20230218201159256"></p>
<p>接下来，只要增加新特征 (所有 features 的幂次运算) 后，线性模型就可以用了。可以使用 Scikit-Learn 的 <code>PolynomialFeatures</code> 增加多项式特征：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">X_poly = poly_features.fit_transform(X)</span><br><span class="line"></span><br><span class="line">X[<span class="number">0</span>], X_poly[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; (array([-0.75275929]), array([-0.75275929,  0.56664654]))</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lin_reg = LinearRegression()</span><br><span class="line">lin_reg.fit(X_poly, y)</span><br></pre></td></tr></table></figure>

<p>线性模型使用扩充特征后的数据集训练后，就可以有效预测非线性数据了，见 Figure 4-13：</p>
<p><img src="https://s2.loli.net/2023/02/18/zrpsFEa3MOt6HgS.png" alt="image-20230218201549350"></p>
<p>Not bad: the model estimates $\hat y &#x3D; 0.56x_1^2 + 0.93x_1 + 1.78$ when in fact the original function was $\hat y &#x3D; 0.5x_1^2 + 1.0x_1 + 2.0 + Gaussian \space noise$.</p>
<p>Scikit-Learn 中 PolynomialFeatures 的 degree 参数应该如何设置？</p>
<p><img src="https://s2.loli.net/2023/02/18/XhJKeVcirgQ5ln3.png" alt="image-20230218203057864"></p>
<p>Warning: <code>PolynomialFeatures(degree=d)</code> transforms an array containing <em>n</em> features into an array containing $(n + d)! &#x2F; d!n!$ features, where $n!$ is the <em>factorial</em> of n, equal to 1 × 2 × 3 × ⋯ × n. Beware of the combinatorial explosion of the number of features!</p>
<h3 id="4-4-Learning-Curves"><a href="#4-4-Learning-Curves" class="headerlink" title="4.4 Learning Curves"></a>4.4 Learning Curves</h3><p><img src="https://s2.loli.net/2023/02/18/3lhvcONRg9s2Sj8.png" alt="image-20230218204850799"></p>
<p>除了可以用 cross-validation 来评估模型，观察是否欠拟合、过拟合外；也可以通过观察 <em>learning curves</em> (包含了模型的 training error 和 validation error) 来判断。</p>
<p>另外注意，如果模型支持 trained incrementally (i.e., support <code>partial_fit()</code> or <code>warm_start</code>)，可以设置 learning curves 的参数 <code>exploit_incremental_learning=True</code>  </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> learning_curve</span><br><span class="line"></span><br><span class="line">train_sizes, train_scores, valid_scores = learning_curve(</span><br><span class="line">    LinearRegression(),</span><br><span class="line">    X, y, cv=<span class="number">5</span>,</span><br><span class="line">    train_sizes=np.linspace(<span class="number">0.01</span>, <span class="number">1.0</span>, <span class="number">40</span>),</span><br><span class="line">    scoring=<span class="string">&#x27;neg_root_mean_squared_error&#x27;</span>)</span><br><span class="line"></span><br><span class="line">train_errors = -train_scores.mean(axis=<span class="number">1</span>)</span><br><span class="line">valid_errors = -valid_scores.mean(axis=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">plt.plot(train_sizes, train_errors, <span class="string">&quot;r-+&quot;</span>, linewidth=<span class="number">2</span>, label=<span class="string">&quot;train&quot;</span>)</span><br><span class="line">plt.plot(train_sizes, valid_errors, <span class="string">&quot;b-&quot;</span>, linewidth=<span class="number">3</span>, label=<span class="string">&quot;valid&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/18/ZjhfWk57GyDEVt8.png" alt="image-20230218225042582"></p>
<p>This model is underfitting (曲线重叠)， 因为就算继续给更多的训练集数据，RMSE 依旧很高且几乎等同于一条水平直线；理想的情况是两条曲线逐渐接近且趋于平稳。</p>
<p>过拟合的学习曲线是怎么样的？</p>
<p><img src="https://s2.loli.net/2023/02/18/9qngaumfsCVwZ7y.png" alt="image-20230218230324503"></p>
<h3 id="4-5-Regularized-Linear-Model"><a href="#4-5-Regularized-Linear-Model" class="headerlink" title="4.5 Regularized Linear Model"></a>4.5 Regularized Linear Model</h3><p>As you saw in Chapters 1 and 2, a good way to reduce overfitting is to regularize (正则化) the model (i.e., to constrain it): the fewer degrees of freedom it has, the harder it will be for it to overfit the data. A simple way to regularize a polynomial model is to reduce the number of polynomial degrees.</p>
<p>For a linear model, regularization is typically achieved by constraining the weights of the model. We will now look at <em>ridge regression</em>, <em>lasso regression</em>, and <em>elastic net regression</em>, which implement three different ways to constrain the weights.</p>
<h4 id="Ridge-Regression"><a href="#Ridge-Regression" class="headerlink" title="Ridge Regression"></a>Ridge Regression</h4><p>Ridge Regression 是一种线性回归的变体，它使用 L2 正则化（在 cost 函数加上正则化项，见 Equation 4-8）将所有特征的系数<strong>缩小</strong>从而限制模型的复杂性。</p>
<p><img src="https://s2.loli.net/2023/02/21/CZMqQzcNE7b3R1n.png" alt="image-20230221010712803"></p>
<p>Equation 4-8 也可以表示为：J(w) &#x3D; SSE + α * ||w||^2</p>
<p>其中，J(w) 是目标函数，w 是模型的系数，SSE 是普通线性回归中使用的损失函数，α 是正则化参数，||w||^2 是模型系数的平方和。当 α 越大时，惩罚项的影响越大，模型系数的取值也越趋近于 0。当 α 为 0 时，Ridge Regression 就退化成了普通线性回归。</p>
<p><img src="https://s2.loli.net/2023/02/21/Rtb43yH8zFUnaDC.png" alt="image-20230221013858716"></p>
<p>As with linear regression, we can perform ridge regression either by computing a closed-form equation or by performing gradient descent.</p>
<p><img src="https://s2.loli.net/2023/02/21/n962z1ZTdskDAvY.png" alt="image-20230221015214682"></p>
<p>Here is how to perform ridge regression with Scikit-Learn using a closed-form solution (a variant of Equation 4-9 that uses a matrix factorization technique by André-Louis Cholesky):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Ridge</span><br><span class="line"></span><br><span class="line">ridge_reg = Ridge(alpha=<span class="number">0.1</span>, solver=<span class="string">&quot;cholesky&quot;</span>)</span><br><span class="line">ridge_reg.fit(X, y)</span><br><span class="line">ridge_reg.predict([[<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>

<p>And using stochastic gradient descent:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">sgd_reg = SGDRegressor(penalty=<span class="string">&quot;l2&quot;</span>, alpha=<span class="number">0.1</span> / m, tol=<span class="literal">None</span>,</span><br><span class="line">                       max_iter=<span class="number">1000</span>, eta0=<span class="number">0.01</span>, random_state=<span class="number">42</span>)</span><br><span class="line">sgd_reg.fit(X, y.ravel())  <span class="comment"># y.ravel() because fit() expects 1D targets</span></span><br><span class="line">sgd_reg.predict([[<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>

<p>The <code>penalty</code> hyperparameter sets the type of regularization term to use. Specifying “<code>l2</code>“ indicates that you want SGD to add a regularization term to the MSE cost function equal to <code>alpha</code> times the square of the ℓ2 norm of the weight vector. This is just like ridge regression, except there’s no division by <code>m</code> in this case; that’s why we passed <code>alpha=0.1 / m</code>, to get the same result as <code>Ridge(alpha=0.1)</code>.</p>
<h4 id="Lasso-Regression"><a href="#Lasso-Regression" class="headerlink" title="Lasso Regression"></a>Lasso Regression</h4><p><em>Least absolute shrinkage and selection operator regression</em> (usually simply called <em>lasso regression</em>) is another regularized version of linear regression: just like ridge regression, it adds a regularization term to the cost function, but it uses the ℓ1 norm of the weight vector instead of the square of the ℓ2 norm (see Equation 4-10). Notice that the ℓ1  norm is multiplied by 2α, whereas the ℓ2  norm was multiplied by $α &#x2F; m$ in ridge regression.</p>
<p><img src="https://s2.loli.net/2023/02/21/A32YPwcZNM5aEC8.png" alt="image-20230221021341803"></p>
<p><img src="https://s2.loli.net/2023/02/21/3HOFpPg2wNxnil8.png" alt="image-20230221021608358"></p>
<p>An important characteristic of lasso regression is that it tends to <strong>eliminate the weights of the least important features (i.e., set them to zero)</strong>.</p>
<p>Here is a small Scikit-Learn example using the <code>Lasso</code> class:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Lasso</span><br><span class="line"></span><br><span class="line">lasso_reg = Lasso(alpha=<span class="number">0.1</span>)</span><br><span class="line">lasso_reg.fit(X, y)</span><br><span class="line">lasso_reg.predict([[<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>

<p>如果需要进行特征选择或数据集中存在不相关或冗余的特征，那么可以使用 Lasso Regression。如果希望尽可能地保留所有特征，但系数不宜过大，可以使用 Ridge Regression。通常情况下，可以尝试使用交叉验证等方法来选择最优的正则化参数，并比较不同模型的性能。</p>
<h4 id="Elastic-Net-Regression"><a href="#Elastic-Net-Regression" class="headerlink" title="Elastic Net Regression"></a>Elastic Net Regression</h4><p>Elastic net regression is a middle ground between ridge regression and lasso regression. The regularization term is a weighted sum of both ridge and lasso’s regularization terms, and you can control the mix ratio <em>r</em>. When <em>r</em> &#x3D; 0, elastic net is equivalent to ridge regression, and when <em>r</em> &#x3D; 1, it is equivalent to lasso regression (Equation 4-12).</p>
<p><img src="https://s2.loli.net/2023/02/21/jfCo1Dawtz4VuMg.png" alt="image-20230221033331929"></p>
<p>Here is a short example that uses Scikit-Learn’s <code>ElasticNet</code> (<code>l1_ratio</code> corresponds to the mix ratio <em>r</em>):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> ElasticNet</span><br><span class="line"></span><br><span class="line">elastic_net = ElasticNet(alpha=<span class="number">0.1</span>, l1_ratio=<span class="number">0.5</span>)</span><br><span class="line">elastic_net.fit(X, y)</span><br><span class="line">elastic_net.predict([[<span class="number">1.5</span>]])</span><br></pre></td></tr></table></figure>



<p>So when should you use elastic net regression, or ridge, lasso, or plain linear regression (i.e., without any regularization)? It is almost always preferable to have at least a little bit of regularization, so generally you should avoid plain linear regression. Ridge is a good default, but if you suspect that only a few features are useful, you should prefer lasso or elastic net because they tend to reduce the useless features’ weights down to zero, as discussed earlier. In general, elastic net is preferred over lasso because lasso may behave erratically when the number of features is greater than the number of training instances or when several features are strongly correlated.</p>
<h4 id="Early-Stopping"><a href="#Early-Stopping" class="headerlink" title="Early Stopping"></a>Early Stopping</h4><p>It is such a simple and efficient regularization technique that Geoffrey Hinton called it a “beautiful free lunch”.</p>
<p><img src="https://s2.loli.net/2023/02/21/DfQX2z6Auk8hmdM.png" alt="image-20230221043509336"></p>
<p>Here is a basic implementation of early stopping:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> copy <span class="keyword">import</span> deepcopy</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">X_train, y_train, X_valid, y_valid = [...]  <span class="comment"># split the quadratic dataset</span></span><br><span class="line"></span><br><span class="line">preprocessing = make_pipeline(</span><br><span class="line">    PolynomialFeatures(degree=<span class="number">90</span>, include_bias=<span class="literal">False</span>),</span><br><span class="line">    StandardScaler())</span><br><span class="line">X_train_prep = preprocessing.fit_transform(X_train)</span><br><span class="line">X_valid_prep = preprocessing.transform(X_valid)</span><br><span class="line">sgd_reg = SGDRegressor(penalty=<span class="literal">None</span>, eta0=<span class="number">0.002</span>, random_state=<span class="number">42</span>)</span><br><span class="line">n_epochs = <span class="number">500</span></span><br><span class="line">best_valid_rmse = <span class="built_in">float</span>(<span class="string">&#x27;inf&#x27;</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(n_epochs):</span><br><span class="line">    sgd_reg.partial_fit(X_train_prep, y_train)</span><br><span class="line">    y_valid_predict = sgd_reg.predict(X_valid_prep)</span><br><span class="line">    val_error = mean_squared_error(y_valid, y_valid_predict, squared=<span class="literal">False</span>)</span><br><span class="line">    <span class="keyword">if</span> val_error &lt; best_valid_rmse:</span><br><span class="line">        best_valid_rmse = val_error</span><br><span class="line">    best_model = deepcopy(sgd_reg)</span><br></pre></td></tr></table></figure>

<p>Note that the model is copied using copy.deepcopy(), because it copies both the model’s hyperparameters and the learned parameters. In contrast, sklearn.base.clone() only copies the model’s hyperparameters.</p>
<h3 id="4-6-Logistic-Regression"><a href="#4-6-Logistic-Regression" class="headerlink" title="4.6 Logistic Regression"></a>4.6 Logistic Regression</h3><p>As discussed in Chapter 1, some regression algorithms can be used for classification (and vice versa). Logistic regression (also called logit regression) is commonly used to estimate the <strong>probability</strong> that an instance belongs to a particular class (e.g., what is the probability that this email is spam?). If the estimated probability is greater than a given threshold (typically 50%), then the model predicts that the instance belongs to that class (called the positive class, labeled “1”), and otherwise it predicts that it does not (i.e., it belongs to the negative class, labeled “0”). This makes it a binary classifier.</p>
<p>逻辑回归的 sigmoid 或 softmax 函数都是将 scores 转换为 probabilities，再返回最大的概率值对应的类别。</p>
<h4 id="Estimating-Probabilities"><a href="#Estimating-Probabilities" class="headerlink" title="Estimating Probabilities"></a>Estimating Probabilities</h4><p>The logistic—noted σ(·)—is a <em>sigmoid function</em> (i.e., S-shaped) that outputs a number between 0 and 1. It is defined as shown in Equation 4-14 and Figure 4-21.</p>
<p>​		<em>Equation 4-14. Logistic function</em></p>
<p>​		$σ(t) &#x3D; \frac 1 {1 + exp (− t)}$</p>
<p><img src="https://s2.loli.net/2023/02/21/TLroG3YvJHnBSzc.png" alt="image-20230221203903776"></p>
<h4 id="Training-and-Cost-Function"><a href="#Training-and-Cost-Function" class="headerlink" title="Training and Cost Function"></a>Training and Cost Function</h4><p>$J(\theta)&#x3D;-\frac{1}{m}\sum_{i&#x3D;1}^{m}[y^{(i)}\log h_\theta(x^{(i)})+(1-y^{(i)})\log (1-h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j&#x3D;1}^n \theta_j^2$</p>
<p>其中，$h_\theta(x)$ 表示逻辑回归的假设函数，$y^{(i)}$ 是样本 i 的实际标签，$\theta$ 是模型参数，$\lambda$ 是正则化超参数。</p>
<h4 id="Decision-Boundaries"><a href="#Decision-Boundaries" class="headerlink" title="Decision Boundaries"></a>Decision Boundaries</h4><p>Description: The hyperparameter controlling the regularization strength of a Scikit-Learn <code>LogisticRegression</code> model is not <code>alpha</code> (as in other linear models), but its inverse: <code>C</code>. The higher the value of <code>C</code>, the less the model is regularized.</p>
<p>Just like the other linear models, logistic regression models can be regularized using ℓ1  or ℓ2  penalties. Scikit-Learn actually adds an ℓ2  penalty by default.</p>
<h4 id="Softmax-Regression"><a href="#Softmax-Regression" class="headerlink" title="Softmax Regression"></a>Softmax Regression</h4><p>The logistic regression model can be generalized to support multiple classes directly, without having to train and combine multiple binary classifiers (as discussed in Chapter 3). This is called <em>softmax regression</em>, or <em>multinomial logistic regression</em>.</p>
<p>Cross entropy is frequently used to measure how well a set of estimated class probabilities matches the target classes.</p>
<p><img src="https://s2.loli.net/2023/02/21/pycSFsqibBO7u3m.png" alt="image-20230221214109885"></p>
<p>Notice that when there are just two classes (<em>K</em> &#x3D; 2), this cost function is equivalent to the logistic regression cost function (log loss; see Equation 4-17).</p>
<p>Let’s use softmax regression to classify the iris plants into all three classes. ScikitLearn’s <code>LogisticRegression</code> classifier uses softmax regression automatically when you train it on more than two classes (assuming you use <code>solver=&quot;lbfgs&quot;</code>, which is the default). It also applies ℓ2  regularization by default, which you can control using the hyperparameter C, as mentioned earlier:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = iris.data[[<span class="string">&quot;petal length (cm)&quot;</span>, <span class="string">&quot;petal width (cm)&quot;</span>]].values</span><br><span class="line">y = iris[<span class="string">&quot;target&quot;</span>]</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">softmax_reg = LogisticRegression(C=<span class="number">30</span>, random_state=<span class="number">42</span>)</span><br><span class="line">softmax_reg.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">softmax_reg.predict([[<span class="number">5</span>, <span class="number">2</span>]])</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([2])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">softmax_reg.predict_proba([[<span class="number">5</span>, <span class="number">2</span>]]).<span class="built_in">round</span>(<span class="number">2</span>)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([[0. , 0.04, 0.96]])</p>
<h3 id="4-x-Exercises"><a href="#4-x-Exercises" class="headerlink" title="4.x Exercises"></a>4.x Exercises</h3><ol>
<li><p>Which linear regression training algorithm can you use if you have a training set with millions of features?</p>
<p>Batch&#x2F;Stochastic&#x2F;mini-batch GD</p>
</li>
<li><p>Suppose the features in your training set have very different scales. Which algorithms might suffer from this, and how? What can you do about it?</p>
<p>Normalization and Standardization</p>
</li>
<li><p>Can gradient descent get stuck in a local minimum when training a logistic regression model?</p>
<p>no</p>
</li>
<li><p>Do all gradient descent algorithms lead to the same model, provided you let them run long enough?</p>
<p>yeah?</p>
</li>
<li><p>Suppose you use batch gradient descent and you plot the validation error at every epoch. If you notice that the validation error consistently goes up, what is likely going on? How can you fix this?</p>
<p>early stopping</p>
</li>
<li><p>Is it a good idea to stop mini-batch gradient descent immediately when the validation error goes up?</p>
<p>数据偏差并且 SGD 训练过程具有随机性</p>
</li>
<li><p>Which gradient descent algorithm (among those we discussed) will reach the vicinity of the optimal solution the fastest? Which will actually converge? How can you make the others converge as well?</p>
<p>Stochastic GD; mini-batch GD; use a good learning schedule</p>
</li>
<li><p>Suppose you are using polynomial regression. You plot the learning curves and you notice that there is a large gap between the training error and the validation error. What is happening? What are three ways to solve this?</p>
<p>underfitting. more data; add epochs; and …</p>
</li>
<li><p>Suppose you are using ridge regression and you notice that the training error and the validation error are almost equal and fairly high. Would you say that the model suffers from high bias or high variance? Should you increase the regularization hyperparameter α or reduce it?</p>
<p>overfitting? reduce $\alpha$</p>
</li>
<li><p>Why would you want to use:</p>
<p>a. Ridge regression instead of plain linear regression (i.e., without any regularization)?</p>
<p>b. Lasso instead of ridge regression?</p>
<p>c. Elastic net instead of lasso regression?</p>
</li>
<li><p>Suppose you want to classify pictures as outdoor&#x2F;indoor and daytime&#x2F;nighttime. Should you implement two logistic regression classifiers or one softmax regression classifier?</p>
</li>
<li><p>Implement batch gradient descent with early stopping for softmax regression without using Scikit-Learn, only NumPy. Use it on a classification task such as the iris dataset.</p>
</li>
</ol>
<h2 id="5-Support-Vector-Machines"><a href="#5-Support-Vector-Machines" class="headerlink" title="5. Support Vector Machines"></a>5. Support Vector Machines</h2><p>A <em>support vector machine</em> (SVM) is a powerful and versatile machine learning model, capable of performing linear or nonlinear classification, regression, and even novelty detection. SVMs shine with small to medium-sized nonlinear datasets (i.e., hundreds to thousands of instances), especially for classification tasks. However, they don’t scale very well to very large datasets, as you will see.</p>
<h3 id="5-1-Linear-SVM-Classification"><a href="#5-1-Linear-SVM-Classification" class="headerlink" title="5.1 Linear SVM Classification"></a>5.1 Linear SVM Classification</h3><p>图5-1所示的数据集来自第4章末尾引用的鸢尾花数据集的一部分。两个类可以轻松地被一条直线（它们是线性可分离的）分开。左图显示了三种可能的线性分类器的决策边界。其中虚线所代表的模型表现非常糟糕，甚至都无法正确实现分类。其余两个模型在这个训练集上表现堪称完美，但是它们的<strong>决策边界与实例过于接近，导致在面对新实例时，表现可能不会太好</strong>。</p>
<p>相比之下，右图中的实线代表SVM分类器的决策边界，这条线不仅分离了两个类，并且尽可能远离了最近的训练实例。你可以将SVM分类器视为在类之间拟合可能的最宽的”街道”（平行的虚线所示）。因此这也叫作 <em>large margin classification</em> (大间隔分类)。</p>
<p><img src="https://s2.loli.net/2022/07/20/IoCQ5eWRG6vDqnB.png" alt="图5-1: 大间隔分类"></p>
<p>Notice that adding more training instances “off the street” will not affect the decision boundary at all: it is fully determined (or “supported”) by the instances located on the edge of the street. These instances are called the support vectors (they are circled in Figure 5-1).</p>
<p>Attension: SVM 对特征的缩放非常敏感，如图 5-2 所示，在左图中，垂直刻度比水平刻度大得多，因此可能的最宽的街道接近于水平。在特征缩放（例如使用Scikit-Learn的StandardScaler）后，决策边界看起来好了很多（见右图）。</p>
<p><img src="https://s2.loli.net/2022/07/20/Md2qe6a1UxpF4r9.png" alt="图5-2：特征缩放敏感性"></p>
<p>在 SVM 中，用超参数 <code>C</code> 表示约束模型程度，如下图：</p>
<p><img src="https://s2.loli.net/2022/07/20/HhUwSR8fV4McZXT.png" alt="图5-4：大间隔（左）与更少的间隔冲突（右）"></p>
<p>Suggestion: If your SVM model is overfitting, you can try regularizing it by reducing <code>C</code>.</p>
<p>The following Scikit-Learn code loads the iris dataset and trains a linear SVM classifier to detect <em>Iris virginica</em> flowers. The pipeline first scales the features, then uses a <code>LinearSVC</code> with <code>C=1</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 上图代码</span></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line">iris = load_iris(as_frame=<span class="literal">True</span>)</span><br><span class="line">X = iris.data[[<span class="string">&quot;petal length (cm)&quot;</span>, <span class="string">&quot;petal width (cm)&quot;</span>]].values</span><br><span class="line">y = (iris.target == <span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">svm_clf = make_pipeline(StandardScaler(),</span><br><span class="line">                      		       LinearSVC(C=<span class="number">1</span>, random_state=<span class="number">42</span>))</span><br><span class="line">svm_clf.fit(X, y)	</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_new = [[<span class="number">5.5</span>, <span class="number">1.7</span>], [<span class="number">5.0</span>, <span class="number">1.5</span>]]</span><br><span class="line">svm_clf.predict(X_new)</span><br></pre></td></tr></table></figure>

<h3 id="5-2-Nonlinear-SVM-Classification"><a href="#5-2-Nonlinear-SVM-Classification" class="headerlink" title="5.2 Nonlinear SVM Classification"></a>5.2 Nonlinear SVM Classification</h3><p>对于非线性数据的分类，和之前一样，可以增加多项式特征 <code>PolynomialFeatures</code>, followed by a <code>StandardScaler</code> and a <code>LinearSVC</code> classifier :  </p>
<p><img src="https://s2.loli.net/2023/02/22/mce8FygjurI1pBD.png" alt="image-20230222190829001"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">100</span>, noise=<span class="number">0.15</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">polynomial_svm_clf = make_pipeline( 								</span><br><span class="line">				    PolynomialFeatures(degree=<span class="number">3</span>),</span><br><span class="line">                                    StandardScaler(),</span><br><span class="line">                                    LinearSVC(C=<span class="number">10</span>, max_iter=<span class="number">10_000</span>, random_state=<span class="number">42</span>))</span><br><span class="line">polynomial_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>

<h4 id="Polynomial-Kernel"><a href="#Polynomial-Kernel" class="headerlink" title="Polynomial Kernel"></a>Polynomial Kernel</h4><p>Adding polynomial features is simple to implement and can work great with all sorts of machine learning algorithms (not just SVMs). That said, at a low polynomial degree this method cannot deal with very complex datasets, and with a high polynomial degree it creates <strong>a huge number of features</strong>, making the model too slow.</p>
<p>Fortunately, when using SVMs you can apply an almost miraculous mathematical technique called the <em>kernel trick</em> (which is explained later in this chapter). The kernel trick makes it possible to get the same result as if you had added many polynomial features, even with a very high degree, without actually having to add them. This means there’s no combinatorial explosion of the number of features. This trick is implemented by the <code>SVC</code> class. Let’s test it on the moons dataset:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">poly_kernel_svm_clf = make_pipeline(StandardScaler(),</span><br><span class="line">                                    SVC(kernel=<span class="string">&quot;poly&quot;</span>, degree=<span class="number">3</span>, coef0=<span class="number">1</span>, C=<span class="number">5</span>))</span><br><span class="line">poly_kernel_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>The hyperparameter coef0 controls how much the model is influenced by high-degree terms versus low-degree terms.</p>
<p><img src="https://s2.loli.net/2023/02/23/R3yUAx8BVN6Geg2.png" alt="image-20230223041300083"></p>
<h4 id="Similarity-Features"><a href="#Similarity-Features" class="headerlink" title="Similarity Features"></a>Similarity Features</h4><p>和增加多项式特征一样，增加相似度特征也可以使数据变为线性可分的。</p>
<h4 id="Gaussian-RBF-Kernel"><a href="#Gaussian-RBF-Kernel" class="headerlink" title="Gaussian RBF Kernel"></a>Gaussian RBF Kernel</h4><p>和多项式特征一样，如果数据集过大，需要增加的特征的数量级也会很大。因此可以使用另一种 kernel trick，如机器学习算法的 Gaussian RBF kernel ：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rbf_kernel_svm_clf = make_pipeline(StandardScaler(),</span><br><span class="line">                                   SVC(kernel=<span class="string">&quot;rbf&quot;</span>, gamma=<span class="number">5</span>, C=<span class="number">0.001</span>))</span><br><span class="line">rbf_kernel_svm_clf.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>Increasing gamma(γ) makes the bell-shaped curve narrower (see Figure 5-9). Conversely, a small gamma value makes the bell-shaped curve wider: instances have a larger range of influence, and the decision boundary ends up smoother. So γ acts like a regularization hyperparameter: if your model is overfitting, you should reduce γ; if it is underfitting, you should increase γ (similar to the C hyperparameter).</p>
<p><img src="https://s2.loli.net/2023/02/23/5vgyjhEZpilIABK.png" alt="image-20230223043203576"></p>
<p>Suggestion: With so many kernels to choose from, how can you decide which one to use? As a rule of thumb, you should always try the linear kernel first. The <code>LinearSVC</code> class is much faster than <code>SVC(kernel=&quot;linear&quot;)</code>, especially if the training set is very large. If it is not too large, you should also try kernelized <code>SVMs</code>, starting with the Gaussian <code>RBF</code>(default) kernel; it often works really well. Then, if you have spare time and computing power, you can experiment with a few other kernels using hyperparameter search. If there are kernels specialized for your training set’s data structure, make sure to give them a try too.</p>
<h4 id="SVM-Classes-and-Computational-Complexity"><a href="#SVM-Classes-and-Computational-Complexity" class="headerlink" title="SVM Classes and Computational Complexity"></a>SVM Classes and Computational Complexity</h4><p>The <code>LinearSVC</code> class is based on the <code>liblinear</code> library, which implements an optimized algorithm for linear SVMs. It does not support the <strong>kernel trick</strong>, but it scales almost linearly with the number of training instances and the number of features. Its training time complexity is roughly $O(m × n)$. The algorithm takes longer if you require very high precision. This is controlled by the tolerance hyperparameter $ϵ$ (<em>epsilon</em>, called <code>tol</code> in Scikit-Learn). In most classification tasks, the default tolerance is fine.</p>
<p>The <code>SVC</code> class is based on the <code>libsvm</code> library, which implements an algorithm that supports the kernel trick. The training time complexity is usually between $O(m^2 × n)$ and $O(m^3  × n)$. Unfortunately, this means that it gets dreadfully slow when the number of training instances gets large (e.g., hundreds of thousands of instances), so this algorithm is best for small or medium-sized nonlinear training sets. It scales well with the number of features, especially with sparse features (i.e., when each instance has few nonzero features). In this case, the algorithm scales roughly with the average number of nonzero features per instance.</p>
<p>The <code>SGDClassifier</code> class also performs large margin classification by default, and its hyperparameters–especially the regularization hyperparameters (<code>alpha </code>and <code>penalty</code>) and the <code>learning_rate</code>–can be adjusted to produce similar results as the linear SVMs. For training it uses stochastic gradient descent (see Chapter 4), which allows incremental learning and uses little memory, so you can use it to train a model on a large dataset that does not fit in RAM (i.e., for out-of-core learning). Moreover, it scales very well, as its computational complexity is $O(m × n)$. Table 5-1 compares Scikit-Learn’s SVM classification classes.</p>
<p><img src="https://s2.loli.net/2023/02/23/23T1XYEyqp7QcIx.png" alt="image-20230223045751417"></p>
<p>Now let’s see how the <code>SVM</code> algorithms can also be used for linear and nonlinear regression.</p>
<h3 id="5-3-SVM-Regression"><a href="#5-3-SVM-Regression" class="headerlink" title="5.3 SVM Regression"></a>5.3 SVM Regression</h3><p>To use SVMs for regression instead of classification, the trick is to tweak the objective: instead of trying to fit the largest possible street between two classes while limiting margin violations, SVM regression tries to fit as many instances as possible on the street while limiting margin violations (i.e., instances off the street). The width of the street is controlled by a hyperparameter, ϵ. Figure 5-10 shows two linear SVM regression models trained on some linear data, one with a small margin (ϵ &#x3D; 0.5) and the other with a larger margin (ϵ &#x3D; 1.2). 和 C 相反，但样本数大小不会影响 ϵ 的街道大小。</p>
<p><img src="https://s2.loli.net/2023/02/25/9po1VylBJn2SgMi.png" alt="image-20230225203129752"></p>
<p>Reducing ϵ increases the number of support vectors, which regularizes the model. Moreover, if you add more training instances within the margin, it will not affect the model’s predictions; thus, the model is said to be <em>ϵ-insensitive</em>.</p>
<p>You can use Scikit-Learn’s <code>LinearSVR</code> class to perform linear SVM regression. The following code produces the model represented on the left in Figure 5-10:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVR</span><br><span class="line"></span><br><span class="line">svm_reg = make_pipeline(StandardScaler(),</span><br><span class="line">                        LinearSVR(epsilon=<span class="number">0.5</span>, random_state=<span class="number">42</span>))</span><br><span class="line">svm_reg.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>The SVR class is the regression equivalent of the SVC class, and the LinearSVR class is the regression equivalent of the LinearSVC class. The LinearSVR class scales linearly with the size of the training set (just like the LinearSVC class), while the SVR class gets much too slow when the training set grows very large (just like the SVC class).</p>
<p>Description: SVMs can also be used for novelty detection, as you will see in Chapter 9.</p>
<h3 id="5-4-Under-the-Hood-of-Linear-SVM-Classifiers"><a href="#5-4-Under-the-Hood-of-Linear-SVM-Classifiers" class="headerlink" title="5.4 Under the Hood of Linear SVM Classifiers"></a>5.4 Under the Hood of Linear SVM Classifiers</h3><p>二次规划（Quadratic Programming, QP）是线性规划的扩展，它的目标函数是一个二次函数，约束条件是线性的不等式或等式。一般形式的二次规划问题可以表示为：</p>
<p>$$\min_{\mathbf{x}} \frac{1}{2} \mathbf{x}^\top \mathbf{P} \mathbf{x} + \mathbf{q}^\top \mathbf{x} + r$$</p>
<p>$$\text{s.t. } \mathbf{Gx} \leq \mathbf{h}, \mathbf{Ax} &#x3D; \mathbf{b}$$</p>
<p>其中，$\mathbf{x}$ 是待求解的向量，$\mathbf{P}$ 是一个对称正定矩阵，$\mathbf{q}$ 是一个向量，$r$ 是一个常数。$\mathbf{G}$ 和 $\mathbf{h}$ 分别是 $m \times n$ 的矩阵和 $m$ 维向量，$\mathbf{A}$ 是 $p \times n$ 的矩阵，$\mathbf{b}$ 是 $p$ 维向量。</p>
<p>二次规划问题的解法相对于线性规划问题更加困难，但是也有很多有效的方法可以求解。其中，一些基本的算法包括：内点法、梯度投影法、坐标轮换法等。在机器学习领域，二次规划问题通常会出现在支持向量机（SVM）的求解中，例如线性核和高斯径向基核的情况。</p>
<hr>
<p>线性规划（Linear Programming, LP）是一种数学优化技术，用于在一组线性约束条件下，最大化或最小化线性目标函数的值。这种技术可以在诸如资源分配、生产规划和运输等实际问题中得到广泛应用。</p>
<p>下面给出一个简单的线性规划示例：假设一家工厂有两种生产线路，每种生产线路需要两种不同的材料（材料1和材料2）生产，且两种生产线路可以同时生产。第一种生产线路每小时可以生产 2 个产品，而第二种生产线路每小时可以生产 3 个产品。公司有 50 个材料1和 80 个材料2，每个产品可以卖 $10。现在的问题是，应该如何分配材料，以使公司的收益最大化？</p>
<p>可以使用线性规划来解决这个问题。假设 $x_1$ 和 $x_2$ 分别表示选择第一种和第二种生产线路的时间（以小时为单位），则目标函数为 $10(2x_1+3x_2)$，表示一小时内公司能够获得的总收益。约束条件为：</p>
<ul>
<li>$x_1+x_2 \leq 50$，因为材料1的数量是有限的；</li>
<li>$2x_1+3x_2 \leq 80$，因为材料2的数量是有限的。</li>
</ul>
<p>同时，因为不能生产负数的时间，所以还需要添加 $x_1 \geq 0$ 和 $x_2 \geq 0$ 的限制条件。</p>
<p>这个问题的线性规划形式可以表示为：</p>
<p>$$\max_{x_1,x_2} 10(2x_1+3x_2)$$</p>
<p>$$\text{s.t. } x_1 + x_2 \leq 50, 2x_1 + 3x_2 \leq 80, x_1 \geq 0, x_2 \geq 0$$</p>
<p>通过线性规划求解器求解这个问题，可以得到最优解 $x_1&#x3D;16.67, x_2&#x3D;16.67$，此时公司的最大收益为 $333.33$ 美元。</p>
<h3 id="5-5-The-Dual-Problem"><a href="#5-5-The-Dual-Problem" class="headerlink" title="5.5 The Dual Problem"></a>5.5 The Dual Problem</h3><p>Given a constrained optimization problem, known as the <em>primal problem</em>, it is possible to express a different but closely related problem (by <em>Lagrange Multiplier Method</em>), called its <em>dual problem</em>.</p>
<p>The dual problem is faster to solve than the primal one when the number of training instances is smaller than the number of features. More importantly, the dual problem makes the kernel trick possible, while the primal problem does not. So what is this kernel trick, anyway?</p>
<p>拉格朗日乘数法是一种处理带有等式约束条件的最优化问题的方法。它将原问题转化为一个无约束问题，通过引入拉格朗日乘数将等式约束条件转化为目标函数的一部分。</p>
<p>具体来说，对于带有等式约束条件的最优化问题：</p>
<p>$$\min_{\mathbf{x}} f(\mathbf{x}) \quad \text{s.t.} \quad g_i(\mathbf{x}) &#x3D; 0, \quad i&#x3D;1,\ldots,m$$</p>
<p>其中，$f(\mathbf{x})$是目标函数，$g_i(\mathbf{x})$是等式约束条件。引入拉格朗日乘数$\boldsymbol{\lambda}&#x3D;[\lambda_1,\ldots,\lambda_m]$，将等式约束条件转化为目标函数的一部分，得到拉格朗日函数：</p>
<p>$$L(\mathbf{x},\boldsymbol{\lambda}) &#x3D; f(\mathbf{x}) + \boldsymbol{\lambda}^T \mathbf{g}(\mathbf{x})$$</p>
<p>其中，$\mathbf{g}(\mathbf{x})&#x3D;[g_1(\mathbf{x}),\ldots,g_m(\mathbf{x})]^T$是等式约束条件向量。则原问题可以转化为一个无约束的问题：</p>
<p>$$\min_{\mathbf{x}} \max_{\boldsymbol{\lambda}} L(\mathbf{x},\boldsymbol{\lambda}) &#x3D; \min_{\mathbf{x}}\left[ f(\mathbf{x}) + \max_{\boldsymbol{\lambda}}\boldsymbol{\lambda}^T \mathbf{g}(\mathbf{x})\right]$$</p>
<p>其中，对于任意给定的$\mathbf{x}$，$\max_{\boldsymbol{\lambda}}\boldsymbol{\lambda}^T \mathbf{g}(\mathbf{x})$就是拉格朗日函数$L(\mathbf{x},\boldsymbol{\lambda})$对$\boldsymbol{\lambda}$的极大值，记作$L^*(\mathbf{x})$。于是，原问题可以转化为：</p>
<p>$$\min_{\mathbf{x}} \left[ f(\mathbf{x}) + L^*(\mathbf{x})\right]$$</p>
<p>这个问题的解和原问题的解是等价的。</p>
<p>拉格朗日乘数法在求解约束优化问题时具有很大的优越性，特别是当原问题不易直接求解时，通过引入拉格朗日乘数，将问题转化为等价的无约束问题，可以更加方便地求解。</p>
<h4 id="Kernelized-SVMs"><a href="#Kernelized-SVMs" class="headerlink" title="Kernelized SVMs"></a>Kernelized SVMs</h4><p><img src="https://s2.loli.net/2023/02/25/2TXNcJ5BEyoVYdL.png" alt="image-20230225224739850"></p>
<h3 id="5-x-Exercises"><a href="#5-x-Exercises" class="headerlink" title="5.x Exercises"></a>5.x Exercises</h3><ol>
<li>What is the fundamental idea behind support vector machines? </li>
<li>What is a support vector?</li>
<li>Why is it important to scale the inputs when using SVMs?</li>
<li>Can an SVM classifier output a confidence score when it classifies an instance? What about a probability?</li>
<li>How can you choose between LinearSVC, SVC, and SGDClassifier?</li>
<li>Say you’ve trained an SVM classifier with an RBF kernel, but it seems to underfit the training set. Should you increase or decrease <code>γ (gamma)</code>? What about <code>C</code>?</li>
<li>What does it mean for a model to be <em>ϵ-insensitive</em>? </li>
<li>What is the point of using the kernel trick? </li>
<li>Train a LinearSVC on a linearly separable dataset. Then train an SVC and a SGDClassifier on the same dataset. See if you can get them to produce roughly the same model.</li>
<li>Train an SVM classifier on the wine dataset, which you can load using <code>sklearn.datasets.load_wine()</code>. This dataset contains the chemical analyses of 178 wine samples produced by 3 different cultivators: the goal is to train a classification model capable of predicting the cultivator based on the wine’s chemical analysis. Since SVM classifiers are binary classifiers, you will need to use one-versus-all to classify all three classes. What accuracy can you reach?</li>
<li>Train and fine-tune an SVM regressor on the California housing dataset. You can use the original dataset rather than the tweaked version we used in Chapter 2, which you can load using <code>sklearn.datasets.fetch_california_housing()</code>. The targets represent hundreds of thousands of dollars. Since there are over 20,000 instances, SVMs can be slow, so for hyperparameter tuning you should use far fewer instances (e.g., 2,000) to test many more hyperparameter combinations. What is your best model’s RMSE?</li>
</ol>
<h2 id="6-Decision-Trees"><a href="#6-Decision-Trees" class="headerlink" title="6. Decision Trees"></a>6. Decision Trees</h2><p>决策树是一种基于树结构进行决策的算法。它是一种监督学习算法，适用于分类和回归问题。决策树通过对数据进行分类或者预测的过程，使用树形结构来表示决策过程，每个节点表示一个特征或者属性，每个分支代表特征或者属性的取值，叶子节点表示决策结果。</p>
<p>决策树算法有以下几个步骤：</p>
<ol>
<li>特征选择：从数据集中选择一个特征作为当前节点的划分依据，选择的依据可以使用信息增益、信息增益率、基尼指数等方法。</li>
<li>树的构建：将数据集根据选定的特征进行划分，得到若干个子集，每个子集对应一个子节点，将子集继续递归划分，直到满足停止条件（例如叶子节点的样本纯度达到一定阈值）。</li>
<li>树的剪枝：剪枝是指通过去掉一些子树，使得树的结构更加简洁，泛化性能更好，常用的剪枝方法有预剪枝和后剪枝。</li>
<li>预测：通过对新样本的特征进行判定，从树的根节点开始遍历，直到叶子节点，得到决策结果。</li>
</ol>
<p>决策树算法的优点包括：易于理解和解释，适用于离散和连续数据，具有很好的可扩展性和易于处理大规模数据集的能力。缺点是容易产生过拟合，因此需要采用剪枝等方法进行优化。</p>
<h3 id="6-1-Training-and-Visuliazing-a-Decision-Tree"><a href="#6-1-Training-and-Visuliazing-a-Decision-Tree" class="headerlink" title="6.1 Training and Visuliazing a Decision Tree"></a>6.1 Training and Visuliazing a Decision Tree</h3><p>To understand decision trees, let’s build one and take a look at how it makes predictions. The following code trains a DecisionTreeClassifier on the iris dataset (see Chapter 4):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">iris = load_iris(as_frame=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">X_iris = iris.data[[<span class="string">&quot;petal length (cm)&quot;</span>, <span class="string">&quot;petal width (cm)&quot;</span>]].values</span><br><span class="line">y_iris = iris.target</span><br><span class="line"></span><br><span class="line">tree_clf = DecisionTreeClassifier(max_depth=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_clf.fit(X_iris, y_iris)</span><br></pre></td></tr></table></figure>

<p>You can visualize the trained decision tree by first using the export_graphviz() function to output a graph definition file called iris_tree.dot:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> export_graphviz</span><br><span class="line"></span><br><span class="line">export_graphviz(</span><br><span class="line">    tree_clf,</span><br><span class="line">    out_file=<span class="string">&quot;iris_tree.dot&quot;</span>,</span><br><span class="line">    feature_names=[<span class="string">&quot;petal length (cm)&quot;</span>, <span class="string">&quot;petal width (cm)&quot;</span>],</span><br><span class="line">    class_names=iris.target_names,</span><br><span class="line">    rounded=<span class="literal">True</span>,</span><br><span class="line">    filled=<span class="literal">True</span></span><br><span class="line">)</span><br></pre></td></tr></table></figure>

<p>Then you can use graphviz.Source.from_file() to load and display the file in a Jupyter notebook:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> graphviz <span class="keyword">import</span> Source</span><br><span class="line"></span><br><span class="line">Source.from_file(<span class="string">&quot;iris_tree.dot&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>or use command line to visualize decision tree: </p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta prompt_"># </span><span class="language-bash">extra code</span></span><br><span class="line">!dot -Tpng &#123;IMAGES_PATH / &quot;iris_tree.dot&quot;&#125; -o &#123;IMAGES_PATH / &quot;iris_tree.png&quot;&#125;</span><br></pre></td></tr></table></figure>

<p>Your first decision tree looks like Figure 6-1.</p>
<p><img src="https://s2.loli.net/2023/02/27/nvsBHFlwofNkD8p.png" alt="image-20230227024409267"></p>
<h3 id="6-2-Making-Predictions"><a href="#6-2-Making-Predictions" class="headerlink" title="6.2 Making Predictions"></a>6.2 Making Predictions</h3><p>Description: One of the many qualities of decision trees is that they require very little data preparation. In fact, they don’t require feature scaling or centering at all. (见其预测过程)</p>
<p>a node’s gini attribute measures its <em>Gini impurity (基尼不纯度)</em>: a node is “pure” (<code>gini=0</code>) if all training instances it applies to belong to the same class. </p>
<p>Equation 6-1 shows how the training algorithm computes the Gini impurity G<del>i</del>  of the i^th^ node. The depth-2 left node has a Gini impurity equal to 1 – (0&#x2F;54)^2^  – (49&#x2F;54)^2^  – (5&#x2F;54)^2^  ≈ 0.168.</p>
<p>​		<em>Equation 6-1. Gini impurity</em></p>
<p>​		$G_i &#x3D; 1 − ∑_{k &#x3D; 1} ^n p_i, k^2$</p>
<p>In this equation:</p>
<p>• G<del>i</del> is the Gini impurity of the <em>i</em> th node.</p>
<p>• p<del>i,k</del> is the ratio of class <em>k</em> instances among the training instances in the <em>i</em>^th^ node. </p>
<p>Figure 6-2 shows this decision tree’s decision boundaries. （即图 6-1 的条件语句）</p>
<p><img src="https://s2.loli.net/2023/02/27/6faJ9PsKcCqEkr2.png" alt="image-20230227031521115"></p>
<blockquote>
<p><strong>Model Interpretation: White Box Versus Black Box</strong></p>
<p>Decision trees are intuitive, and their decisions are easy to interpret. Such models are often called white box models. In contrast, as you will see, random forests and neural networks are generally considered black box models. They make great predictions, and you can easily check the calculations that they performed to make these predic‐ tions; nevertheless, it is usually hard to explain in simple terms why the predictions were made. For example, if a neural network says that a particular person appears in a picture, it is hard to know what contributed to this prediction: Did the model recog‐ nize that person’s eyes? Their mouth? Their nose? Their shoes? Or even the couch that they were sitting on? Conversely, decision trees provide nice, simple classification rules that can even be applied manually if need be (e.g., for flower classification). The field of interpretable ML aims at creating ML systems that can explain their decisions in a way humans can understand. This is important in many domains—for example, to ensure the system does not make unfair decisions.</p>
</blockquote>
<h3 id="6-3-Estimating-Class-Probabilities"><a href="#6-3-Estimating-Class-Probabilities" class="headerlink" title="6.3 Estimating Class Probabilities"></a>6.3 Estimating Class Probabilities</h3><p><img src="https://s2.loli.net/2023/02/27/bofdAa6XKt2EgHL.png" alt="image-20230227090915434"></p>
<h3 id="6-4-The-CART-Training-Algorithm"><a href="#6-4-The-CART-Training-Algorithm" class="headerlink" title="6.4 The CART Training Algorithm"></a>6.4 The CART Training Algorithm</h3><p>Scikit-Learn uses the <em>Classification and Regression Tree</em> (CART) algorithm to train decision trees (also called “growing” trees). The algorithm works by first splitting the training set into two subsets using a single feature <em>k</em> and a threshold <em>t<del>k</del></em> (e.g., “petal length ≤ 2.45 cm”). How does it choose <em>k</em> and <em>t<del>k</del></em>? It searches for the pair (<em>k</em>,  <em>t<del>k</del></em>) that produces the purest subsets, weighted by their size. Equation 6-2 gives the cost function that the algorithm tries to minimize.</p>
<p><img src="https://s2.loli.net/2023/02/27/Uz2QkBKZylT9LvX.png" alt="image-20230227092835290"></p>
<p>Once the CART algorithm has successfully split the training set in two, it splits the subsets using the same logic, then the sub-subsets, and so on, recursively. It stops recursing once it reaches the maximum depth (defined by the <code>max_depth</code> hyperparameter), or if it cannot find a split that will reduce impurity. A few other hyperparameters (described in a moment) control additional stopping con‐ ditions: <code>min_samples_split</code>, <code>min_samples_leaf</code>, <code>min_weight_fraction_leaf</code>, and <code>max_leaf_nodes</code>.</p>
<h3 id="6-5-Computational-Complexity"><a href="#6-5-Computational-Complexity" class="headerlink" title="6.5 Computational Complexity"></a>6.5 Computational Complexity</h3><p>Making predictions requires traversing the decision tree from the root to a leaf. Decision trees generally are approximately balanced, so traversing the decision tree requires going through roughly $O(log_2(m))$ nodes, So predictions are very fast, even when dealing with large training sets.</p>
<p>The training algorithm compares all features (or less if <code>max_features</code> is set) on all samples at each node. Comparing all features on all samples at each node results in a training complexity of $O(n × m \space log_2(m))$.</p>
<h3 id="6-6-Gini-Impurity-or-Entropy"><a href="#6-6-Gini-Impurity-or-Entropy" class="headerlink" title="6.6 Gini Impurity or Entropy ?"></a>6.6 Gini Impurity or Entropy ?</h3><h3 id="6-7-Regularization-Hyperparameters"><a href="#6-7-Regularization-Hyperparameters" class="headerlink" title="6.7 Regularization Hyperparameters"></a>6.7 Regularization Hyperparameters</h3><p>决策树默认情况下（即无约束）是过拟合的，称为 <em>nonparametric model</em>，表示模型中的参数数量没有事先设定好。 In contrast, a <em>parametric model</em>, such as a linear model, has a predetermined number of parameters, so its degree of freedom is limited, reducing the risk of overfitting.</p>
<p>决策树模型中，建议正则化的超参数：</p>
<ul>
<li><p><code>max_depth</code> (recommended)</p>
<p>树的最大深度，默认 None</p>
</li>
<li><p><code>max_features</code></p>
<p>Maximum number of features that are evaluated for splitting at each node</p>
</li>
<li><p><code>max_leaf_nodes</code></p>
<p>Maximum number of leaf nodes</p>
</li>
<li><p><code>min_samples_split</code></p>
<p>Minimum number of samples a node must have before it can be split</p>
</li>
<li><p><code>min_samples_leaf</code></p>
<p>Minimum number of samples a leaf node must have to be created</p>
</li>
<li><p><code>min_weight_fraction_leaf</code></p>
<p>Same as <code>min_samples_leaf</code> but expressed as a fraction of the total number of weighted instances</p>
</li>
</ul>
<p>Increasing min_* hyperparameters or reducing max_* hyperparameters will regularize the model.</p>
<p>Let’s test regularization on the moons dataset, introduced in Chapter 5. We’ll train one decision tree without regularization, and another with min_samples_leaf&#x3D;5. Here’s the code; Figure 6-3 shows the decision boundaries of each tree:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line">X_moons, y_moons = make_moons(n_samples=<span class="number">150</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">tree_clf1 = DecisionTreeClassifier(random_state=<span class="number">42</span>)</span><br><span class="line">tree_clf2 = DecisionTreeClassifier(min_samples_leaf=<span class="number">5</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_clf1.fit(X_moons, y_moons)</span><br><span class="line">tree_clf2.fit(X_moons, y_moons)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/28/Rv479Ab5aT2qNWJ.png" alt="image-20230228144248536"></p>
<p>The unregularized model on the left is clearly overfitting, and the regularized model on the right will probably generalize better. We can verify this by evaluating both trees on a test set generated using a different random seed:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_moons_test, y_moons_test = make_moons(n_samples=<span class="number">1000</span>, noise=<span class="number">0.2</span>,</span><br><span class="line">                                        random_state=<span class="number">43</span>)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tree_clf1.score(X_moons_test, y_moons_test)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.898</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tree_clf2.score(X_moons_test, y_moons_test)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.92</p>
<p>Indeed, the second tree has a better accuracy on the test set.</p>
<h3 id="6-8-Regression"><a href="#6-8-Regression" class="headerlink" title="6.8 Regression"></a>6.8 Regression</h3><p>Decision trees are also capable of performing regression tasks. Let’s build a regression tree using Scikit-Learn’s DecisionTreeRegressor class, training it on a noisy quadratic dataset with <code>max_depth</code>&#x3D;2:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X_quad = np.random.rand(<span class="number">200</span>, <span class="number">1</span>) - <span class="number">0.5</span> <span class="comment"># a single random input feature</span></span><br><span class="line">y_quad = X_quad ** <span class="number">2</span> + <span class="number">0.025</span> * np.random.randn(<span class="number">200</span>, <span class="number">1</span>)</span><br><span class="line">tree_reg = DecisionTreeRegressor(max_depth=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_reg.fit(X_quad, y_quad)</span><br></pre></td></tr></table></figure>

<p>The resulting tree is represented in Figure 6-4.</p>
<p><img src="https://s2.loli.net/2023/02/28/7EK6gcWte5Gf9Su.png" alt="image-20230228162931614"></p>
<p>This prediction is the average target value of the 110 training instances associated with this leaf node, and it results in a mean squared error equal to 0.015 over these 110 instances.</p>
<p><img src="https://s2.loli.net/2023/02/28/pU7htMAO8yqCzWN.png" alt="image-20230228163248625"></p>
<p>The CART algorithm works as described earlier, except that instead of trying to split the training set in a way that minimizes impurity, it now tries to split the training set in a way that minimizes the MSE. Equation 6-4 shows the cost function that the algorithm tries to minimize.</p>
<p>Just like for classification tasks, decision trees are prone to overfitting when dealing with regression tasks. Without any regularization (i.e., using the default hyperparameters), you get the predictions on the left in Figure 6-6. These predictions are obviously overfitting the training set very badly. Just setting min_samples_leaf&#x3D;10 results in a much more reasonable model, represented on the right in Figure 6-6.</p>
<p><img src="https://s2.loli.net/2023/02/28/sVdWUaGxtHAS6e4.png" alt="image-20230228163502091"></p>
<h3 id="6-9-Sensitivity-to-Axis-Orientation"><a href="#6-9-Sensitivity-to-Axis-Orientation" class="headerlink" title="6.9 Sensitivity to Axis Orientation"></a>6.9 Sensitivity to Axis Orientation</h3><p><img src="https://s2.loli.net/2023/02/28/CSWeN3KVdzxiQ8B.png" alt="image-20230228164820876"></p>
<p>One way to limit this problem is to scale the data, then apply a <em>principal component analysis</em> transformation. We will look at PCA in detail in Chapter 8, but for now you only need to know that it rotates the data in a way that reduces the correlation between the features, which often (not always) makes things easier for trees.</p>
<p>Let’s create a small pipeline that scales the data and rotates it using PCA, then train a DecisionTreeClassifier on that data. Figure 6-8 shows the decision boundaries of that tree: as you can see, the rotation makes it possible to fit the dataset pretty well using only one feature, z1 , which is a linear function of the original petal length and width. Here’s the code:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">pca_pipeline = make_pipeline(StandardScaler(), PCA())</span><br><span class="line">X_iris_rotated = pca_pipeline.fit_transform(X_iris)</span><br><span class="line">tree_clf_pca = DecisionTreeClassifier(max_depth=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_clf_pca.fit(X_iris_rotated, y_iris)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/02/28/jWrgNLeIaqSsE9n.png" alt="image-20230228165344613"></p>
<h3 id="6-10-Decision-Trees-Have-a-High-Variance"><a href="#6-10-Decision-Trees-Have-a-High-Variance" class="headerlink" title="6.10 Decision Trees Have a High Variance"></a>6.10 Decision Trees Have a High Variance</h3><p>该小节解释了为什么要使用集成学习。</p>
<p>More generally, the main issue with decision trees is that they have quite a high variance: small changes to the hyperparameters or to the data may produce very different models. In fact, since the training algorithm used by Scikit-Learn is stochas‐ tic—it randomly selects the set of features to evaluate at each node—even retraining the same decision tree on the exact same data may produce a very different model, such as the one represented in Figure 6-9 (unless you set the random_state hyper‐ parameter). As you can see, it looks very different from the previous decision tree (Figure 6-2).</p>
<p><img src="https://s2.loli.net/2023/02/28/QJN2RbOzoK7hpYw.png" alt="image-20230228223123292"></p>
<p>Luckily, <strong>by averaging predictions over many trees</strong>, it’s possible to reduce variance significantly. Such an <em>ensemble</em> of trees is called a random forest, and it’s one of the most powerful types of models available today, as you will see in the next chapter.</p>
<h3 id="6-x-Exercises"><a href="#6-x-Exercises" class="headerlink" title="6.x Exercises"></a>6.x Exercises</h3><ol>
<li><p>What is the approximate depth of a decision tree trained (without restrictions) on a training set with one million instances?</p>
<p>same one million ?</p>
</li>
<li><p>Is a node’s Gini impurity generally lower or higher than its parent’s? Is it <em>generally</em> lower&#x2F;higher, or <em>always</em> lower&#x2F;higher?</p>
<p>general</p>
</li>
<li><p>If a decision tree is overfitting the training set, is it a good idea to try decreasing <code>max_depth</code>?</p>
<p>yes</p>
</li>
<li><p>If a decision tree is underfitting the training set, is it a good idea to try scaling the input features?</p>
<p>no</p>
</li>
<li><p>If it takes one hour to train a decision tree on a training set containing one million instances, roughly how much time will it take to train another decision tree on a training set containing ten million instances? Hint: consider the CART algorithm’s computational complexity.</p>
</li>
<li></li>
</ol>
<h2 id="7-Ensemble-Learning-and-Random-Forests"><a href="#7-Ensemble-Learning-and-Random-Forests" class="headerlink" title="7. Ensemble Learning and Random Forests"></a>7. Ensemble Learning and Random Forests</h2><p><strong>A group of predictors</strong> is called an <em>ensemble</em>, they aggregate their answers. This technique is called <em>ensemble learning</em>, and an ensemble learning algorithm is called an <em>ensemble method</em>.</p>
<p>As an example of an ensemble method, you can train a group of decision tree classifiers, each on a different random subset of the training set. You can then obtain the predictions of all the individual trees, and the class that gets the most votes is the ensemble’s prediction (see the last exercise in Chapter 6). Such an ensemble of decision trees is called a <em>random forest</em>, and despite its simplicity, this is one of the most powerful machine learning algorithms available today.</p>
<p>In this chapter we will examine the most popular ensemble methods, including voting classifiers, bagging and pasting ensembles, random forests, and boosting, and stacking ensembles.</p>
<h3 id="7-1-Voting-Classifiers"><a href="#7-1-Voting-Classifiers" class="headerlink" title="7.1 Voting Classifiers"></a>7.1 Voting Classifiers</h3><p><img src="https://s2.loli.net/2023/03/01/azEHnY5mNxCq6cW.png" alt="image-20230301003038670"></p>
<p>A very simple way to create an even better classifier is to aggregate the predictions of each classifier: the class that gets the most votes is the ensemble’s prediction. This majority-vote classifier is called a <em>hard voting</em> (硬投票) classifier (see Figure 7-2).</p>
<p><img src="https://s2.loli.net/2023/03/01/bqWGVBSNvDAifht.png" alt="image-20230301003058886"></p>
<p>Somewhat surprisingly, this voting classifier often achieves a higher accuracy than the best classifier in the ensemble. In fact, even if each classifier is a weak learner (meaning it does only slightly better than random guessing), the ensemble can still be a strong learner (achieving high accuracy), provided there are a sufficient number of weak learners in the ensemble and they are sufficiently diverse.</p>
<p><img src="https://s2.loli.net/2023/03/01/feOdyFjxt4ubamH.png" alt="image-20230301005022201"></p>
<p>They are likely to make the same types of errors, so there will be many majority votes for the wrong class, reducing the ensemble’s accuracy.</p>
<p>Suggestion: Ensemble methods work best when the predictors are as independ‐ ent from one another as possible. One way to get diverse classifiers is to train them using very different algorithms. This increases the chance that they will make very different types of errors, improving the ensemble’s accuracy.</p>
<p>Scikit-Learn provides a <code>VotingClassifier</code> class that’s quite easy to use:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier, VotingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LogisticRegression</span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">500</span>, noise=<span class="number">0.30</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line">voting_clf = VotingClassifier(</span><br><span class="line">    estimators=[</span><br><span class="line">        (<span class="string">&#x27;lr&#x27;</span>, LogisticRegression(random_state=<span class="number">42</span>)),</span><br><span class="line">        (<span class="string">&#x27;rf&#x27;</span>, RandomForestClassifier(random_state=<span class="number">42</span>)),</span><br><span class="line">        (<span class="string">&#x27;svc&#x27;</span>, SVC(random_state=<span class="number">42</span>))</span><br><span class="line">    ]</span><br><span class="line">)</span><br><span class="line">voting_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>When you fit a <code>VotingClassifier</code>, it clones every estimator and fits the clones. The original estimators are available via the <code>estimators</code> attribute, while the fitted clones are available via the <code>estimators_</code> attribute. If you prefer a dict rather than a list, you can use <code>named_estimators</code> or <code>named_estimators_</code> instead. To begin, let’s look at each fitted classifier’s accuracy on the test set:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">for</span> name, clf <span class="keyword">in</span> voting_clf.named_estimators_.items():</span><br><span class="line">    <span class="built_in">print</span>(name, <span class="string">&quot;=&quot;</span>, clf.score(X_test, y_test))</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; lr &#x3D; 0.864</p>
<p>&gt;&gt;&gt; rf &#x3D; 0.896</p>
<p>&gt;&gt;&gt; svc &#x3D; 0.896</p>
<p>When you call the voting classifier’s predict() method, it performs hard voting. For example, the voting classifier predicts class 1 for the first instance of the test set, because two out of three classifiers predict that class:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">voting_clf.predict(X_test[:<span class="number">1</span>])</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([1])</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[clf.predict(X_test[:<span class="number">1</span>]) <span class="keyword">for</span> clf <span class="keyword">in</span> voting_clf.estimators_]</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; [array([1]), array([1]), array([0])]</p>
<p>Now let’s look at the performance of the voting classifier on the test set:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">voting_clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.912</p>
<p>There you have it! The voting classifier outperforms all the individual classifiers.</p>
<p>If all classifiers are able to estimate class probabilities (i.e., if they all have a <code>predict_proba()</code> method), then you can tell Scikit-Learn to predict the class with the highest class probability, averaged over all the individual classifiers. This is called <em>soft voting</em> (软投票). It often achieves higher performance than hard voting because it gives more weight to highly confident votes. All you need to do is set the voting classi‐ fier’s voting hyperparameter to “soft”, and ensure that all classifiers can estimate class probabilities. This is not the case for the SVC class by default, so you need to set its probability hyperparameter to True (this will make the SVC class use cross-validation to estimate class probabilities, slowing down training, and it will add a predict_proba() method). Let’s try that:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">voting_clf.voting = <span class="string">&#x27;soft&#x27;</span></span><br><span class="line">voting_clf.named_estimators[<span class="string">&quot;svc&quot;</span>].probability = <span class="literal">True</span></span><br><span class="line">voting_clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">voting_clf.score(X_test, y_test)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.92</p>
<p>We reach 92% accuracy simply by using soft voting—not bad!</p>
<h3 id="7-2-Bagging-and-Pasting"><a href="#7-2-Bagging-and-Pasting" class="headerlink" title="7.2 Bagging and Pasting"></a>7.2 Bagging and Pasting</h3><p>bagging 和 pasting 集成方法的中文名称均为装袋法，也有人称之为自助聚合法。由于这两种方法都是通过对训练数据进行随机抽样，生成多个子集来训练多个模型，再通过投票或平均的方式来预测结果，因此也称为集成学习（Ensemble Learning）的一种。</p>
<p>One way to get a diverse set of classifiers is to use very different training algorithms, as just discussed. Another approach is to use the same training algorithm for every predictor but train them on different random subsets of the training set. When sampling is performed with replacement,  this method is called <em>bagging</em> (short for <em>bootstrap aggregating</em>). When sampling is performed without replacement, it is called <em>pasting</em>.</p>
<p><img src="https://s2.loli.net/2023/03/01/CLqiMvlHWbBJDwn.png" alt="image-20230301095001709"></p>
<p>As you can see in Figure 7-4, predictors can all be trained in parallel, via different CPU cores or even different servers. Similarly, predictions can be made in parallel. This is one of the reasons bagging and pasting are such popular methods: they scale very well.</p>
<h4 id="Bagging-and-Pasting-in-Scikit-Learn"><a href="#Bagging-and-Pasting-in-Scikit-Learn" class="headerlink" title="Bagging and Pasting in Scikit-Learn"></a>Bagging and Pasting in Scikit-Learn</h4><p>Scikit-Learn offers a simple API for both bagging and pasting: <code>BaggingClassifier</code> class (or <code>BaggingRegressor</code> for regression). The following code trains an ensemble of 500 decision tree classifiers:6  each is trained on 100 training instances randomly sampled from the training set with replacement (this is an example of bagging, but if you want to use pasting instead, just set <code>bootstrap=False</code>, defualt True). The <code>n_jobs</code> parameter tells Scikit-Learn the number of CPU cores to use for training and predictions, and –1 tells Scikit-Learn to use all available cores:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> BaggingClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    DecisionTreeClassifier(),</span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    max_samples=<span class="number">100</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span>,</span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Description: A <code>BaggingClassifier</code> automatically performs soft voting instead of hard voting if the base classifier can estimate class probabilities (i.e., if it has a <code>predict_proba()</code> method), which is the case with decision tree classifiers.</p>
<p><img src="https://s2.loli.net/2023/03/01/EVNfrBCL8Fgim9w.png" alt="image-20230301131047657"></p>
<h4 id="Out-of-Bag-Evaluation"><a href="#Out-of-Bag-Evaluation" class="headerlink" title="Out-of-Bag Evaluation"></a>Out-of-Bag Evaluation</h4><p>With bagging, some training instances may be sampled several times for any given predictor, while others may not be sampled at all. The remaining training instances that are not sampled are called <em>out-of-bag</em> (OOB) instances.  Note that they are not the same for all predictors.</p>
<p>A bagging ensemble can be evaluated using OOB instances, without the need for a separate validation set.</p>
<p>In Scikit-Learn, you can set <code>oob_score=True</code> when creating a <code>BaggingClassifier</code> to request an automatic <code>OOB</code> evaluation after training. The following code demonstrates this. The resulting evaluation score is available in the <code>oob_score_</code> attribute:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line">    DecisionTreeClassifier(),</span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    oob_score=<span class="literal">True</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span>,</span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">bag_clf.fit(X_train, y_train)</span><br><span class="line">bag_clf.oob_score_</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.896</p>
<p>According to this OOB evaluation, this <code>BaggingClassifier</code> is likely to achieve about 89.6% accuracy on the test set. Let’s verify this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line"></span><br><span class="line">y_pred = bag_clf.predict(X_test)</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.92</p>
<p>The OOB decision function for each training instance is also available through the <code>oob_decision_function_</code> attribute. Since the base estimator has a <code>predict_proba()</code> method, the decision function returns the class probabilities for each training instance. For example, the OOB evaluation estimates that the first training instance has a 67.6% probability of belonging to the positive class and a 32.4% probability of belonging to the negative class:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bag_clf.oob_decision_function_[:<span class="number">3</span>]  <span class="comment"># probas for the first 3 instances</span></span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([[0.32352941, 0.67647059],</p>
<p>&gt;&gt;&gt; [0.3375 , 0.6625 ],</p>
<p>&gt;&gt;&gt; [1. , 0. ]])</p>
<h4 id="Random-Patches-and-Random-Subspaces"><a href="#Random-Patches-and-Random-Subspaces" class="headerlink" title="Random Patches and Random Subspaces"></a>Random Patches and Random Subspaces</h4><p>The <code>BaggingClassifier</code> class supports sampling the features as well. Sampling is controlled by two hyperparameters: <code>max_features</code> and <code>bootstrap_features</code>. They work the same way as <code>max_samples</code> and <code>bootstrap</code>, but for feature sampling instead of instance sampling. Thus, each predictor will be trained on a random subset of the input features. </p>
<p>This technique is particularly useful when you are dealing with high-dimensional inputs (such as images), as it can considerably speed up training. Sampling both training instances and features is called the <em>random patches</em> method. Keeping all training instances (by setting <code>bootstrap=False</code> and <code>max_samples=1.0</code>) but sampling features (by setting <code>bootstrap_features</code> to <code>True</code> and&#x2F;or <code>max_features</code> to a value smaller than 1.0) is called the <em>random subspaces</em> method.</p>
<p>Sampling features results in even more predictor diversity, trading a bit more bias for a lower variance.</p>
<h3 id="7-3-Random-Forest"><a href="#7-3-Random-Forest" class="headerlink" title="7.3 Random Forest"></a>7.3 Random Forest</h3><p>Instead of building a BaggingClassifier and passing it a DecisionTreeClassifier, you can use the RandomForestClassifier class, which is more convenient and optimized for decision trees (similarly, there is a RandomForestRegressor class for regression tasks).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">rnd_clf = RandomForestClassifier(</span><br><span class="line">    n_estimators=<span class="number">500</span>,</span><br><span class="line">    max_leaf_nodes=<span class="number">16</span>,</span><br><span class="line">    n_jobs=-<span class="number">1</span>,</span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">rnd_clf.fit(X_train, y_train)</span><br><span class="line"></span><br><span class="line">y_pred_rf = rnd_clf.predict(X_test)</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>The random forest algorithm introduces extra randomness when growing trees; instead of searching for the very best feature when splitting a node (see Chapter 6), it searches for the best feature among a random subset of features. By default, it samples $\sqrt n$  features. The algorithm results in greater tree diversity, which (again) trades a higher bias for a lower variance, generally yielding an overall better model. So, the following <code>BaggingClassifier</code> is equivalent to the previous <code>RandomForestClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">bag_clf = BaggingClassifier(</span><br><span class="line"> 	DecisionTreeClassifier(max_features=<span class="string">&quot;sqrt&quot;</span>, max_leaf_nodes=<span class="number">16</span>),</span><br><span class="line"> 	n_estimators=<span class="number">500</span>, n_jobs=-<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br></pre></td></tr></table></figure>

<h4 id="Extra-Trees"><a href="#Extra-Trees" class="headerlink" title="Extra-Trees"></a>Extra-Trees</h4><p>It is possible to make trees even more random by also using random thresholds for each feature rather than searching for the best possible thresholds (like regular decision trees do). For this, simply set <code>splitter=&quot;random&quot;</code> when creating a <code>DecisionTreeClassifier</code>. </p>
<p>A forest of such extremely random trees is called an <em>extremely randomized trees</em> (or <em>extra-trees</em> for short) ensemble. Once again, this technique trades more bias for a lower variance. It also makes extra-trees classifiers much faster to train than regular random forests, because finding the best possible threshold for each feature at every node is one of the most time-consuming tasks of growing a tree. (ExtraTrees 不仅随机选择特征，还随机选择分割点)</p>
<p>You can create an extra-trees classifier using Scikit-Learn’s <code>ExtraTreesClassifier</code> class. Its API is identical to the <code>RandomForestClassifier</code> class, except bootstrap defaults to False. Similarly, the <code>ExtraTreesRegressor</code> class has the same API as the <code>RandomForestRegressor</code> class, except <code>bootstrap</code> defaults to False.</p>
<h4 id="Feature-Importance"><a href="#Feature-Importance" class="headerlink" title="Feature Importance"></a>Feature Importance</h4><p>Yet another great quality of random forests is that they make it easy to measure the relative importance of each feature by attribute <code>feature_importances_</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"></span><br><span class="line">iris = load_iris(as_frame=<span class="literal">True</span>)</span><br><span class="line">rnd_clf = RandomForestClassifier(n_estimators=<span class="number">500</span>, random_state=<span class="number">42</span>)</span><br><span class="line">rnd_clf.fit(iris.data, iris.target)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> score, name <span class="keyword">in</span> <span class="built_in">zip</span>(rnd_clf.feature_importances_, iris.data.columns):</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">round</span>(score, <span class="number">2</span>), name)</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 0.11 sepal length (cm)<br>&gt;&gt;&gt; 0.02 sepal width (cm)<br>&gt;&gt;&gt; 0.44 petal length (cm)<br>&gt;&gt;&gt; 0.42 petal width (cm)</p>
<p>Similarly, if you train a random forest classifier on the MNIST dataset (introduced in Chapter 3) and plot each pixel’s importance, you get the image represented in Figure 7-6.</p>
<p><img src="https://s2.loli.net/2023/03/02/6DtnHVq81vzfY2r.png" alt="image-20230302160857091"></p>
<p>Random forests are very handy to get a quick understanding of what features actually matter, in particular if you need to perform feature selection.</p>
<h3 id="7-4-Boosting"><a href="#7-4-Boosting" class="headerlink" title="7.4 Boosting"></a>7.4 Boosting</h3><p>Boosting (originally called hypothesis boosting) refers to any ensemble method that can combine several weak learners into a strong learner. The general idea of most boosting methods is to train predictors sequentially, each trying to correct its predecessor. There are many boosting methods available, but by far the most popular are AdaBoost13 (short for adaptive boosting) and gradient boosting.</p>
<h4 id="AdaBoost"><a href="#AdaBoost" class="headerlink" title="AdaBoost"></a>AdaBoost</h4><p>One way for a new predictor to correct its predecessor is to pay a bit more attention to the training instances that the predecessor underfit. This results in new predictors focusing more and more on the hard cases. This is the technique used by AdaBoost.</p>
<p>For example, when training an AdaBoost classifier, the algorithm first trains a base classifier (such as a decision tree) and uses it to make predictions on the training set. The algorithm then increases the relative weight of misclassified training instances. Then it trains a second classifier, using the updated weights, and again makes predictions on the training set, updates the instance weights, and so on (see Figure 7-7).</p>
<p><img src="https://s2.loli.net/2023/03/02/7x2fcEAbPoGdWTL.png" alt="image-20230302163629042"></p>
<p>Figure 7-8 shows the decision boundaries of five consecutive predictors on the moons dataset (in this example, each predictor is a highly regularized SVM classifier with an RBF kernel). The first classifier gets many instances wrong, so their weights get boosted. The second classifier therefore does a better job on these instances, and so on. The plot on the right represents the same sequence of predictors, except that the learning rate is halved (i.e., the misclassified instance weights are boosted much less at every iteration). As you can see, this sequential learning technique has some similarities with gradient descent, except that instead of tweaking a single predictor’s parameters to minimize a cost function, AdaBoost adds predictors to the ensemble, gradually making it better.</p>
<p><img src="https://s2.loli.net/2023/03/02/85vq13mLjsZRNyo.png" alt="image-20230302165510517"></p>
<p>Once all predictors are trained, the ensemble makes predictions very much like bagging or pasting, except that predictors have different weights depending on their overall accuracy on the weighted training set.</p>
<p>Warning: There is one important drawback to this sequential learning technique: training cannot be parallelized since each predictor can only be trained after the previous predictor has been trained and evaluated. As a result, it does not scale as well as bagging or pasting.</p>
<p>The more accurate the predictor is, the higher its weight will be.</p>
<p>The following code trains an AdaBoost classifier based on 30 decision stumps using Scikit-Learn’s <code>AdaBoostClassifier</code> class (as you might expect, there is also an <code>AdaBoostRegressor</code> class). A decision stump is a decision tree with <code>max_depth=1</code>—in other words, a tree composed of a single decision node plus two leaf nodes. This is the default base estimator for the AdaBoostClassifier class:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> AdaBoostClassifier</span><br><span class="line"></span><br><span class="line">ada_clf = AdaBoostClassifier(</span><br><span class="line">    DecisionTreeClassifier(max_depth=<span class="number">1</span>),</span><br><span class="line">    n_estimators=<span class="number">30</span>,</span><br><span class="line">    learning_rate=<span class="number">0.5</span>,</span><br><span class="line">    random_state=<span class="number">42</span></span><br><span class="line">)</span><br><span class="line">ada_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>Suggestion: If your AdaBoost ensemble is overfitting the training set, you can try reducing the number of estimators or more strongly regularizing the base estimator.</p>
<h4 id="Gradient-Boosting"><a href="#Gradient-Boosting" class="headerlink" title="Gradient Boosting"></a>Gradient Boosting</h4><p>Another very popular boosting algorithm is <em>gradient boosting</em>. Just like AdaBoost, gradient boosting works by sequentially adding predictors to an ensemble, each one correcting its predecessor. However, instead of tweaking the instance weights at every iteration like <code>AdaBoost</code> does, this method tries to fit the new predictor to the <em>residual errors</em> made by the previous predictor.</p>
<p>Let’s go through a simple regression example, using decision trees as the base predictors; this is called <em>gradient tree boosting</em>, or <em>gradient boosted regression trees</em> (GBRT). First, let’s generate a noisy quadratic dataset and fit a <code>DecisionTreeRegressor</code> to it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeRegressor</span><br><span class="line"></span><br><span class="line">np.random.seed(<span class="number">42</span>)</span><br><span class="line">X = np.random.rand(<span class="number">100</span>, <span class="number">1</span>) - <span class="number">0.5</span></span><br><span class="line">y = <span class="number">3</span> * X[:, <span class="number">0</span>]**<span class="number">2</span> + <span class="number">0.05</span> * np.random.randn(<span class="number">100</span>) <span class="comment"># y = 3x^2 + Gaussian noise</span></span><br><span class="line"></span><br><span class="line">tree_reg1 = DecisionTreeRegressor(max_depth=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_reg1.fit(X, y)</span><br><span class="line"></span><br><span class="line">y2 = y - tree_reg1.predict(X)</span><br><span class="line">tree_reg2 = DecisionTreeRegressor(max_depth=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_reg2.fit(X, y2)</span><br><span class="line"></span><br><span class="line">y3 = y2 - tree_reg2.predict(X)</span><br><span class="line">tree_reg3 = DecisionTreeRegressor(max_depth=<span class="number">2</span>, random_state=<span class="number">42</span>)</span><br><span class="line">tree_reg3.fit(X, y3)</span><br></pre></td></tr></table></figure>

<p>Now we have an ensemble containing three trees. It can make predictions on a new instance simply by adding up the predictions of all the trees:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X_new = np.array([[-<span class="number">0.4</span>], [<span class="number">0.</span>], [<span class="number">0.5</span>]])</span><br><span class="line"><span class="built_in">sum</span>(tree.predict(X_new) <span class="keyword">for</span> tree <span class="keyword">in</span> (tree_reg1, tree_reg2, tree_reg3))</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; array([0.49484029, 0.04021166, 0.75026781])</p>
<p>上述过程可以简单地使用 <code>GradientBoostingRegressor</code> 完成  (there’s also a <code>GradientBoostingClassifier</code> class for classification). ：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> GradientBoostingRegressor</span><br><span class="line"></span><br><span class="line">gbrt = GradientBoostingRegressor(max_depth=<span class="number">2</span>, n_estimators=<span class="number">3</span>,</span><br><span class="line">                                 learning_rate=<span class="number">1.0</span>, random_state=<span class="number">42</span>)</span><br><span class="line">gbrt.fit(X, y)</span><br></pre></td></tr></table></figure>

<p>The learning_rate hyperparameter scales the contribution of each tree. If you set it to a low value, such as 0.05, you will need more trees in the ensemble to fit the training set, but the predictions will usually generalize better. This is a regularization technique called shrinkage. Figure 7-10 shows two GBRT ensembles trained with different hyperparameters: the one on the left does not have enough trees to fit the training set, while the one on the right has about the right amount. If we added more trees, the GBRT would start to overfit the training set.</p>
<p><img src="https://s2.loli.net/2023/03/03/G49iTwzZpsbxkXn.png" alt="image-20230303183437138"></p>
<p>To find the optimal number of trees, you could perform cross-validation using GridSearchCV or RandomizedSearchCV, as usual, but there’s a simpler way: if you set the <code>n_iter_no_change</code> hyperparameter to an integer value, say 10, then the Gradient BoostingRegressor will automatically stop adding more trees during training if it sees that the last 10 trees didn’t help. This is simply early stopping, but with a little bit of patience: it tolerates having no progress for a few iterations before it stops. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbrt_best = GradientBoostingRegressor(max_depth=<span class="number">2</span>,</span><br><span class="line">                                 n_estimators=<span class="number">500</span>, n_iter_no_change=<span class="number">10</span>,</span><br><span class="line">                                 learning_rate=<span class="number">0.05</span>, random_state=<span class="number">42</span>)</span><br><span class="line">gbrt_best.fit(X, y)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">gbrt_best.n_estimators_</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; 92</p>
<p>When <code>n_iter_no_change</code> is set, the <code>fit()</code> method automatically splits the training set into a smaller training set and a validation set: this allows it to evaluate the model’s performance each time it adds a new tree. The size of the validation set is controlled by the <code>validation_fraction</code> hyperparameter, which is 10% by default. The <code>tol</code> hyperparameter determines the maximum performance improvement that still counts as negligible. It defaults to 0.0001.</p>
<h4 id="Histogram-Based-Gradient-Boosting"><a href="#Histogram-Based-Gradient-Boosting" class="headerlink" title="Histogram-Based Gradient Boosting"></a>Histogram-Based Gradient Boosting</h4><p>Scikit-Learn also provides another GBRT implementation, optimized for large datasets: <em>histogram-based gradient boosting</em> (HGB). It works by binning the input features, replacing them with integers. The number of bins is controlled by the <code>max_bins</code> hyperparameter, which defaults to 255 and cannot be set any higher than this.</p>
<p>As a result, this implementation has a computational complexity of $O(b×m)$ instead of $O(n×m×log(m))$, where <em>b</em> is the number of bins, <em>m</em> is the number of training instances, and <em>n</em> is the number of features. In practice, this means that HGB can train hundreds of times faster than regular GBRT on large datasets. However, binning causes a precision loss, which acts as a regularizer: depending on the dataset, this may help reduce overfitting, or it may cause underfitting.</p>
<p>Scikit-Learn provides two classes for <code>HGB</code>: <code>HistGradientBoostingRegressor</code> and <code>HistGradientBoostingClassifier</code>. They’re similar to <code>GradientBoostingRegressor</code> and <code>GradientBoostingClassifier</code>, with a few notable differences:</p>
<ul>
<li>Early stopping is automatically activated if the number of instances is greater than 10,000. You can turn early stopping always on or always off by setting the <code>early_stopping</code> hyperparameter to True or False.</li>
<li>Subsampling is not supported.</li>
<li><code>n_estimators</code> is renamed to <code>max_iter</code>.</li>
<li>The only decision tree hyperparameters that can be tweaked are <code>max_leaf_nodes</code>, <code>min_samples_leaf</code>, and <code>max_depth</code>.</li>
</ul>
<p>The HGB classes also have two nice features: they support both categorical features and missing values. This simplifies preprocessing quite a bit. However, the categorical features must be represented as integers ranging from 0 to a number lower than max_bins. You can use an OrdinalEncoder for this. For example, here’s how to build and train a complete pipeline for the California housing dataset introduced in Chapter 2:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"><span class="keyword">from</span> sklearn.compose <span class="keyword">import</span> make_column_transformer</span><br><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> HistGradientBoostingRegressor</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OrdinalEncoder</span><br><span class="line"></span><br><span class="line">hgb_reg = make_pipeline(</span><br><span class="line">    make_column_transformer(</span><br><span class="line">        (OrdinalEncoder(), [<span class="string">&quot;ocean_proximity&quot;</span>]),</span><br><span class="line">        remainder=<span class="string">&quot;passthrough&quot;</span></span><br><span class="line">    ),</span><br><span class="line">    HistGradientBoostingRegressor(</span><br><span class="line">        categorical_features=[<span class="number">0</span>],</span><br><span class="line">        random_state=<span class="number">42</span></span><br><span class="line">    )</span><br><span class="line">)</span><br><span class="line">hgb_reg.fit(housing, housing_labels)</span><br></pre></td></tr></table></figure>

<p>Suggestion: Several other optimized implementations of gradient boosting are available in the Python ML ecosystem: in particular, XGBoost, CatBoost, and LightGBM. These libraries have been around for several years. They are all specialized for gradient boosting, their APIs are very similar to Scikit-Learn’s, and they provide many additional features, including GPU acceleration; you should definitely check them out! Moreover, the TensorFlow Random Forests library pro‐ vides optimized implementations of a variety of random forest algorithms, including plain random forests, extra-trees, GBRT, and several more.</p>
<h3 id="7-5-Stacking"><a href="#7-5-Stacking" class="headerlink" title="7.5 Stacking"></a>7.5 Stacking</h3><p><img src="https://s2.loli.net/2023/03/04/ESj7hZaRTFGIblm.png" alt="image-20230304093217512"></p>
<p><img src="https://s2.loli.net/2023/03/04/BdawDXhp1yzM2Ax.png" alt="image-20230304093508724"></p>
<p>It is actually possible to train several different blenders this way (e.g., one using linear regression, another using random forest regression) to get a whole layer of blenders, and then add another blender on top of that to produce the final prediction, as shown in Figure 7-13.</p>
<p><img src="https://s2.loli.net/2023/03/04/wDEurx2LNJ6Al4Y.png" alt="image-20230304093649571"></p>
<p>Scikit-Learn provides two classes for stacking ensembles: StackingClassifier and StackingRegressor. For example, we can replace the VotingClassifier we used at the beginning of this chapter on the moons dataset with a StackingClassifier:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> StackingClassifier</span><br><span class="line"></span><br><span class="line">stacking_clf = StackingClassifier(</span><br><span class="line">    estimators=[</span><br><span class="line">        (<span class="string">&#x27;lr&#x27;</span>, LogisticRegression(random_state=<span class="number">42</span>)),</span><br><span class="line">        (<span class="string">&#x27;rf&#x27;</span>, RandomForestClassifier(random_state=<span class="number">42</span>)),</span><br><span class="line">        (<span class="string">&#x27;svc&#x27;</span>, SVC(probability=<span class="literal">True</span>, random_state=<span class="number">42</span>))</span><br><span class="line">    ],</span><br><span class="line">    final_estimator=RandomForestClassifier(random_state=<span class="number">43</span>),</span><br><span class="line">    cv=<span class="number">5</span></span><br><span class="line">)</span><br><span class="line">stacking_clf.fit(X_train, y_train)</span><br></pre></td></tr></table></figure>

<p>For each predictor, the stacking classifier will call <code>predict_proba()</code> if available; if not it will fall back to <code>decision_function()</code> or, as a last resort, call <code>predict()</code>. If you don’t provide a final estimator, <code>StackingClassifier</code> will use <code>LogisticRegression</code> and <code>StackingRegressor</code> will use <code>RidgeCV</code>.</p>
<h3 id="7-x-Exercises"><a href="#7-x-Exercises" class="headerlink" title="7.x Exercises"></a>7.x Exercises</h3><ol>
<li><p>If you have trained five different models on the exact same training data, and they all achieve 95% precision, is there any chance that you can combine these models to get better results? If so, how? If not, why?</p>
<p>ensemble methods; Voting, bagging ?, </p>
</li>
<li><p>What is the difference between hard and soft voting classifiers?</p>
<p>hard based on the most votes; soft bases on the probabilities</p>
</li>
<li><p>Is it possible to speed up training of a bagging ensemble by distributing it across multiple servers? What about pasting ensembles, boosting ensembles, random forests, or stacking ensembles?</p>
<p>yes, it’s possible. except stacking ensembles.</p>
</li>
<li><p>What is the benefit of out-of-bag evaluation? </p>
<p>do not need to use validation-set</p>
</li>
<li><p>What makes extra-trees ensembles more random than regular random forests? How can this extra randomness help? Are extra-trees classifiers slower or faster than regular random forests?</p>
</li>
<li><p>If your AdaBoost ensemble underfits the training data, which hyperparameters should you tweak, and how?</p>
</li>
<li><p>If your gradient boosting ensemble overfits the training set, should you increase or decrease the learning rate?</p>
</li>
<li><p>Load the MNIST dataset (introduced in Chapter 3), and split it into a training set, a validation set, and a test set (e.g., use 50,000 instances for training, 10,000 for validation, and 10,000 for testing). Then train various classifiers, such as a random forest classifier, an extra-trees classifier, and an SVM classifier. Next, try to combine them into an ensemble that outperforms each individual classifier on the validation set, using soft or hard voting. Once you have found one, try it on the test set. How much better does it perform compared to the individual classifiers?</p>
</li>
<li><p>Run the individual classifiers from the previous exercise to make predictions on the validation set, and create a new training set with the resulting predictions: each training instance is a vector containing the set of predictions from all your classifiers for an image, and the target is the image’s class. Train a classifier on this new training set. Congratulations—you have just trained a blender, and together with the classifiers it forms a stacking ensemble! Now evaluate the ensemble on the test set. For each image in the test set, make predictions with all your classifiers, then feed the predictions to the blender to get the ensemble’s predictions. How does it compare to the voting classifier you trained earlier? Now try again using a StackingClassifier instead. Do you get better performance? If so, why?</p>
</li>
</ol>
<h2 id="8-Dimensionality-Reduction"><a href="#8-Dimensionality-Reduction" class="headerlink" title="8. Dimensionality Reduction"></a>8. Dimensionality Reduction</h2><p>Warning: Reducing dimensionality does cause some information loss, just like compressing an image to JPEG can degrade its quality, so even though it will speed up training, it may make your system perform slightly worse. It also makes your pipelines a bit more complex and thus harder to maintain. Therefore, I recommend you first try to train your system with the original data before considering using dimensionality reduction. In some cases, reducing the dimension‐ ality of the training data may filter out some noise and unnecessary details and thus result in higher performance, but in general it won’t; it will just speed up training.</p>
<p>Apart from speeding up training, dimensionality reduction is also extremely useful for data visualization.</p>
<p>Then we will consider the two main approaches to dimensionality reduction (<em>projection (投影)</em>  and <em>manifold learning (流形学习)</em>), and we will go through three of the most popular dimensionality reduction techniques: <em>PCA (主成分分析)</em>, <em>random projection (随机投影)</em>, and <em>locally linear embedding (LLE, 局部线性嵌入)</em>.</p>
<h3 id="8-1-The-Curse-of-Dimensionality"><a href="#8-1-The-Curse-of-Dimensionality" class="headerlink" title="8.1 The Curse of Dimensionality"></a>8.1 The Curse of Dimensionality</h3><p>Most points in a high-dimensional hypercube are very close to the border. </p>
<p>Here is a more troublesome difference: if you pick two points randomly in a unit square, the distance between these two points will be, on average, roughly 0.52. If you pick two random points in a 3D unit cube, the average distance will be roughly 0.66. But what about two points picked randomly in a 1,000,000-dimensional unit hypercube? The average distance, believe it or not, will be about 408.25. As a result, high-dimensional datasets are at risk of being very sparse: most training instances are likely to be far away from each other. This also means that a new instance will likely be far away from any training instance, making predictions much less reliable than in lower dimensions, since they will be based on much larger extrapolations.  In short, the more dimensions the training set has, the greater the risk of overfitting it.</p>
<h3 id="8-2-Main-Approaches-for-Dimensionality-Reduction"><a href="#8-2-Main-Approaches-for-Dimensionality-Reduction" class="headerlink" title="8.2 Main Approaches for Dimensionality Reduction"></a>8.2 Main Approaches for Dimensionality Reduction</h3><p>Before we dive into specific dimensionality reduction algorithms, let’s take a look at the two main approaches to reducing dimensionality: projection and manifold learning.</p>
<h4 id="Projection"><a href="#Projection" class="headerlink" title="Projection"></a>Projection</h4><p>把 instances 从 3D 空间投影 (project) 到 2D 平面中（该 2D 平面称为原空间的 <em>subspace</em>），instances 之间的距离会大大缩小。</p>
<p><img src="https://s2.loli.net/2023/03/05/IAE9ZjwzruTOKya.png" alt="image-20230305153241526"></p>
<h4 id="Manifold-Learning"><a href="#Manifold-Learning" class="headerlink" title="Manifold Learning"></a>Manifold Learning</h4><p>虽然 projection 能使 instances 距离缩小，但在大多数情况下，投影到 subspace 后的 instances 呈瑞士卷状，再次 project 后 instances&#x2F;datasets 会重叠。Swiss roll toy dataset represented in Figure 8-4. </p>
<p><img src="https://s2.loli.net/2023/03/05/OzPteB4idEN89Gq.png" alt="image-20230305172832858"></p>
<p>当把 Swiss roll dataset 再次 project 到平面上后，数据会如左下图所示：</p>
<p><img src="https://s2.loli.net/2023/03/05/pXGMbtIzZAlnk5C.png" alt="image-20230305173359332"></p>
<p>这种由 Swiss roll 投影成的具有重叠特征的 subspace，称为 manifold (流动的形状)。More generally, a d-dimensional manifold is a part of an n-dimensional space (where d &lt; n) that locally resembles a d-dimensional hyperplane. In the case of the Swiss roll, d &#x3D; 2 and n &#x3D; 3: it locally resembles a 2D plane, but it is rolled in the third dimension.</p>
<p>In short, reducing the dimensionality of your training set before training a model will usually speed up training, <strong>but</strong> it may not always lead to a better or simpler solution; it all depends on the dataset. 见 Figure 8-6，下面原始的 datasets 的决策边界比其 manifold 更加直观。</p>
<p><img src="https://s2.loli.net/2023/03/05/gsEt8hdbLDInxe9.png" alt="image-20230305175035091"></p>
<h3 id="8-3-PCA"><a href="#8-3-PCA" class="headerlink" title="8.3 PCA"></a>8.3 PCA</h3><p><em>Principal component analysis</em> (PCA) is by far the most popular dimensionality reduction algorithm. First it identifies the hyperplane that lies closest to the data, and then it projects the data onto it, just like in Figure 8-2.</p>
<h4 id="Preserving-the-Variance"><a href="#Preserving-the-Variance" class="headerlink" title="Preserving the Variance"></a>Preserving the Variance</h4><p>Before you can project the training set onto a lower-dimensional hyperplane, you first need to choose the right hyperplane. For example, a simple 2D dataset is represented on the left in Figure 8-7, along with three different axes (i.e., 1D hyperplanes). On the right is the result of the projection of the dataset onto each of these axes. As you can see, the projection onto the solid line preserves the maximum variance (top), while the projection onto the dotted line preserves very little variance (bottom) and the projection onto the dashed line preserves an intermediate amount of variance (middle).</p>
<p><img src="https://s2.loli.net/2023/03/05/TF1cyedOuIhExPV.png" alt="image-20230305181446313"></p>
<p>It seems reasonable to select the axis that preserves the maximum amount of var‐ iance, as it will most likely lose less information than the other projections. Another way to justify this choice is that it is the axis that minimizes the mean squared distance between the original dataset and its projection onto that axis. This is the rather simple idea behind PCA.</p>
<h4 id="Principal-Components"><a href="#Principal-Components" class="headerlink" title="Principal Components"></a>Principal Components</h4><p>PCA identifies the axis that accounts for the largest amount of variance in the training set. In Figure 8-7, it is the solid line. It also finds a second axis, orthogonal to the first one, that accounts for the largest amount of the remaining variance. In this 2D example there is no choice: it is the dotted line. If it were a higher-dimensional dataset, PCA would also find a third axis, <strong>orthogonal</strong> to both previous axes, and a fourth, a fifth, and so on—<strong>as many axes as the number of dimensions in the dataset.</strong></p>
<p>The <em>i</em>^th^ axis is called the <em>i</em>^th^ <em>principal component</em> (PC) of the data. In Figure 8-7, the first PC is the axis on which vector <strong>c<del>1</del></strong> lies, and the second PC is the axis on which vector <strong>c<del>2</del></strong> lies. In Figure 8-2 the first two PCs are on the projection plane, and the third PC is the axis orthogonal to that plane. After the projection, in Figure 8-3, the first PC corresponds to the z<del>1</del> axis, and the second PC corresponds to the z<del>2</del> axis.</p>
<p>So how can you find the principal components of a training set? Luckily, there is a standard matrix factorization technique called singular value decomposition (SVD) that can decompose the training set matrix <strong>X</strong> into the matrix multiplication of three matrices <strong>U Σ V^⊺^</strong> , where <strong>V</strong> contains the unit vectors that define all the principal components that you are looking for, as shown in Equation 8-1.</p>
<p><img src="https://s2.loli.net/2023/03/05/lMeaF3d5Hk8N7og.png" alt="image-20230305205626028"></p>
<p>The following Python code uses NumPy’s svd() function to obtain all the principal components of the 3D training set represented in Figure 8-2, then it extracts the two unit vectors that define the first two PCs (步骤 3):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">X = [...] <span class="comment"># create a small 3D dataset</span></span><br><span class="line">X_centered = X - X.mean(axis=<span class="number">0</span>) </span><br><span class="line">U, s, Vt = np.linalg.svd(X_centered)</span><br><span class="line">c1 = Vt[<span class="number">0</span>]</span><br><span class="line">c2 = Vt[<span class="number">1</span>]</span><br></pre></td></tr></table></figure>

<p>The following Python code projects the training set onto the plane defined by the first two principal components (步骤 4; with Figure 8-2):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">W2 = Vt[:<span class="number">2</span>].T</span><br><span class="line">X2D = X_centered @ W2</span><br></pre></td></tr></table></figure>

<p>具体地，PCA 的实现分为以下几个步骤：</p>
<ol>
<li>对每个特征进行标准化，使其均值为 0，标准差为 1。</li>
<li>计算数据的协方差矩阵 $C &#x3D; \frac{1}{m} X^T X$。</li>
<li>对协方差矩阵 $C$ 进行特征值分解，得到特征值和对应的特征向量。</li>
<li>将特征向量按照对应的特征值大小排序，选取前 $k$ 个特征向量作为投影矩阵 $W$。</li>
<li>将数据矩阵 $X$ 乘以投影矩阵 $W$，得到投影后的数据矩阵 $Y$。</li>
</ol>
<p>在实际应用中，我们通常使用 SVD (Singular Value Decomposition) 方法来计算 PCA，这种方法更加高效和稳定。</p>
<h4 id="Using-Scikit-Learn"><a href="#Using-Scikit-Learn" class="headerlink" title="Using Scikit-Learn"></a>Using Scikit-Learn</h4><p>Scikit-Learn’s PCA class uses SVD to implement PCA, just like we did earlier in this chapter. The following code applies PCA to reduce the dimensionality of the dataset down to two dimensions (note that it automatically takes care of centering the data): </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA </span><br><span class="line"></span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>) </span><br><span class="line">X2D = pca.fit_transform(X)</span><br></pre></td></tr></table></figure>

<p>After fitting the PCA transformer to the dataset, its <code>components_</code> attribute holds the transpose of <strong>W<del>d</del></strong> : it contains one row for each of the first <em>d</em> principal components.</p>
<h4 id="Explained-Variance-Ratio"><a href="#Explained-Variance-Ratio" class="headerlink" title="Explained Variance Ratio"></a>Explained Variance Ratio</h4><p>Another useful piece of information is the explained variance ratio of each principal component, available via the explained_variance_ratio_ variable. The ratio indicates the proportion of the dataset’s variance that lies along each principal component. For example, let’s look at the explained variance ratios of the first two components of the 3D dataset represented in Figure 8-2:</p>
<p><strong>&gt;&gt;&gt;</strong> <code>pca.explained_variance_ratio_</code></p>
<p>array([0.7578477 , 0.15186921])</p>
<p>This output tells us that about 76% of the dataset’s variance lies along the first PC, and about 15% lies along the second PC. This leaves about 9% for the third PC, so it is reasonable to assume that the third PC probably carries little information. </p>
<h4 id="Choosing-the-Right-Number-Of-Dimensions"><a href="#Choosing-the-Right-Number-Of-Dimensions" class="headerlink" title="Choosing the Right Number Of Dimensions"></a>Choosing the Right Number Of Dimensions</h4><p>Instead of arbitrarily choosing the number of dimensions to reduce down to, it is simpler to choose the number of dimensions that add up to a sufficiently large portion of the variance—say, 95%.</p>
<p>The following code loads and splits the MNIST dataset (introduced in Chapter 3) and performs PCA without reducing dimensionality, then computes the minimum number of dimensions required to preserve 95% of the training set’s variance:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> fetch_openml</span><br><span class="line"></span><br><span class="line">mnist = fetch_openml(<span class="string">&#x27;mnist_784&#x27;</span>, as_frame=<span class="literal">False</span>)</span><br><span class="line">X_train, y_train = mnist.data[:<span class="number">60000</span>], mnist.target[:<span class="number">60000</span>]</span><br><span class="line">X_test, y_test = mnist.data[<span class="number">60000</span>:], mnist.target[<span class="number">60000</span>:]</span><br><span class="line"></span><br><span class="line">pca = PCA()</span><br><span class="line">pca.fit(X_train)</span><br><span class="line">cumsum = np.cumsum(pca.explained_variance_ratio_)</span><br><span class="line">d = np.argmax(cumsum &gt;= <span class="number">0.95</span>) + <span class="number">1</span>  <span class="comment"># d equals 154</span></span><br></pre></td></tr></table></figure>

<p>You could then set <code>n_components=d</code> and run PCA again, but there’s a better option. Instead of specifying the number of principal components you want to preserve, you can set <code>n_components</code> to be a float between 0.0 and 1.0, indicating the ratio of variance you wish to preserve:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">pca = PCA(n_components=<span class="number">0.95</span>)</span><br><span class="line">X_reduced = pca.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<p>The actual number of components is determined during training, and it is stored in the <code>n_components_</code> attribute:</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <code>pca.n_components_</code></p>
<p>​		154</p>
<p><img src="https://s2.loli.net/2023/03/08/LXKkpqN2lhEotAb.png" alt="image-20230308142905418"></p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> RandomizedSearchCV</span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line">clf = make_pipeline(PCA(random_state=<span class="number">42</span>),</span><br><span class="line">                    RandomForestClassifier(random_state=<span class="number">42</span>))</span><br><span class="line">param_distrib = &#123;</span><br><span class="line">    <span class="string">&quot;pca__n_components&quot;</span>: np.arange(<span class="number">10</span>, <span class="number">80</span>),</span><br><span class="line">    <span class="string">&quot;randomforestclassifier__n_estimators&quot;</span>: np.arange(<span class="number">50</span>, <span class="number">500</span>)</span><br><span class="line">&#125;</span><br><span class="line">rnd_search = RandomizedSearchCV(clf, param_distrib, n_iter=<span class="number">10</span>, cv=<span class="number">3</span>,</span><br><span class="line">                                random_state=<span class="number">42</span>)</span><br><span class="line">rnd_search.fit(X_train[:<span class="number">1000</span>], y_train[:<span class="number">1000</span>])</span><br></pre></td></tr></table></figure>

<p>Let’s look at the best hyperparameters found:</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <code>print(rnd_search.best_params_)</code></p>
<p>​		{‘randomforestclassifier__n_estimators’: 465, ‘pca__n_components’: 23}</p>
<p>It’s interesting to note how low the optimal number of components is: we reduced a 784-dimensional dataset to just 23 dimensions! This is tied to the fact that we used a random forest, which is a pretty powerful model. If we used a linear model instead, such as an SGDClassifier, the search would find that we need to preserve more dimensions (about 70).</p>
<h4 id="PCA-for-Compression"><a href="#PCA-for-Compression" class="headerlink" title="PCA for Compression"></a>PCA for Compression</h4><p>After dimensionality reduction, the training set takes up much less space. For example, after applying PCA to the MNIST dataset while preserving 95% of its variance, we are left with 154 features, instead of the original 784 features.</p>
<p>The <code>inverse_transform()</code> method lets us decompress the reduced MNIST dataset back to 784 dimensions within 5% reconstruction error:</p>
<p>​		<code>X_recovered = pca.inverse_transform(X_reduced)</code></p>
<p><img src="https://s2.loli.net/2023/03/08/Lw4ipcyxnATPUIk.png" alt="image-20230308150810935"></p>
<h4 id="Randomized-PCA"><a href="#Randomized-PCA" class="headerlink" title="Randomized PCA"></a>Randomized PCA</h4><p>If you set the <code>svd_solver</code> hyperparameter to “<code>randomized</code>“, Scikit-Learn uses a stochastic algorithm called <em>randomized</em> PCA that quickly finds an approximation of the first d principal components. Its computational complexity is O(m × d^2^) + O(d^3^), instead of O(m × n^2^) + O(n^3^) for the full SVD approach, so it is dramatically faster than full SVD when d is much smaller than n:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">rnd_pca = PCA(n_components=<span class="number">154</span>, svd_solver=<span class="string">&quot;randomized&quot;</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_reduced = rnd_pca.fit_transform(X_train)</span><br></pre></td></tr></table></figure>

<p>Suggestion: By default, <code>svd_solver</code> is actually set to <code>&quot;auto&quot;</code>: Scikit-Learn auto‐ matically uses the randomized PCA algorithm if max(m, n) &gt; 500 and <code>n_components</code> is an integer smaller than 80% of min(m, n), or else it uses the full SVD approach. So the preceding code would use the randomized PCA algorithm even if you removed the <code>svd_solver=&quot;randomized&quot;</code> argument, since 154 &lt; 0.8 × 784. If you want to force Scikit-Learn to use full SVD for a slightly more precise result, you can set the <code>svd_solver</code> hyperparameter to <code>&quot;full&quot;</code>.</p>
<h4 id="Incremental-PCA"><a href="#Incremental-PCA" class="headerlink" title="Incremental PCA"></a>Incremental PCA</h4><p>One problem with the preceding implementations of PCA is that they require the whole training set to fit in memory in order for the algorithm to run. Fortunately, <em>incremental</em> PCA (IPCA) algorithms have been developed that allow you to split the training set into mini-batches and feed these in one mini-batch at a time. This is useful for large training sets and for applying PCA online (i.e., on the fly, as new instances arrive).</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> IncrementalPCA</span><br><span class="line"></span><br><span class="line">n_batches = <span class="number">100</span></span><br><span class="line">inc_pca = IncrementalPCA(n_components=<span class="number">154</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> X_batch <span class="keyword">in</span> np.array_split(X_train, n_batches):</span><br><span class="line">    inc_pca.partial_fit(X_batch)</span><br><span class="line"></span><br><span class="line">X_reduced = inc_pca.transform(X_train)</span><br></pre></td></tr></table></figure>

<p>Alternatively, you can use NumPy’s memmap class, which allows you to manipulate a large array stored in a binary file on disk as if it were entirely in memory; the class loads only the data it needs in memory, when it needs it.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">filename = <span class="string">&quot;my_mnist.mmap&quot;</span></span><br><span class="line">X_mmap = np.memmap(filename, dtype=<span class="string">&#x27;float32&#x27;</span>, mode=<span class="string">&#x27;write&#x27;</span>, shape=X_train.shape)</span><br><span class="line">X_mmap[:] = X_train <span class="comment"># could be a loop instead, saving the data chunk by chunk</span></span><br><span class="line">X_mmap.flush()</span><br><span class="line"></span><br><span class="line">X_mmap = np.memmap(filename, dtype=<span class="string">&quot;float32&quot;</span>, mode=<span class="string">&quot;readonly&quot;</span>).reshape(-<span class="number">1</span>, <span class="number">784</span>)</span><br><span class="line">batch_size = X_mmap.shape[<span class="number">0</span>] // n_batches</span><br><span class="line">inc_pca = IncrementalPCA(n_components=<span class="number">154</span>, batch_size=batch_size)</span><br><span class="line">inc_pca.fit(X_mmap)</span><br></pre></td></tr></table></figure>



<p>For very high-dimensional datasets, PCA can be too slow. As you saw earlier, even if you use randomized PCA its computational complexity is still O(m × d^2^) + O(d^3^), so the target number of dimensions <em>d</em> must not be too large. If you are dealing with a dataset with tens of thousands of features or more (e.g., images), then training may become much too slow: in this case, you should consider using random projection instead.</p>
<h3 id="8-4-Random-Projection"><a href="#8-4-Random-Projection" class="headerlink" title="8.4 Random Projection"></a>8.4 Random Projection</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.random_projection <span class="keyword">import</span> GaussianRandomProjection</span><br><span class="line"></span><br><span class="line">gaussian_rnd_proj = GaussianRandomProjection(eps=<span class="number">0.1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_reduced = gaussian_rnd_proj.fit_transform(X)</span><br></pre></td></tr></table></figure>

<p>Scikit-Learn also provides a second random projection transformer, known as <code>SparseRandomProjection</code>. It’s usually preferable to use this transformer instead of the first one, especially for large or sparse datasets.</p>
<p>In summary, random projection is a simple, fast, memory-efficient, and surprisingly powerful dimensionality reduction algorithm that you should keep in mind, especially when you deal with high-dimensional datasets.</p>
<h3 id="8-5-LLE"><a href="#8-5-LLE" class="headerlink" title="8.5 LLE"></a>8.5 LLE</h3><p><em>Locally linear embedding</em> (LLE) is a nonlinear dimensionality reduction (NLDR) tech‐ nique. It is a manifold learning technique that does not rely on projections, unlike PCA and random projection. In a nutshell, LLE works by first measuring how each training instance linearly relates to its nearest neighbors, and then looking for a low-dimensional representation of the training set where these local relationships are best preserved (more details shortly). This approach makes it particularly good at unrolling twisted manifolds, especially when there is not too much noise.</p>
<p>The following code makes a Swiss roll, then uses Scikit-Learn’s <code>LocallyLinearEmbedding</code> class to unroll it:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_swiss_roll</span><br><span class="line"><span class="keyword">from</span> sklearn.manifold <span class="keyword">import</span> LocallyLinearEmbedding</span><br><span class="line"></span><br><span class="line">X_swiss, t = make_swiss_roll(n_samples=<span class="number">1000</span>, noise=<span class="number">0.2</span>, random_state=<span class="number">41</span>)</span><br><span class="line">lle = LocallyLinearEmbedding(n_components=<span class="number">2</span>, n_neighbors=<span class="number">10</span>, random_state=<span class="number">42</span>)</span><br><span class="line">X_unrolled = lle.fit_transform(X_swiss)</span><br></pre></td></tr></table></figure>

<p>Scikit-Learn’s LLE implementation has the following computational complexity: O(m log(m)n log(k)) for finding the k-nearest neighbors, O(mnk^3^) for optimizing the weights, and O(dm^2^) for constructing the low-dimensional representations. Unfortunately, the m^2^ in the last term makes this algorithm scale poorly to very large datasets. </p>
<p>As you can see, LLE is quite different from the projection techniques, and it’s sig‐ nificantly more complex, but it can also construct much better low-dimensional representations, especially if the data is nonlinear.</p>
<p>Locally Linear Embedding (LLE) 是一种非线性降维方法，可以将高维数据映射到低维空间中，并保留原始数据的局部关系。LLE 基于以下假设：在高维空间中，数据点的局部邻域可以近似为一个线性组合，因此，将每个数据点表示为其邻域中其他数据点的线性组合，可以保留原始数据点之间的局部关系，而不需要显式地计算数据点之间的距离。</p>
<p>LLE 的运行原理和步骤如下：</p>
<ol>
<li>计算邻域：对于每个数据点，找到其 k 个最近邻的数据点。</li>
<li>重构权重矩阵：对于每个数据点，将其表示为其 k 个最近邻数据点的线性组合，即 $W_{ij} &#x3D; \sum_{k \in N_i}^{} {w_{ik} x_j}$，其中 $x_j$ 是第 j 个数据点，$N_i$ 是第 i 个数据点的 k 个最近邻数据点的集合，$w_{ik}$ 是权重，用来表示第 i 个数据点在重构第 j 个数据点时所占的比重。重构权重矩阵 $W$ 是一个稀疏矩阵。</li>
<li>计算降维表示：构建一个低维嵌入矩阵 $Y$，其中每行是一个数据点的低维表示。通过最小化重构误差来计算 $Y$，即 $J(Y) &#x3D; \sum_{i&#x3D;1}^{n} {||y_i - \sum_{j&#x3D;1}^{n} {W_{ij} y_j}||^2}$，其中 $y_i$ 是第 i 个数据点的低维表示。</li>
<li>返回结果：返回低维嵌入矩阵 $Y$。</li>
</ol>
<p>LLE 的关键在于重构权重矩阵的计算，它可以保留原始数据点之间的局部关系，并使得降维后的数据保留了原始数据的局部结构。通过最小化重构误差来计算低维嵌入矩阵 $Y$，可以将原始数据映射到低维空间中，并且保留了原始数据的局部结构。</p>
<h3 id="8-6-Other-Dimensionality-Reduction-Techniques"><a href="#8-6-Other-Dimensionality-Reduction-Techniques" class="headerlink" title="8.6 Other Dimensionality Reduction Techniques"></a>8.6 Other Dimensionality Reduction Techniques</h3><p><code>sklearn.manifold.MDS</code>: <em>Multidimensional scaling</em> (MDS)</p>
<p><code>sklearn.manifold.Isomap</code>: <em>Isomap</em></p>
<p><code>sklearn.manifold.TSNE</code>: <em>t-distributed stochastic neighbor embedding</em> (t-SNE) </p>
<p><code>sklearn.discriminant_analysis.LinearDiscriminantAnalysis</code>: <em>Linear discriminant analysis</em> (LDA) </p>
<p><img src="https://s2.loli.net/2023/03/10/LByEQaU4l8oqID1.png" alt="image-20230310151602929"></p>
<h3 id="8-x-Exercises"><a href="#8-x-Exercises" class="headerlink" title="8.x Exercises"></a>8.x Exercises</h3><ol>
<li><p>What are the main motivations for reducing a dataset’s dimensionality? What are the main drawbacks?</p>
<p>speed; a little loss</p>
</li>
<li><p>What is the curse of dimensionality?</p>
<p>you</p>
</li>
</ol>
<h2 id="9-Unsupervised-Learning-Technique"><a href="#9-Unsupervised-Learning-Technique" class="headerlink" title="9. Unsupervised Learning Technique"></a>9. Unsupervised Learning Technique</h2><p>In Chapter 8 we looked at the most common unsupervised learning task: dimensionality reduction. In this chapter we will look at a few more unsupervised tasks:</p>
<ul>
<li><p><em>Clustering</em></p>
</li>
<li><p><em>Anomaly detection (also called outlier detection)</em></p>
</li>
<li><p><em>Density estimation</em></p>
<p>This is the task of estimating the probability density function (PDF) of the random process that generated the dataset. Density estimation is commonly used for anomaly detection: instances located in very low-density regions are likely to be anomalies. It is also useful for data analysis and visualization.</p>
</li>
</ul>
<p>We will start with two clustering algorithms, k-means and DBSCAN, then we’ll discuss Gaussian mixture models and see how they can be used for density estimation, clustering, and anomaly detection.</p>
<h3 id="9-1-Clustering-Algorithms-k-means-and-DBSCAN"><a href="#9-1-Clustering-Algorithms-k-means-and-DBSCAN" class="headerlink" title="9.1 Clustering Algorithms: k-means and DBSCAN"></a>9.1 Clustering Algorithms: k-means and DBSCAN</h3><h4 id="k-means"><a href="#k-means" class="headerlink" title="k-means"></a>k-means</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"></span><br><span class="line">X, y = make_blobs([...])  <span class="comment"># make the blobs: y contains the cluster IDs, but we will not use them; that&#x27;s what we want to predict</span></span><br><span class="line">k = <span class="number">5</span></span><br><span class="line">kmeans = KMeans(n_clusters=k, random_state=<span class="number">42</span>)</span><br><span class="line">y_pred = kmeans.fit_predict(X)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/03/10/LlASjsH6fU1ZJGv.png" alt="image-20230310171936157"></p>
<p>Note that you have to specify the number of clusters <em>k</em> that the algorithm must find. In this example, it is pretty obvious from looking at the data that <em>k</em> should be set to 5, but in general it is not that easy. We will discuss this shortly.</p>
<p>KMeans instance preserves the predicted labels of the instances it was trained on, available via the <code>labels_</code> instance variable:</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <code>y_pred</code></p>
<p>​		array([4, 0, 1, …, 2, 1, 0], dtype&#x3D;int32)</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <code>y_pred **is** kmeans.labels_</code></p>
<p>​		True</p>
<p>We can also take a look at the five centroids that the algorithm found:</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <code>kmeans.cluster_centers_</code></p>
<p>​		array([[-2.80389616, 1.80117999],<br>​				  [ 0.20876306, 2.25551336],<br>​				  [-2.79290307, 2.79641063],<br>​		  		[-1.46679593, 2.28585348],<br>​		  		[-2.80037642, 1.30082566]])</p>
<p>You can easily assign new instances to the cluster whose centroid is closest:</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <strong>import</strong> <strong>numpy</strong> <strong>as</strong> <strong>np</strong></p>
<p>​		<strong>&gt;&gt;&gt;</strong> X_new &#x3D; np.array([[0, 2], [3, 2], [-3, 3], [-3, 2.5]])</p>
<p>​		<strong>&gt;&gt;&gt;</strong> kmeans.predict(X_new)</p>
<p>​		array([1, 1, 2, 2], dtype&#x3D;int32)</p>
<p>If you plot the cluster’s decision boundaries, you get a Voronoi tessellation: see Figure 9-3, where each centroid is represented with an X.</p>
<p><img src="https://s2.loli.net/2023/03/10/PH7fbKrwgLJko5U.png" alt="image-20230310181104698"></p>
<p>Instead of assigning each instance to a single cluster, which is called <em>hard clustering</em>, it can be useful to give each instance a score per cluster, which is called <em>soft clustering</em>. The score can be the distance between the instance and the centroid or a similarity score (or affinity), such as the Gaussian radial basis function we used in Chapter 2. In the KMeans class, the transform() method measures the distance from each instance to every centroid:</p>
<p><img src="https://s2.loli.net/2023/03/11/OkIDhPZjB4YtfoK.png" alt="image-20230311011104970"></p>
<p>If you have a highdimensional dataset and you transform it this way, you end up with a k-dimensional dataset: this transformation can be a very efficient nonlinear dimensionality reduc‐ tion technique. Alternatively, you can use these distances as extra features to train another model, as in Chapter 2.</p>
<p><img src="https://s2.loli.net/2023/03/11/3jGbxgNHfopk2ZO.png" alt="image-20230311021513832"></p>
<p>Let’s take a look at a few ways you can mitigate this risk by improving the centroid initialization.</p>
<p>If you happen to know approximately where the centroids should be (e.g., if you ran another clustering algorithm earlier), then you can set the init hyperparameter to a NumPy array containing the list of centroids, and set <code>n_init</code> to 1:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">good_init = np.array([[-<span class="number">3</span>, <span class="number">3</span>], [-<span class="number">3</span>, <span class="number">2</span>], [-<span class="number">3</span>, <span class="number">1</span>], [-<span class="number">1</span>, <span class="number">2</span>], [<span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">5</span>, init=good_init, n_init=<span class="number">1</span>, random_state=<span class="number">42</span>)</span><br><span class="line">kmeans.fit(X)</span><br></pre></td></tr></table></figure>

<p>Another solution is to run the algorithm multiple times with different random initializations and keep the best solution. The number of random initializations is controlled by the <code>n_init</code> hyperparameter: by default it is equal to 10, which means that the whole algorithm described earlier runs 10 times when you call fit(), and Scikit-Learn keeps the best solution. But how exactly does it know which solution is the best? It uses a performance metric! That metric is called the model’s inertia, which is the sum of the squared distances between the instances and their closest centroids. It is roughly equal to 219.4 for the model on the left in Figure 9-5, 258.6 for the model on the right in Figure 9-5, and only 211.6 for the model in Figure 9-3. The KMeans class runs the algorithm n_init times and keeps the model with the lowest inertia.</p>
<p>Scikit Learn 中的 k-means 实现使用了多个启发式算法来帮助避免局部最优解。具体来说，它采用了以下两种方法：</p>
<ol>
<li><p>K-Means++ 初始化</p>
<p>Scikit Learn 中的 k-means 实现采用了 K-Means++ 初始化方法来选择初始聚类中心。K-Means++ 初始化方法是一种基于概率的算法，它会选择离前一个聚类中心越远的点作为下一个聚类中心的位置。这种方法可以帮助确保初始聚类中心的选择更加随机，并且能够更好地代表整个数据集。由于聚类中心的初始位置对算法结果的影响非常大，因此这种初始化方法可以大大减少算法陷入局部最优解的风险。</p>
</li>
<li><p>多次随机初始化</p>
<p>为了进一步减少算法陷入局部最优解的风险，Scikit Learn 中的 k-means 实现会默认多次 (default <code>n_init=10</code> times) 随机初始化聚类中心，每次初始化都是独立的，并且最终的结果是多次运行中得分最好的结果。通过使用多次随机初始化，可以大大增加算法找到全局最优解的概率，并且可以减少算法陷入局部最优解的风险。</p>
</li>
</ol>
<p>需要注意的是，这些启发式算法并不能保证 k-means 算法一定能够避免局部最优解，但它们可以显著提高算法的效果，尤其是在数据集规模较大，数据点数量较多的情况下。此外，在实际应用中，还需要根据具体的问题和数据集特点选择合适的聚类算法和初始化方法。</p>
<p>Accelerated k-means and mini-batch k-means: This speeds up the algorithm (typically by a factor of three to four) and makes it possible to cluster huge datasets that do not fit in memory. Scikit-Learn implements this algorithm in the MiniBatchKMeans class, which you can use just like the KMeans class:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> MiniBatchKMeans</span><br><span class="line"></span><br><span class="line">minibatch_kmeans = MiniBatchKMeans(n_clusters=k, random_state=<span class="number">42</span>)</span><br><span class="line">minibatch_kmeans.fit(X)</span><br></pre></td></tr></table></figure>

<p>If the dataset does not fit in memory, the simplest option is to use the memmap class, as we did for incremental PCA in Chapter 8. Alternatively, you can pass one mini-batch at a time to the partial_fit() method, but this will require much more work, since you will need to perform multiple initializations and select the best one yourself.</p>
<p><img src="https://s2.loli.net/2023/03/13/NlMLD7U3tAmebBw.png" alt="image-20230313052335399"></p>
<p>k-means 怎么找到合适的 k 值？</p>
<p>在k-means算法中，k值是聚类的簇数，如何选择合适的k值是一个重要的问题。下面列出了一些常用的方法：</p>
<ol>
<li>手肘法（Elbow method）：对于不同的 k 值，计算聚类误差（通常用SSE衡量），并将误差值绘制成图像。选择 k 值时，找到误差曲线上的 “elbow (肘点)”，即误差开始下降缓慢的位置。</li>
<li>轮廓系数法（Silhouette method）：计算每个点的轮廓系数，然后计算整个聚类的平均轮廓系数。轮廓系数越大表示聚类效果越好，通常选择轮廓系数最大的 k 值。</li>
<li>Calinski-Harabasz 指数法：该方法使用了簇内距离和簇间距离的比值作为聚类效果的评估指标，选择指标最大的 k 值。</li>
<li>Gap statistic 法：该方法比较聚类结果和随机数据生成的结果的差异程度，选择差异程度最大的 k 值。</li>
</ol>
<p>以上是常用的一些方法，不同的方法可能会得到不同的k值，需要结合具体问题和数据集选择合适的方法。</p>
<p><img src="https://s2.loli.net/2023/03/13/OSL1ACQf3c5YGTz.png" alt="image-20230313062119294"></p>
<p>This technique for choosing the best value for the number of clusters is rather coarse. A more precise (but also more computationally expensive) approach is to use the <em>silhouette score (轮廓系数)</em>, which is the mean silhouette coefficient over all the instances. The silhouette coefficient can vary between –1 and +1. A coefficient close to +1 means that the instance is well inside its own cluster and far from other clusters, while a coefficient close to 0 means that it is close to a cluster boundary; finally, a coefficient close to –1 means that the instance may have been assigned to the wrong cluster.</p>
<p>To compute the silhouette score, you can use Scikit-Learn’s <code>silhouette_score()</code> function, giving it all the instances in the dataset and the labels they were assigned:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> silhouette_score</span><br><span class="line"></span><br><span class="line">silhouette_score(X, kmeans.labels_)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/03/13/ugmaxqLzCntpOKf.png" alt="image-20230313172722614"></p>
<p><img src="https://s2.loli.net/2023/03/13/mdKrqXGY1OHScVf.png" alt="image-20230313173143575"></p>
<h4 id="Limits-of-k-means"><a href="#Limits-of-k-means" class="headerlink" title="Limits of k-means"></a>Limits of k-means</h4><p>Despite its many merits, most notably being fast and scalable, k-means is not perfect. As we saw, it is necessary to run the algorithm several times to avoid suboptimal solutions, plus you need to specify the number of clusters, which can be quite a hassle. Moreover, k-means does not behave very well when the clusters have varying sizes, different densities, or nonspherical shapes. For example, Figure 9-11 shows how k-means clusters a dataset containing three ellipsoidal clusters of different dimen‐ sions, densities, and orientations.</p>
<p><img src="https://s2.loli.net/2023/03/13/leYsJdExILFOTA4.png" alt="image-20230313174821117"></p>
<p>Suggestion: It is important to scale the input features (see Chapter 2) before you run k-means, or the clusters may be very stretched and k-means will perform poorly. Scaling the features does not guarantee that all the clusters will be nice and spherical, but it generally helps k-means.</p>
<h4 id="Using-Clustering-for-Image-Segmentation"><a href="#Using-Clustering-for-Image-Segmentation" class="headerlink" title="Using Clustering for Image Segmentation"></a>Using Clustering for Image Segmentation</h4><p><em>Image segmentation</em> is the task of partitioning an image into multiple segments. There are several variants:</p>
<ul>
<li><em>color sementation</em></li>
<li><em>semantic sementation</em></li>
<li><em>instance segmentation</em></li>
</ul>
<p>The state of the art in semantic or instance segmentation today is achieved using complex architectures based on convolutional neural networks (see Chapter 14). In this chapter we are going to focus on the (much simpler) color segmentation task, using k-means.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> PIL</span><br><span class="line">image = np.asarray(PIL.Image.<span class="built_in">open</span>(filepath))</span><br><span class="line">image.shape</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; (533, 800, 3)</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">X = image.reshape(-<span class="number">1</span>, <span class="number">3</span>)</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">8</span>, random_state=<span class="number">42</span>).fit(X)</span><br><span class="line">segmented_img = kmeans.cluster_centers_[kmeans.labels_]</span><br><span class="line">segmented_img = segmented_img.reshape(image.shape)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/03/16/ft1xcKyDFCas4zS.png" alt="image-20230316042240079"></p>
<h4 id="Using-Clustering-for-Semi-Supervised-Learning"><a href="#Using-Clustering-for-Semi-Supervised-Learning" class="headerlink" title="Using Clustering for Semi-Supervised Learning"></a>Using Clustering for Semi-Supervised Learning</h4><p>Scikit-Learn also offers two classes that can propagate labels automatically: LabelSpreading and LabelPropagation in the sklearn.semi_supervised package. Both classes construct a simi‐ larity matrix between all the instances, and iteratively propagate labels from labeled instances to similar unlabeled instances. There’s also a very different class called SelfTrainingClassifier in the same package: you give it a base classifier (such as a RandomForest Classifier) and it trains it on the labeled instances, then uses it to predict labels for the unlabeled samples. It then updates the train‐ ing set with the labels it is most confident about, and repeats this process of training and labeling until it cannot add labels anymore. These techniques are not magic bullets, but they can occasionally give your model a little boost.</p>
<h4 id="DBSCAN"><a href="#DBSCAN" class="headerlink" title="DBSCAN"></a>DBSCAN</h4><p>The <em>density-based spatial clustering of applications with noise</em> (DBSCAN) algorithm defines clusters as **continuous regions of high density **(depends on $\epsilon$  and <em>min_samples</em>). </p>
<p>DBSCAN（Density-Based Spatial Clustering of Applications with Noise）是一种基于密度的聚类算法，可以自动发现具有相似密度的样本集合，并将它们划分为不同的簇。与传统的聚类算法（如K-Means）不同，DBSCAN不需要事先指定簇的个数，能够识别任意形状的簇，并且能够将孤立点（噪声）单独标识出来。</p>
<p>DBSCAN的主要思想是通过定义密度连通性来刻画簇，即将一些密度相邻的点聚到一起，这种密度连通性可以用点的邻域半径 $\epsilon$ 和最小点数 $min_samples$ 两个参数来控制。对于一个样本点，如果其 $\epsilon$ 半径内的样本点数大于等于 $min_samples$，则将其视为核心点，以该核心点为中心，以 $\epsilon$ 为半径的圆形区域内的所有样本点都属于同一个簇。对于那些不满足条件的样本点，如果它位于某个核心点的邻域半径内，即可被视为该簇的边界点，否则被视为噪声点。</p>
<p>DBSCAN的优点包括：不需要预先知道簇的数量；可以找到任意形状的簇；可以识别噪声点。但是 DBSCAN 也有一些缺点，例如对参数的选择比较敏感，需要调整多个超参数，且对于大数据集，它的效果可能不如其他聚类算法，因为时间复杂度近似 O(m^2^n)。</p>
<p>以下是一个使用 Scikit-learn 库中的 DBSCAN 进行聚类的示例代码：</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_blobs</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成数据集</span></span><br><span class="line">X, y = make_blobs(n_samples=<span class="number">1000</span>, centers=<span class="number">3</span>, random_state=<span class="number">42</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 调用DBSCAN进行聚类</span></span><br><span class="line">dbscan = DBSCAN(eps=<span class="number">0.5</span>, min_samples=<span class="number">5</span>)</span><br><span class="line">dbscan.fit(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可视化聚类结果</span></span><br><span class="line">plt.scatter(X[:, <span class="number">0</span>], X[:, <span class="number">1</span>], c=dbscan.labels_)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br></pre></td></tr></table></figure>

<p>其中，<code>eps</code> 和 <code>min_samples</code>是 DBSCAN 算法的两个超参数，需要根据实际情况进行调整。</p>
<p><img src="https://s2.loli.net/2023/03/16/UzJTMp7CWB8dGjS.png" alt="image-20230316220506557"></p>
<hr>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> DBSCAN</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_moons</span><br><span class="line"></span><br><span class="line">X, y = make_moons(n_samples=<span class="number">1000</span>, noise=<span class="number">0.05</span>)</span><br><span class="line">dbscan = DBSCAN(eps=<span class="number">0.05</span>, min_samples=<span class="number">5</span>)</span><br><span class="line">dbscan.fit(X)</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/03/16/3Bdm5oxPWeyLIiA.png" alt="image-20230316220556150"></p>
<p>Surprisingly, the DBSCAN class does not have a <code>predict()</code> method, although it has a <code>fit_predict()</code> method. In other words, it cannot predict which cluster a new instance belongs to. This decision was made because different classification algo‐ rithms can be better for different tasks, so the authors decided to let the user choose which one to use. Moreover, it’s not hard to implement. For example, let’s train a <code>KNeighborsClassifier</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"></span><br><span class="line">knn = KNeighborsClassifier(n_neighbors=<span class="number">50</span>)</span><br><span class="line">knn.fit(dbscan.components_, dbscan.labels_[dbscan.core_sample_indices_])</span><br></pre></td></tr></table></figure>

<p>Now, given a few new instances, we can predict which clusters they most likely belong to and even estimate a probability for each cluster:</p>
<p><img src="https://s2.loli.net/2023/03/16/DrgqId2Zw6L43Xn.png" alt="image-20230316221711026"></p>
<p>Note that we only trained the classifier on the core instances, but we could also have chosen to train it on all the instances, or all but the anomalies: this choice depends on the final task.</p>
<p>Suggestion: You may also want to try hierarchical DBSCAN (HDBSCAN), which is implemented in the scikit-learn-contrib project, as it is usually better than DBSCAN at finding clusters of varying densities.</p>
<h4 id="Other-Clustering-Algorithms"><a href="#Other-Clustering-Algorithms" class="headerlink" title="Other Clustering Algorithms"></a>Other Clustering Algorithms</h4><ul>
<li><p><em>Agglomerative clustering</em></p>
</li>
<li><p><em>BIRCH</em>: balanced iterative reducing and clustering using hierarchies</p>
</li>
<li><p><em>Mean-shift</em></p>
</li>
<li><p><em>Affinity propagation</em></p>
</li>
<li><p><em>Spectral clustering</em></p>
</li>
</ul>
<p>Now let’s dive into Gaussian mixture models, which can be used for density estimation, clustering, and anomaly detection.</p>
<h3 id="9-2-Gaussian-Mixtures"><a href="#9-2-Gaussian-Mixtures" class="headerlink" title="9.2 Gaussian Mixtures"></a>9.2 Gaussian Mixtures</h3><h4 id="Using-Gaussian-Mixtures-for-Anomaly-Detection"><a href="#Using-Gaussian-Mixtures-for-Anomaly-Detection" class="headerlink" title="Using Gaussian Mixtures for Anomaly Detection"></a>Using Gaussian Mixtures for Anomaly Detection</h4><h4 id="Selecting-the-Number-of-Cluster"><a href="#Selecting-the-Number-of-Cluster" class="headerlink" title="Selecting the Number of Cluster"></a>Selecting the Number of Cluster</h4><h4 id="Bayesian-Gaussian-Mixture-Models"><a href="#Bayesian-Gaussian-Mixture-Models" class="headerlink" title="Bayesian Gaussian Mixture Models"></a>Bayesian Gaussian Mixture Models</h4><h4 id="Other-Algorithm-for-Anomaly-and-Novelty-Detection"><a href="#Other-Algorithm-for-Anomaly-and-Novelty-Detection" class="headerlink" title="Other Algorithm for Anomaly and Novelty Detection"></a>Other Algorithm for Anomaly and Novelty Detection</h4><h3 id="9-x-Exercises"><a href="#9-x-Exercises" class="headerlink" title="9.x Exercises"></a>9.x Exercises</h3><h1 id="Part-II-Neural-Networks-and-Deep-Learning"><a href="#Part-II-Neural-Networks-and-Deep-Learning" class="headerlink" title="Part II. Neural Networks and Deep Learning"></a>Part II. Neural Networks and Deep Learning</h1><h2 id="10-Keras-人工神经网络简介"><a href="#10-Keras-人工神经网络简介" class="headerlink" title="10. Keras 人工神经网络简介"></a>10. Keras 人工神经网络简介</h2><h3 id="10-1-From-Biological-to-Artificial-Neurons"><a href="#10-1-From-Biological-to-Artificial-Neurons" class="headerlink" title="10.1 From Biological to Artificial Neurons"></a>10.1 From Biological to Artificial Neurons</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> load_iris</span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> Perceptron</span><br><span class="line"></span><br><span class="line">iris = load_iris(as_frame=<span class="literal">True</span>)</span><br><span class="line">X = iris.data[[<span class="string">&quot;petal length (cm)&quot;</span>, <span class="string">&quot;petal width (cm)&quot;</span>]].values</span><br><span class="line">y = (iris.target == <span class="number">0</span>) <span class="comment"># Iris setosa</span></span><br><span class="line"></span><br><span class="line">per_clf = Perceptron(random_state=<span class="number">42</span>)</span><br><span class="line">per_clf.fit(X, y)</span><br><span class="line"></span><br><span class="line">X_new = np.array([[<span class="number">2</span>, <span class="number">0.5</span>], [<span class="number">3</span>, <span class="number">1</span>]])</span><br><span class="line">y_pred = per_clf.predict(X_new)  <span class="comment"># predicts True and False for these 2 flowers</span></span><br></pre></td></tr></table></figure>

<p>You may have noticed that the perceptron learning algorithm strongly resembles sto‐ chastic gradient descent (introduced in Chapter 4). In fact, Scikit-Learn’s Perceptron class is equivalent to using an SGDClassifier with the following hyperparameters: <code>loss=&quot;perceptron&quot;</code>, <code>learning_rate=&quot;constant&quot;</code>, <code>eta0=1</code> (the learning rate), and <code>penalty=None</code> (no regularization).</p>
<p>A <em>multilayer perceptron</em> (MLP) solves <em>exclusive OR</em> (XOR) classification problem:</p>
<p><img src="https://s2.loli.net/2023/03/18/LRU6POYwI5uFsn4.png" alt="image-20230318023344854"></p>
<p>Description: Contrary to logistic regression classifiers, perceptrons do not out‐ put a class probability. This is one reason to prefer logistic regression over perceptrons. Moreover, perceptrons do not use any regularization by default, and training stops as soon as there are no more prediction errors on the training set, so the model typically does not generalize as well as logistic regression or a linear SVM classifier. However, perceptrons may train a bit faster.</p>
<hr>
<p>反向传播（Backpropagation）是一种用于训练神经网络的算法，它可以有效地计算神经网络中每个权重对最终输出误差的影响，并相应地更新权重。反向传播算法通过将输入样本送入神经网络进行正向传播，然后通过误差反向传播来调整每个权重。</p>
<p>反向传播算法的步骤如下：</p>
<ol>
<li>随机初始化权重：在开始训练前，需要随机初始化神经网络中的权重和偏置。</li>
<li>正向传播：将训练样本送入神经网络，并计算每个神经元的输出。从输入层开始，依次计算每一层的输出，直到输出层得到网络的预测结果。</li>
<li>计算误差：将神经网络的输出与训练数据的真实标签进行比较，并计算误差。常用的误差函数包括均方误差和交叉熵等。</li>
<li>反向传播误差：从输出层开始，计算每个神经元的误差，并将误差传递回之前的层。这个过程中使用链式法则计算每个权重对误差的贡献。</li>
<li>更新权重：根据反向传播算法计算出的权重梯度，使用梯度下降法更新每个权重。学习率是一个超参数，需要手动设置，决定了每次更新时的步长。</li>
<li>重复迭代：重复以上步骤，直到达到指定的迭代次数或误差阈值。</li>
</ol>
<p>反向传播算法通过逐步调整每个权重，使神经网络能够更好地拟合训练数据，并实现更好的泛化能力。</p>
<p><img src="https://s2.loli.net/2023/03/18/bQiHNCtTX8uGylo.png" alt="image-20230318040407780"></p>
<ul>
<li><p>什么样的函数适合做激活函数？对激活函数的导数有什么要求吗？</p>
<p>在神经网络中，通常选择一些非线性的函数作为激活函数，因为它们可以让神经网络学习非线性关系。常用的激活函数包括Sigmoid、ReLU、LeakyReLU、ELU等。</p>
<p>对于激活函数的导数，有两个要求：</p>
<ol>
<li><p>导数必须存在，这是因为反向传播算法中需要计算梯度，而梯度是导数的值。</p>
</li>
<li><p>导数不能太小或太大。如果导数太小，则反向传播算法可能会出现梯度消失的问题，导致神经网络无法学习；如果导数太大，则反向传播算法可能会出现梯度爆炸的问题，导致优化过程不稳定。</p>
</li>
</ol>
<p>因此，一些常用的激活函数，如Sigmoid和tanh函数，它们的导数在输入较大或较小的时候会趋近于0，容易出现梯度消失的问题；而ReLU函数在输入为负数时的导数为0，也容易出现梯度消失的问题。因此，在实际应用中，通常使用LeakyReLU、ELU等激活函数来避免梯度消失的问题。</p>
</li>
</ul>
<p>Gradient descent does not converge very well when the features have very different scales.</p>
<h3 id="10-2-Implementing-MLPs-with-Keras"><a href="#10-2-Implementing-MLPs-with-Keras" class="headerlink" title="10.2 Implementing MLPs with Keras"></a>10.2 Implementing MLPs with Keras</h3><p>Here is a classification MLP with two hidden layers:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">tf.random.set_seed(<span class="number">42</span>)</span><br><span class="line">model = tf.keras.models.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>]),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>You could also choose to use the <code>tf.keras.utils.set_random_seed()</code> function, which conveniently sets the random seeds for TensorFlow, Python (<code>random.seed()</code>), and NumPy (<code>np.random.seed()</code>).</p>
<p>All the parameters of a layer can be accessed using its <code>get_weights()</code> and <code>set_weights()</code> methods. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hidden1 = model.layers[<span class="number">1</span>]  <span class="comment"># hidden.name = &#x27;dense&#x27;, model.get_layer(&#x27;dense&#x27;) is hidden1</span></span><br><span class="line">weights, biases = hidden1.get_weights()</span><br><span class="line"></span><br><span class="line">weights.shape, biases,shape</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; (784, 300), (300,)</p>
<p>Notice that the Dense layer initialized the connection weights randomly (which is needed to break symmetry, as discussed earlier), and the biases were initialized to zeros, which is fine. If you want to use a different initialization method, you can set <code>kernel_initializer</code> (kernel is another name for the matrix of connection weights) or <code>bias_initializer</code> when creating the layer.</p>
<p>Description: 若不指定 input_shape, 在模式 build&#x2F;train 之前, you will not be able to do certain things, such as display the model summary or save the model.</p>
<p>Description: When using the SGD optimizer, it is important to tune the learning rate. So, you will generally want to use optimizer&#x3D;tf.keras. optimizers.SGD(learning_rate&#x3D;__???__) to set the learning rate, rather than optimizer&#x3D;”sgd”, which defaults to a learning rate of 0.01.</p>
<p>history.history is a dictionary containing the loss and extra metrics. If you use this dictionary to create a Pandas DataFrame and call its plot() method, you get the learning curves shown in Figure 10-11:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">pd.DataFrame(history.history).plot(</span><br><span class="line">    figsize=(<span class="number">8</span>, <span class="number">5</span>), xlim=[<span class="number">0</span>, <span class="number">29</span>], ylim=[<span class="number">0</span>, <span class="number">1</span>], grid=<span class="literal">True</span>, xlabel=<span class="string">&quot;Epoch&quot;</span>,</span><br><span class="line">    style=[<span class="string">&quot;r--&quot;</span>, <span class="string">&quot;r--.&quot;</span>, <span class="string">&quot;b-&quot;</span>, <span class="string">&quot;b-*&quot;</span>])  <span class="comment"># 字母表示颜色，dashed line --, solid line -</span></span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/03/21/dsrPqatbNeyO5VE.png" alt="image-20230321012208802"></p>
<p><strong>Always retune the learning rate after changing any hyperparameter.</strong></p>
<p>One example of a nonsequential neural network is a <em>Wide &amp; Deep</em> neural network.</p>
<p><img src="https://s2.loli.net/2023/03/22/rhfWl5byqaDwE21.png" alt="image-20230322023838330"></p>
<p>Saving a trained Keras model (<em>Saved-Model</em> format) is as simple as it gets :</p>
<p>​		<code>model.save(&quot;my_keras_model&quot;, save_format=&quot;tf&quot;)</code></p>
<p>Description: If you set <code>save_format=&quot;h5&quot;</code> or use a filename that ends with <em>.h5</em>, <em>.hdf5</em>, or <em>.keras</em>, then Keras will save the model to a single file using a Keras-specific format based on the <code>HDF5</code> format. However, most TensorFlow deployment tools require the <code>SavedModel</code> format instead.</p>
<p>There are some useful <em>callbacks</em>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">checkpoint_cb = tf.keras.callbacks.ModelCheckpoint(</span><br><span class="line">    <span class="string">&quot;my_checkpoints&quot;</span>, save_weights_only=<span class="literal">True</span>)</span><br><span class="line">history = model.fit([...], callbacks=[checkpoint_cb])</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">early_stopping_cb = tf.keras.callbacks.EarlyStopping(</span><br><span class="line">    patience=<span class="number">10</span>, restore_best_weights=<span class="literal">True</span>)</span><br><span class="line">history = model.fit([...], callbacks=[checkpoint_cb, early_stopping_cb])</span><br></pre></td></tr></table></figure>

<p>If you need extra control, you can easily write your own custom callbacks. For example, the following custom callback will display the ratio between the validation loss and the training loss during training (e.g., to detect overfitting):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PrintValTrainRatioCallback</span>(tf.keras.callbacks.Callback):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">on_epoch_end</span>(<span class="params">self, epoch, logs</span>):</span><br><span class="line">        ratio = logs[<span class="string">&quot;val_loss&quot;</span>] / logs[<span class="string">&quot;loss&quot;</span>]</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f&quot;Epoch=<span class="subst">&#123;epoch&#125;</span>, val/train=<span class="subst">&#123;ratio:<span class="number">.2</span>f&#125;</span>&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>As you might expect, you can implement on_train_begin(), on_train_end(), on_epoch_begin(), on_epoch_end(), on_batch_begin(), and on_batch_end(). Call‐ backs can also be used during evaluation and predictions, should you ever need them (e.g., for debugging). For evaluation, you should implement on_test_begin(), on_test_end(), on_test_batch_begin(), or on_test_batch_end(), which are called by evaluate(). For prediction, you should implement on_predict_begin(), on_predict_end(), on_predict_batch_begin(), or on_predict_batch_end(), which are called by predict().</p>
<p>ign. Using TensorBoard for Visualization</p>
<h3 id="10-3-Fine-Tuning-Neural-Network-Hyperparameters"><a href="#10-3-Fine-Tuning-Neural-Network-Hyperparameters" class="headerlink" title="10.3 Fine-Tuning Neural Network Hyperparameters"></a>10.3 Fine-Tuning Neural Network Hyperparameters</h3><p>One option is to convert your Keras model to a Scikit-Learn estimator, and then use GridSearchCV or RandomizedSearchCV to fine-tune the hyperparameters, as you did in Chapter 2. However, there’s a better way: you can use the <em>Keras Tuner</em> library, which is a hyperparameter tuning library for Keras models. (<code>%pip install -q -U keras-tuner</code>)</p>
<p>For example, the following function builds and compiles an MLP to classify Fashion MNIST images, using hyperparameters such as the number of hidden layers (n_hidden), the number of neurons per layer (n_neurons), the learning rate (learning_rate), and the type of optimizer to use (optimizer):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> keras_tuner <span class="keyword">as</span> kt</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">build_model</span>(<span class="params">hp</span>):</span><br><span class="line">    n_hidden = hp.Int(<span class="string">&quot;n_hidden&quot;</span>, min_value=<span class="number">0</span>, max_value=<span class="number">8</span>, default=<span class="number">2</span>)</span><br><span class="line">    n_neurons = hp.Int(<span class="string">&quot;n_neurons&quot;</span>, min_value=<span class="number">16</span>, max_value=<span class="number">256</span>)</span><br><span class="line">    learning_rate = hp.Float(<span class="string">&quot;learning_rate&quot;</span>, min_value=<span class="number">1e-4</span>,</span><br><span class="line">                             max_value=<span class="number">1e-2</span>, sampling=<span class="string">&quot;log&quot;</span>)</span><br><span class="line">    optimizer = hp.Choice(<span class="string">&quot;optimizer&quot;</span>, values=[<span class="string">&quot;adam&quot;</span>, <span class="string">&quot;sgd&quot;</span>])</span><br><span class="line">    <span class="keyword">if</span> optimizer == <span class="string">&quot;sgd&quot;</span>:</span><br><span class="line">        optimizer = tf.keras.optimizers.SGD(learning_rate=learning_rate)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)</span><br><span class="line"></span><br><span class="line">    model = tf.keras.Sequential()</span><br><span class="line">    model.add(tf.keras.layers.Flatten())</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_hidden):</span><br><span class="line">        model.add(tf.keras.layers.Dense(n_neurons, activation=<span class="string">&quot;relu&quot;</span>))</span><br><span class="line">    model.add(tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>))</span><br><span class="line">    model.<span class="built_in">compile</span>(loss=<span class="string">&quot;sparse_categorical_crossentropy&quot;</span>,</span><br><span class="line">                  optimizer=optimizer,</span><br><span class="line">                  metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">    <span class="keyword">return</span> model</span><br></pre></td></tr></table></figure>

<p>Now if you want to do a basic random search, you can create a <code>kt.RandomSearch</code> tuner, passing the <code>build_model</code> function to the constructor, and call the tuner’s <code>search()</code> method:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">random_search_tuner = kt.RandomSearch(</span><br><span class="line">    build_model, objective=<span class="string">&quot;val_accuracy&quot;</span>,</span><br><span class="line">    max_trials=<span class="number">5</span>, overwrite=<span class="literal">True</span>,</span><br><span class="line">    directory=<span class="string">&quot;my_fashion_mnist&quot;</span>, project_name=<span class="string">&quot;my_rnd_search&quot;</span>,</span><br><span class="line">    seed=<span class="number">42</span>)</span><br><span class="line">random_search_tuner.search(X_train, y_train, epochs=<span class="number">10</span>,</span><br><span class="line">                           validation_data=(X_valid, y_valid))</span><br></pre></td></tr></table></figure>

<p>If you run this code a second time but with <code>overwrite=False</code> and <code>max_ trials=10</code>, the tuner will continue tuning where it left off, running 5 more trials: this means you don’t have to run all the trials in one shot. Lastly, since objective is set to “<code>val_accuracy</code>“, the tuner prefers models with a higher validation accuracy, so once the tuner has finished searching, you can get the best models like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">top3_models = random_search_tuner.get_best_models(num_models=<span class="number">3</span>)</span><br><span class="line">best_model = top3_models[<span class="number">0</span>]</span><br></pre></td></tr></table></figure>

<p>You can also call <code>get_best_hyperparameters()</code> to get the <code>kt.HyperParameters</code> of the best models:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">top3_params = random_search_tuner.get_best_hyperparameters(num_trials=<span class="number">3</span>)</span><br><span class="line">top3_params[<span class="number">0</span>].values  <span class="comment"># best hyperparameter values</span></span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; {‘n_hidden’: 5, ‘n_neurons’: 70, ‘learning_rate’: 0.00041268008323824807, ‘optimizer’: ‘adam’}</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_trial = random_search_tuner.oracle.get_best_trials(num_trials=<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">best_trial.summary()</span><br></pre></td></tr></table></figure>

<p>&gt;&gt;&gt; Trial summary<br>&gt;&gt;&gt; Hyperparameters:<br>&gt;&gt;&gt; n_hidden: 5<br>&gt;&gt;&gt; n_neurons: 70<br>&gt;&gt;&gt; learning_rate: 0.00041268008323824807<br>&gt;&gt;&gt; optimizer: adam<br>&gt;&gt;&gt; Score: 0.8736000061035156</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_trial.metrics.get_last_value(<span class="string">&quot;val_accuracy&quot;</span>)</span><br></pre></td></tr></table></figure>

<p><strong>&gt;&gt;&gt;</strong> 0.8736000061035156</p>
<p>If you are happy with the best model’s performance, you may continue training it for a few epochs on the full training set (X_train_full and y_train_full), then evaluate it on the test set, and deploy it to production (see Chapter 19):</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">best_model.fit(X_train_full, y_train_full, epochs=<span class="number">10</span>)</span><br><span class="line">test_loss, test_accuracy = best_model.evaluate(X_test, y_test)</span><br></pre></td></tr></table></figure>

<p>or you can use <code>kt.Hyperband()</code> instead of <code>kt.RandomSearch()</code>:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MyClassificationHyperModel</span>(kt.HyperModel):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">build</span>(<span class="params">self, hp</span>):</span><br><span class="line">        <span class="keyword">return</span> build_model(hp)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">fit</span>(<span class="params">self, hp, model, X, y, **kwargs</span>):</span><br><span class="line">        <span class="keyword">if</span> hp.Boolean(<span class="string">&quot;normalize&quot;</span>):</span><br><span class="line">            norm_layer = tf.keras.layers.Normalization()</span><br><span class="line">            X = norm_layer(X)</span><br><span class="line">        <span class="keyword">return</span> model.fit(X, y, **kwargs)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">hyperband_tuner = kt.Hyperband(</span><br><span class="line">    MyClassificationHyperModel(), objective=<span class="string">&quot;val_accuracy&quot;</span>, seed=<span class="number">42</span>,</span><br><span class="line">    max_epochs=<span class="number">10</span>, factor=<span class="number">3</span>, hyperband_iterations=<span class="number">2</span>,</span><br><span class="line">    overwrite=<span class="literal">True</span>, directory=<span class="string">&quot;my_fashion_mnist&quot;</span>, project_name=<span class="string">&quot;hyperband&quot;</span>)</span><br></pre></td></tr></table></figure>

<p>keras_tuner 中的 Hyperband 和 RandomSearch 有什么区别？</p>
<p>Hyperband 和 RandomSearch 都是 Keras Tuner 中的调参算法，它们的区别在于：</p>
<ul>
<li><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">Hyperband 是一种基于多臂赌博机的算法，它可以在有限的时间内找到最优的超参数组合</a><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">1</a><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">。它通过分配不同的训练轮数（epochs）给不同的模型，然后根据它们的表现逐步淘汰一些模型，最后留下最优的模型</a><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">1</a>。</li>
<li><a href="https://keras.io/keras_tuner/">RandomSearch 是一种随机搜索算法，它通过在超参数空间中随机采样一定数量的模型，然后比较它们的表现，选择最优的模型</a><a href="https://keras.io/keras_tuner/">2</a><a href="https://keras.io/api/keras_tuner/tuners/random/">3</a><a href="https://keras.io/keras_tuner/">。它不需要指定训练轮数，但是可能需要更多的尝试次数（trials）才能找到最优的模型</a><a href="https://keras.io/keras_tuner/">2</a>。</li>
</ul>
<p>Hyperband 和 RandomSearch 的优缺点如下：</p>
<ul>
<li>Hyperband 的优点是可以快速收敛到最优的模型，节省计算资源和时间<a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">1</a><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">。它的缺点是需要指定最大训练轮数（max_epochs），这可能会影响模型的收敛性和稳定性</a><a href="https://www.tensorflow.org/tutorials/keras/keras_tuner">1</a>。</li>
<li>RandomSearch 的优点是可以探索更广泛的超参数空间，不受训练轮数的限制<a href="https://keras.io/keras_tuner/">2</a><a href="https://keras.io/keras_tuner/">。它的缺点是可能需要更多的尝试次数才能找到最优的模型，消耗更多的计算资源和时间</a><a href="https://keras.io/keras_tuner/">2</a>。</li>
</ul>
<p>因此，Hyperband 和 RandomSearch 的选择取决于你的具体需求和场景。一般来说，如果你有足够的计算资源和时间，你可以使用 RandomSearch 来寻找更好的超参数组合；如果你有限制的计算资源和时间，你可以使用 Hyperband 来快速找到一个<strong>较好</strong>的超参数组合。</p>
<p>Number of Hidden Layers,</p>
<p>Number of Neurons per Hidden Layer: instead of wasting time looking for pants that perfectly match your size, just use large stretch pants that will shrink down to the right size.  Indeed, if a layer has too few neurons, it will not have enough representational power to preserve all the useful information from the inputs. </p>
<p>Suggestion: In general you will get more bang for your buck by increasing the number of layers instead of the number of neurons per layer.</p>
<p>Learning Rate, Batch Size, and Other Hyperparameters</p>
<h3 id="10-x-Exercises"><a href="#10-x-Exercises" class="headerlink" title="10.x Exercises"></a>10.x Exercises</h3><h2 id="11-Training-Deep-Neural-Networks"><a href="#11-Training-Deep-Neural-Networks" class="headerlink" title="11. Training Deep Neural Networks"></a>11. Training Deep Neural Networks</h2><h3 id="11-1-The-Vanishing-Exploding-Gradients-Problems"><a href="#11-1-The-Vanishing-Exploding-Gradients-Problems" class="headerlink" title="11.1 The Vanishing&#x2F;Exploding Gradients Problems"></a>11.1 The Vanishing&#x2F;Exploding Gradients Problems</h3><ul>
<li><p>梯度消失的产生原因？</p>
<p>梯度消失是指在深度神经网络训练中，后面的层（接近输出层）的权重更新变化很小，即梯度接近于0，导致这些层的参数无法得到有效的训练，使得整个网络的性能无法得到有效提升。</p>
<p>梯度消失的产生原因主要有两个：一是激活函数的选择不合适，二是网络结构过深。</p>
<p>1. 激活函数的选择不合适</p>
<p>常用的激活函数如sigmoid、tanh等在输入值很大或很小的时候，导数趋于0，使得梯度逐渐减小。当网络的深度增加时，这种现象更加明显，导致后面的层的梯度几乎为0，无法有效更新参数，从而影响整个网络的训练效果。为了解决这个问题，可以选择其他的激活函数，如ReLU等，这些激活函数在输入值为正的时候，导数为常数，不会出现梯度消失的现象。</p>
<p>2. 网络结构过深</p>
<p>当网络结构过深时，前面层的梯度传递到后面层时可能会逐渐减小，导致后面层的梯度接近于0。这个问题可以通过改变网络结构、使用残差连接等方法来解决。例如，使用ResNet等具有跨层连接的网络结构，可以使得前面层的梯度可以直接传递到后面层，从而避免了梯度消失的问题。</p>
<p><img src="https://s2.loli.net/2023/04/06/6RzZcSPoqnpLrI3.png" alt="image-20230406171205826"></p>
</li>
</ul>
<p>ReLU, leaky ReLU, and PReLU all suffer from the fact that they are not smooth functions: their derivatives abruptly change (at z &#x3D; 0). As we saw in Chapter 4 when we discussed lasso, this sort of discontinuity can make gradient descent bounce around the optimum, and slow down convergence. So now we will look at some smooth variants of the ReLU activation function, starting with ELU and SELU.</p>
<p><img src="https://s2.loli.net/2023/04/06/qVR123yPIrx78Gh.png" alt="image-20230406181310300"></p>
<p>ELU函数的公式为：</p>
<p><img src="https://s2.loli.net/2023/04/07/GZj73DYwL8czqRJ.png" alt="image-20230407014209696"></p>
<p>其中 $\alpha$ 是一个超参数，通常取值为 $1$。</p>
<p>ELU函数的图像与ReLU函数类似，也有一个线性段和一个非线性段。不同的是，在非线性段，ELU函数的值是允许小于0的，并且相对于ReLU函数，ELU函数有一个更加平滑的转折点。</p>
<p>ELU函数的非线性段相对于ReLU函数的主要优点在于：它可以处理输入中的负值，而ReLU函数则将所有负值都裁剪为0。这对于避免神经元的“死亡”（即神经元输出为0）非常重要，特别是对于深度神经网络来说。</p>
<p>下面是ELU函数的图像：略</p>
<p>可以看到，ELU函数在非线性段中比ReLU函数更平滑，可以处理负值，而且两个函数在正值范围内非常相似。</p>
<p><img src="C:\Users\Jacky\AppData\Roaming\Typora\typora-user-images\image-20230407013541277.png" alt="image-20230407013541277"></p>
<p>ELU 函数的唯一缺点就是计算量大 (due to the use of the exponential function).</p>
<p>Suggestion: So, which activation function should you use for the hidden layers of your deep neural networks? ReLU remains a good default for simple tasks: it’s often just as good as the more sophisticated activa‐ tion functions, plus it’s very fast to compute, and many libraries and hardware accelerators provide ReLU-specific optimizations. However, Swish is probably a better default for more complex tasks, and you can even try parametrized Swish with a learnable β parameter for the most complex tasks. Mish may give you slightly better results, but it requires a bit more compute. If you care a lot about runtime latency, then you may prefer leaky ReLU, or parametrized leaky ReLU for more complex tasks. For deep MLPs, give SELU a try, but make sure to respect the constraints listed earlier. If you have spare time and computing power, you can use cross-validation to evaluate other activation functions as well.</p>
<p>Implementing batch normalization with Keras. Just add a BatchNormalization layer before or after each hidden layer’s activation function. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>]),</span><br><span class="line">    tf.keras.layers.BatchNormalization(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">300</span>, activation=<span class="string">&quot;relu&quot;</span>,</span><br><span class="line">                          kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    tf.keras.layers.BatchNormalization(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">100</span>, activation=<span class="string">&quot;relu&quot;</span>,</span><br><span class="line">                          kernel_initializer=<span class="string">&quot;he_normal&quot;</span>),</span><br><span class="line">    tf.keras.layers.BatchNormalization(),</span><br><span class="line">    tf.keras.layers.Dense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p><img src="https://s2.loli.net/2023/04/12/dqiP4kT1XDjWl2b.png" alt="image-20230412192327449"></p>
<p>As you can see, each BN layer adds four parameters per input: <strong>γ</strong>, <strong>β</strong>, <strong>μ</strong>, and <strong>σ</strong> (for example, the first BN layer adds 3,136 parameters, which is 4 × 784). The last two parameters, <strong>μ</strong> and <strong>σ</strong>, are the moving averages; they are not affected by backpropagation, so Keras calls them “non-trainable”13 (if you count the total number of BN parameters, 3,136 + 1,200 + 400, and divide by 2, you get 2,368, which is the total number of non-trainable parameters in this model).</p>
<p>Let’s look at the parameters of the first BN layer. Two are trainable (by backpropagation), and two are not:</p>
<p>​		<strong>&gt;&gt;&gt;</strong> <code>[(var.name, var.trainable) for var in model.layers[1].variables]</code><br>​		[(‘batch_normalization&#x2F;gamma:0’, True),<br>​		 (‘batch_normalization&#x2F;beta:0’, True),<br>​		 (‘batch_normalization&#x2F;moving_mean:0’, False),<br>​		 (‘batch_normalization&#x2F;moving_variance:0’, False)]</p>
<p>Another technique to mitigate the exploding gradients problem is to clip the gradients during backpropagation so that they never exceed some threshold. This is called <em>gradient clipping</em>. This technique is generally used in recurrent neural networks, where using batch normalization is tricky (as you will see in Chapter 15).</p>
<p>In Keras, implementing gradient clipping is just a matter of setting the <code>clipvalue</code> or <code>clipnorm</code> argument when creating an optimizer, like this:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = tf.keras.optimizers.SGD(clipvalue=<span class="number">1.0</span>)</span><br><span class="line">model.<span class="built_in">compile</span>([...], optimizer=optimizer)</span><br></pre></td></tr></table></figure>

<h3 id="11-2-Reusing-Pretrained-Layers"><a href="#11-2-Reusing-Pretrained-Layers" class="headerlink" title="11.2 Reusing Pretrained Layers"></a>11.2 Reusing Pretrained Layers</h3><p>This technique is called <em>transfer learning</em>. It will not only speed up training considerably, but also requires significantly less training data.</p>
<p><img src="https://s2.loli.net/2023/04/13/4PXHFqjGoD6c7hI.png" alt="image-20230413223919847"></p>
<p>Suggestion: The more similar the tasks are, the more layers you will want to reuse (starting with the lower layers). For very similar tasks, try to keep all the hidden layers and just replace the output layer.</p>
<p>Try freezing all the reused layers first (i.e., make their weights non-trainable so that gradient descent won’t modify them and they will remain fixed), then train your model and see how it performs. Then try unfreezing one or two of the top hidden layers to let backpropagation tweak them and see if performance improves. The more training data you have, the more layers you can unfreeze. It is also useful to reduce the learning rate when you unfreeze reused layers: this will avoid wrecking their fine-tuned weights.</p>
<p>If you still cannot get good performance, and you have little training data, try dropping the top hidden layer(s) and freezing all the remaining hidden layers again.</p>
<p>You can iterate until you find the right number of layers to reuse. If you have plenty of training data, you may try replacing the top hidden layers instead of dropping them, and even adding more hidden layers.</p>
<p>Transfer Learning with Keras. Let’s look at an example. </p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">[...] <span class="comment"># Assuming model A was already trained and saved to &quot;my_model_A&quot;</span></span><br><span class="line">model_A = tf.keras.models.load_model(<span class="string">&quot;my_model_A&quot;</span>)</span><br><span class="line">model_A_clone = tf.keras.models.clone_model(model_A)</span><br><span class="line">model_A_clone.set_weights(model_A.get_weights())</span><br><span class="line"></span><br><span class="line">model_B_on_A = tf.keras.Sequential(model_A_clone.layers[:-<span class="number">1</span>])</span><br><span class="line">model_B_on_A.add(tf.keras.layers.Dense(<span class="number">1</span>, activation=<span class="string">&quot;sigmoid&quot;</span>))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model_B_on_A.layers[:-<span class="number">1</span>]:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_B_on_A.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>, </span><br><span class="line">                     optimizer=optimizer,</span><br><span class="line">                     metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br></pre></td></tr></table></figure>

<p>Attention: <code>tf.keras.models.clone_model()</code> only clones the architecture, not the weights. If you don’t copy them manually using <code>set_weights()</code>, they will be initialized randomly when the cloned model is first used.</p>
<p>Description: You must always compile your model after you freeze or unfreeze layers.</p>
<p>Now you can train the model for a few epochs, then unfreeze the reused layers (which requires compiling the model again) and continue training to fine-tune the reused layers for task B. After unfreezing the reused layers, it is usually a good idea to reduce the learning rate, once again to avoid damaging the reused weights.</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">history = model_B_on_A.fit(X_train_B, y_train_B, epochs=<span class="number">4</span>,</span><br><span class="line">                           validation_data=(X_valid_B, y_valid_B))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model_B_on_A.layers[:-<span class="number">1</span>]:</span><br><span class="line">    layer.trainable = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.001</span>)</span><br><span class="line">model_B_on_A.<span class="built_in">compile</span>(loss=<span class="string">&quot;binary_crossentropy&quot;</span>,</span><br><span class="line">                     optimizer=optimizer,</span><br><span class="line">                     metrics=[<span class="string">&quot;accuracy&quot;</span>])</span><br><span class="line">history = model_B_on_A.fit(X_train_B, y_train_B, epochs=<span class="number">16</span>,</span><br><span class="line">                           validation_data=(X_valid_B, y_valid_B))</span><br></pre></td></tr></table></figure>

<p>​		<strong>&gt;&gt;&gt;</strong> model_B_on_A.evaluate(X_test_B, y_test_B)</p>
<p>​		[0.2546142041683197, 0.9384999871253967]</p>
<p><img src="https://s2.loli.net/2023/04/17/FIdmLwHfYzEPoO9.png" alt="image-20230417215638296"></p>
<h3 id="11-3-Faster-Optimizers"><a href="#11-3-Faster-Optimizers" class="headerlink" title="11.3 Faster Optimizers"></a>11.3 Faster Optimizers</h3><p>Momentum<br>Nesterov Accelerated Gradient<br>AdaGrad<br>RMSProp<br>Adam<br>AdaMax<br>Nadam<br>AdamW</p>
<h3 id="11-4-Learning-Rate-Scheduling"><a href="#11-4-Learning-Rate-Scheduling" class="headerlink" title="11.4 Learning Rate Scheduling"></a>11.4 Learning Rate Scheduling</h3><p>These are the most commonly used learning schedules:</p>
<ul>
<li><p><em>Power scheduling</em></p>
<p>Set the learning rate to a function of the iteration number t: η(t) &#x3D; η<del>0</del> &#x2F; (1 + t&#x2F;s)c. The initial learning rate η<del>0</del>, the power c (typically set to 1), and the steps s are hyperparameters. The learning rate drops at each step. After s steps, the learning rate is down to η<del>0</del> &#x2F; 2. After s more steps it is down to η<del>0</del> &#x2F; 3, then it goes down to η<del>0</del> &#x2F; 4, then η<del>0</del> &#x2F; 5, and so on. As you can see, this schedule first drops quickly, then more and more slowly. Of course, power scheduling requires tuning η<del>0</del> and s (and possibly c).</p>
</li>
<li><p><em>Exponential scheduling</em></p>
</li>
</ul>
<p>Set the learning rate to η(t) &#x3D; η<del>0</del> 0.1^t&#x2F;s^. The learning rate will gradually drop by a factor of 10 every <em>s</em> steps. While power scheduling reduces the learning rate more and more slowly, exponential scheduling keeps slashing it by a factor of 10 every <em>s</em> steps.</p>
<ul>
<li><p><em>Piecewise constant scheduling</em></p>
</li>
<li><p><em>Performance scheduling</em></p>
</li>
<li><p><em>1cycle scheduling</em></p>
</li>
</ul>
<p>Implementing power scheduling in Keras is the easiest option—just set the decay hyperparameter when creating an optimizer:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=<span class="number">0.01</span>, decay=<span class="number">1e-4</span>)</span><br></pre></td></tr></table></figure>

<p>The decay is the inverse of s (the number of steps it takes to divide the learning rate by one more unit), and Keras assumes that c is equal to 1.</p>
<p>Exponential scheduling and piecewise scheduling are quite simple too. You first need to define a function that takes the current epoch and returns the learning rate. For example, let’s implement exponential scheduling:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exponential_decay_fn</span>(<span class="params">epoch</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.01</span> * <span class="number">0.1</span> ** (epoch / <span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>If you do not want to hardcode <em>η</em><del>0</del> and <em>s</em>, you can create a function that returns a configured function:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exponential_decay</span>(<span class="params">lr0, s</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">exponential_decay_fn</span>(<span class="params">epoch</span>):</span><br><span class="line">        <span class="keyword">return</span> lr0 * <span class="number">0.1</span> ** (epoch / s)</span><br><span class="line">    <span class="keyword">return</span> exponential_decay_fn</span><br><span class="line"></span><br><span class="line">exponential_decay_fn = exponential_decay(lr0=<span class="number">0.01</span>, s=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>Next, create a <code>LearningRateScheduler</code> callback, giving it the schedule function, and pass this callback to the <code>fit()</code> method:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line">lr_scheduler = tf.keras.callbacks.LearningRateScheduler(exponential_decay_fn)</span><br><span class="line">history = model.fit(X_train, y_train, [...], callbacks=[lr_scheduler])</span><br></pre></td></tr></table></figure>

<p>The LearningRateScheduler will update the optimizer’s learning_rate attribute at the beginning of each epoch. After training, history.history[“lr”] gives you access to the list of learning rates used during training.</p>
<p>The schedule function can take the current learning rate as a second argument:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exponential_decay_fn</span>(<span class="params">epoch, lr</span>):</span><br><span class="line">    <span class="keyword">return</span> lr * <span class="number">0.1</span> ** (<span class="number">1</span> / <span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<p>Lastly, Keras offers an alternative way to implement learning rate scheduling: you can define a scheduled learning rate using one of the classes available in <code>tf.keras. optimizers.schedules</code>, then pass it to any optimizer. This approach updates the learning rate at each step rather than at each epoch. For example, here is how to implement the same exponential schedule as the exponential_decay_fn() function we defined earlier:</p>
<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">n_epochs = <span class="number">25</span></span><br><span class="line">n_steps = n_epochs * math.ceil(<span class="built_in">len</span>(X_train) / batch_size)</span><br><span class="line">scheduled_learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(</span><br><span class="line">    initial_learning_rate=<span class="number">0.01</span>, decay_steps=n_steps, decay_rate=<span class="number">0.1</span>)</span><br><span class="line">optimizer = tf.keras.optimizers.SGD(learning_rate=scheduled_learning_rate)</span><br></pre></td></tr></table></figure>

<h3 id="11-5-Avoiding-Overfitting-Through-Regularization"><a href="#11-5-Avoiding-Overfitting-Through-Regularization" class="headerlink" title="11.5 Avoiding Overfitting Through Regularization"></a>11.5 Avoiding Overfitting Through Regularization</h3><h4 id="ℓ1-and-ℓ2-Regularization"><a href="#ℓ1-and-ℓ2-Regularization" class="headerlink" title="ℓ1 and ℓ2 Regularization"></a>ℓ1 and ℓ2 Regularization</h4><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> functools <span class="keyword">import</span> partial</span><br><span class="line"></span><br><span class="line">RegularizedDense = partial(tf.keras.layers.Dense,</span><br><span class="line">                           activation=<span class="string">&quot;relu&quot;</span>,</span><br><span class="line">                           kernel_initializer=<span class="string">&quot;he_normal&quot;</span>,</span><br><span class="line">                           kernel_regularizer=tf.keras.regularizers.l2(<span class="number">0.01</span>))</span><br><span class="line"></span><br><span class="line">model = tf.keras.Sequential([</span><br><span class="line">    tf.keras.layers.Flatten(input_shape=[<span class="number">28</span>, <span class="number">28</span>]),</span><br><span class="line">    RegularizedDense(<span class="number">100</span>),</span><br><span class="line">    RegularizedDense(<span class="number">100</span>),</span><br><span class="line">    RegularizedDense(<span class="number">10</span>, activation=<span class="string">&quot;softmax&quot;</span>)</span><br><span class="line">])</span><br></pre></td></tr></table></figure>

<p>Warning: As we saw earlier, ℓ2  regularization is fine when using SGD, momentum optimization, and Nesterov momentum optimization, but not with Adam and its variants. If you want to use Adam with weight decay, then do not use ℓ2  regularization: use AdamW instead.</p>
<h4 id="Dropout"><a href="#Dropout" class="headerlink" title="Dropout"></a>Dropout</h4><p>It is a fairly simple algorithm: at every training step, every neuron (including the input neurons, but always excluding the output neurons) has a probability p of being temporarily “dropped out”, meaning it will be entirely ignored during this training step, but it may be active during the next step (see Figure 11-10). The hyperparameter p is called the dropout rate, and it is typically set between 10% and 50%: closer to 20%–30% in recurrent neural nets (see Chapter 15), and closer to 40%–50% in convo‐ lutional neural networks (see Chapter 14). After training, neurons don’t get dropped anymore. And that’s all (except for a technical detail we will discuss momentarily).</p>
<h3 id="11-6-Summary-and-Practical-Guidelines"><a href="#11-6-Summary-and-Practical-Guidelines" class="headerlink" title="11.6 Summary and Practical Guidelines"></a>11.6 Summary and Practical Guidelines</h3><h3 id="11-x-Exercises"><a href="#11-x-Exercises" class="headerlink" title="11.x Exercises"></a>11.x Exercises</h3><h2 id="12-使用-TensorFlow-自定义模型和训练"><a href="#12-使用-TensorFlow-自定义模型和训练" class="headerlink" title="12. 使用 TensorFlow 自定义模型和训练"></a>12. 使用 TensorFlow 自定义模型和训练</h2><h2 id="13-使用-TensorFlow-加载和预处理数据"><a href="#13-使用-TensorFlow-加载和预处理数据" class="headerlink" title="13. 使用 TensorFlow 加载和预处理数据"></a>13. 使用 TensorFlow 加载和预处理数据</h2><h2 id="14-CV-with-CNNs"><a href="#14-CV-with-CNNs" class="headerlink" title="14. CV with CNNs"></a>14. CV with CNNs</h2><h2 id="15-Sequences-with-RNNs-and-CNNs"><a href="#15-Sequences-with-RNNs-and-CNNs" class="headerlink" title="15. Sequences with RNNs and CNNs"></a>15. Sequences with RNNs and CNNs</h2><h2 id="16-NLP-with-RNNs-and-Attention"><a href="#16-NLP-with-RNNs-and-Attention" class="headerlink" title="16. NLP with RNNs and Attention"></a>16. NLP with RNNs and Attention</h2><p>CoT: <a href="https://zhuanlan.zhihu.com/p/493533589">思维链（Chain-of-thoughts）作为提示 - 知乎 (zhihu.com)</a></p>
<h2 id="17-Autoencoders-GANs-和-Diffusion-Models"><a href="#17-Autoencoders-GANs-和-Diffusion-Models" class="headerlink" title="17. Autoencoders, GANs, 和 Diffusion Models"></a>17. Autoencoders, GANs, 和 Diffusion Models</h2><h2 id="18-强化学习"><a href="#18-强化学习" class="headerlink" title="18. 强化学习"></a>18. 强化学习</h2><h2 id="19-大规模训练和部署-TensorFlow-模型"><a href="#19-大规模训练和部署-TensorFlow-模型" class="headerlink" title="19. 大规模训练和部署 TensorFlow 模型"></a>19. 大规模训练和部署 TensorFlow 模型</h2>]]></content>
      <categories>
        <category>machine_learning</category>
      </categories>
      <tags>
        <tag>jacky</tag>
        <tag>python</tag>
        <tag>机器学习</tag>
        <tag>machine learning</tag>
        <tag>tensorflow</tag>
      </tags>
  </entry>
</search>
